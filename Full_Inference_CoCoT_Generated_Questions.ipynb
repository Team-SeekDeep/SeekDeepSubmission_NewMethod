{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9874197",
   "metadata": {},
   "source": [
    "#  Vertex Inference Unified\n",
    "\n",
    "This notebook uses the unified `google-genai` library (imported as `from google import genai`). It supports:\n",
    "- **Vertex AI Backend:** Uploads videos to GCS during the 'Prepare' step.\n",
    "- **Gemini API Backend:** Uploads videos using the **File API** during the 'Prepare' step.\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1.  **Import Libraries & Configure.**\n",
    "2.  **Config** - Set up the configuration for the pipeline, including the model, model config, prompts, and prompt config.\n",
    "2.  **Initialize Clients:** Set up AI client and Storage client.\n",
    "3.  **(Only the first time) Fetch Dataset:** Downloads metadata from HuggingFace.\n",
    "4.  **(Only the first time on each API type) Download, Extract & Prepare Videos:** Downloads, extracts, uploads (GCS/File API). Updates metadata.\n",
    "5.  **Bulk Inference (Async):** Performs inference using pre-uploaded video resources.\n",
    "6.  **Single Prompt Testing (UI):** Allows interactive testing of the video with prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19484ef",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "1. After switching from Vertex to Gemini and vice versa, be sure to follow the steps:\n",
    "    - Run all cells in order to re-upload the videos to the correct storage client, you can enable the SKIP_DOWNLOAD and SKIP_EXTRACT flags to skip the download and extraction steps. Only the upload step is needed\n",
    "\n",
    "2. Gemini API's file client has a expiry time of 1 day or so for the uploaded files. You may need to follow the steps above to re-upload the files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c07e40",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports (Corrected for `google.genai`)\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import datetime\n",
    "import zipfile\n",
    "import math\n",
    "import sys\n",
    "import asyncio\n",
    "from typing import Dict, List, Optional, Set, Tuple, Any\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import fractions\n",
    "\n",
    "# Google Cloud & AI Libraries (Unified SDK)\n",
    "try:\n",
    "    import google.genai as genai\n",
    "    from google.genai import types\n",
    "    from google.genai import errors as genai_errors\n",
    "    from google.api_core import exceptions as api_core_exceptions\n",
    "    # GCS Client (Optional, for Vertex Mode)\n",
    "    try:\n",
    "        from google.cloud import storage\n",
    "        GCS_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"INFO: google-cloud-storage not found. Vertex AI GCS operations unavailable.\")\n",
    "        storage = None\n",
    "        GCS_AVAILABLE = False\n",
    "    print(\"`google.genai` SDK and helpers imported successfully.\")\n",
    "except ImportError as e:\n",
    "     print(f\"ERROR: Failed to import Google libraries: {e}. Install: pip install google-genai google-api-core google-cloud-storage\")\n",
    "     genai = None; types = None; genai_errors = None; api_core_exceptions = None\n",
    "     storage = None; GCS_AVAILABLE = False\n",
    "     raise ImportError(\"FATAL: `google.genai` or `google-api-core` SDK not found.\")\n",
    "\n",
    "# Data Handling & Progress\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# UI Elements\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "\n",
    "# Async in Notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbce404",
   "metadata": {},
   "source": [
    "## Config Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GCP Configuration ---\n",
    "\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID_HERE\" # Your Google Cloud Project ID (Needed for GCS and Vertex AI mode)\n",
    "LOCATION = \"us-central1\"      # Your Google Cloud Region (Needed for Vertex AI mode)\n",
    "GCS_BUCKET = \"YOUR_GCS_BUCKET_HERE\" # Your GCS bucket name (Needed for video storage)\n",
    "\n",
    "# --- Choose Backend Mode ---\n",
    "# Set USE_VERTEX to True to use the Vertex AI backend (requires ADC or service account auth).\n",
    "# Set USE_VERTEX to False to use the Gemini API backend (requires GEMINI_API_KEY).\n",
    "USE_VERTEX = False  # <-- CHANGE THIS TO True TO USE VERTEX AI\n",
    "\n",
    "# --- Gemini API Key (Only required if USE_VERTEX is False) ---\n",
    "# IMPORTANT: Replace with your actual Gemini API Key if USE_VERTEX is False.\n",
    "# Consider loading from environment variables (GOOGLE_API_KEY) or a secure secrets manager.\n",
    "GEMINI_API_KEY = \"\"  # Replace with your actual Gemini API Key\n",
    "\n",
    "\n",
    "# --- File Paths ---\n",
    "DATASET_CSV = \"dataset.csv\"               # Input dataset metadata from HuggingFace\n",
    "METADATA_FILE = \"video_metadata_vertex.csv\" if USE_VERTEX else \"video_metadata_non_vertex.csv\"      # Stores video info: video_id, local_path, gcs_uri (if Vertex), question data\n",
    "RESULTS_FILE = \"results_ccot_full_inference.csv\"              # Output file for inference predictions\n",
    "DOWNLOADS_DIR = \"downloads\"               # Directory for downloaded zip file\n",
    "EXTRACTED_VIDEOS_DIR = \"extracted_videos\" # Directory storing extracted .mp4 files locally\n",
    "SPEED_VIDEOS_DIR = \"speed_videos\"         # Stores sped up/slowed down videos\n",
    "HF_CACHE_DIR = \"./hf_cache\"               # Cache directory for HuggingFace datasets\n",
    "\n",
    "# --- Step 1: Fetch Dataset Configuration ---\n",
    "HF_DATASET_NAME = \"lmms-lab/AISG_Challenge\" # HuggingFace dataset identifier\n",
    "HF_DATASET_SPLIT = \"test\"                 # Dataset split to use\n",
    "SKIP_FETCH = False                        # Set True to skip fetching if DATASET_CSV exists\n",
    "\n",
    "# --- Step 2: Download & Prepare Videos Configuration ---\n",
    "VIDEO_ZIP_URL = \"https://huggingface.co/datasets/lmms-lab/AISG_Challenge/resolve/main/Benchmark-AllVideos-HQ-Encoded-challenge.zip?download=true\"\n",
    "ZIP_FILE_NAME = \"all_videos.zip\"\n",
    "SKIP_DOWNLOAD_ZIP = True                 # Set True to skip downloading if zip exists\n",
    "SKIP_EXTRACT = True                   # Set True to skip extraction if videos exist locally\n",
    "SKIP_PREPARE = True                    # Set True to skip video preparation (GCS upload for Vertex, metadata update)\n",
    "MAX_VIDEOS_TO_PROCESS = None              # Limit videos for testing (e.g., 5), None for all\n",
    "UPLOAD_BATCH_SIZE_GCS = 10                # Batch size for GCS uploads (Vertex mode only)\n",
    "\n",
    "# --- Inference Configuration ---\n",
    "# Choose a model name compatible with your selected method (Vertex AI or Gemini API)\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# Vertex AI: gemini-2.0-flash, gemini-2.0-flash-lite, gemini-2.0-pro-exp-02-05, gemini-2.0-flash-thinking-exp-01-21\n",
    "# Rate limits: https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#gemini-2.0-flash\n",
    "# Basically 500 requests per minute for 2.0-flash and 2.0-flash-lite (unlimited), 10 requests per minute for 2.0-pro-exp-02-05, gemini-2.5-flash-preview-04-17\n",
    "\n",
    "# Gemini API: gemini-2.0-flash, gemini-2.0-flash-lite, gemini-2.0-flash-thinking-exp-01-21, gemini-2.5-pro-exp-03-25\n",
    "# Rate limits: https://ai.google.dev/gemini-api/docs/rate-limits#tier-1\n",
    "# For free tier: 30 requests per minute for 2.0-flash and 2.0-flash-lite, 10 requests per minute for 2.0-pro-exp-02-05\n",
    "# For tier-1: 2000 requests per minute for 2.0-flash and 2.0-flash-lite (have to pay), 10 requests per minute for 2.0-pro-exp-02-05 and gemini-2.0-flash-thinking-exp-01-21, gemini-2.5-flash-preview-04-17\n",
    "\n",
    "# 1.0=normal speed, 0.5=half speed, etc.\n",
    "VIDEO_SPEED_FACTOR = 0.5\n",
    "\n",
    "# --- Setup Derived Paths & Directories ---\n",
    "zip_file_path = Path(DOWNLOADS_DIR) / ZIP_FILE_NAME\n",
    "extracted_videos_path = Path(EXTRACTED_VIDEOS_DIR)\n",
    "speed_videos_path = Path(SPEED_VIDEOS_DIR) / str(VIDEO_SPEED_FACTOR)\n",
    "Path(DOWNLOADS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "extracted_videos_path.mkdir(parents=True, exist_ok=True)\n",
    "speed_videos_path.mkdir(parents=True, exist_ok=True)\n",
    "Path(HF_CACHE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler(sys.stdout)])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Configuration Validation & Display --- #\n",
    "warnings_found = False\n",
    "if USE_VERTEX:\n",
    "    if not PROJECT_ID or PROJECT_ID == \"your-gcp-project-id\":\n",
    "        logger.error(\"Vertex AI mode requires PROJECT_ID to be set.\")\n",
    "        warnings_found = True\n",
    "    if not LOCATION:\n",
    "        logger.error(\"Vertex AI mode requires LOCATION to be set.\")\n",
    "        warnings_found = True\n",
    "    if not GCS_BUCKET or GCS_BUCKET == \"your-gcs-bucket-name\":\n",
    "        logger.error(\"Vertex AI mode requires GCS_BUCKET for video uploads.\")\n",
    "        warnings_found = True\n",
    "    if not GCS_AVAILABLE:\n",
    "        logger.error(\"Vertex AI mode requires 'google-cloud-storage', but it's not installed.\")\n",
    "        warnings_found = True\n",
    "else: # Gemini API Mode\n",
    "    # Check API Key (explicit or env var)\n",
    "    effective_api_key = GEMINI_API_KEY if GEMINI_API_KEY != \"YOUR_API_KEY_HERE\" else os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not effective_api_key:\n",
    "        logger.error(\"Gemini API mode requires GEMINI_API_KEY or GOOGLE_API_KEY environment variable.\")\n",
    "        warnings_found = True\n",
    "    else:\n",
    "        # Don't store the key in the config display if loaded from env\n",
    "        if GEMINI_API_KEY == \"YOUR_API_KEY_HERE\" and os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "            GEMINI_API_KEY = \"(Loaded from GOOGLE_API_KEY env var)\"\n",
    "        logger.info(\"Gemini API mode configured. Videos will be uploaded via File API.\")\n",
    "\n",
    "if warnings_found:\n",
    "     print(\"\\n\\n************************* WARNING *************************\")\n",
    "     print(\"Configuration errors detected above. Execution might fail.\")\n",
    "     print(\"***********************************************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d8e3e7",
   "metadata": {},
   "source": [
    "# Main Model Selection (Innovation 1)\n",
    "Agentic CoT - Chain of Thought + Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc4b1bf",
   "metadata": {},
   "source": [
    "## 1. Select CoT Model and Summary Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7caaf",
   "metadata": {},
   "source": [
    "### Generated Questions Modesl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c88bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS_MODEL_NAME = \"gemini-2.0-flash\"\n",
    "QUESTIONS_DIR = os.path.join(f\"generated_questions/{QUESTIONS_MODEL_NAME}\", \"questions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fdd5bf",
   "metadata": {},
   "source": [
    "### CoT output models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26644fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To See All Available Models, Go into the CoT_ouput_models.py file\n",
    "\n",
    "from models.CoT_ouput_models import get_cot_model\n",
    "\n",
    "CoT_model_list = [\"gemini-2.0-flash\",\"gemini-2.5-flash-preview-04-17\", \"gemini-2.5-pro-preview-03-25\"]\n",
    "\n",
    "MODEL_NAME, SYSTEM_PROMPT, PROMPT_TEMPLATES, CONFIG, REQUESTS_PER_MINUTE, MAX_RETRIES, MAX_ASYNC_WORKERS = get_cot_model(CoT_model_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d61f0c",
   "metadata": {},
   "source": [
    "### Summary Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616de83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see all available models, go into the Summary_models.py file\n",
    "from models.Summary_models import get_summary_model\n",
    "summary_model_list = [\"gemini-2.0-flash-ver1\", \"gemini-2.0-flash-ver2\", \"gemini-2.0-flash-ver3\"]\n",
    "\n",
    "QUESTION_MODEL_NAME, QUESTION_SYSTEM_PROMPT, QUESTION_CONFIG = get_summary_model(summary_model_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11065a9c",
   "metadata": {},
   "source": [
    "**Summary Model Prompt Versions (Gemini Flash) Comparison**\n",
    "\n",
    "Concise summary of different post-processing approaches for LLM outputs with `thinking` blocks.\n",
    "\n",
    "\n",
    "#### Version 1:  No Yap Model (Extractor Model)\n",
    "\n",
    "*   **Action:** Extracts final answer, minimal changes.\n",
    "*   **`thinking`:** Mostly ignored; extracts text after it.\n",
    "*   **Output:** Final answer only (For MCQ, letter only). \"No yap\".\n",
    "\n",
    "#### Version 2: Detailed Reformatter & Explainer\n",
    "\n",
    "*   **Action:** Answer first, then detailed *synthesized* explanation.\n",
    "*   **`thinking`:** Source for detailed step-by-step explanation; removes examiner notes.\n",
    "*   **Output:** `Answer \\n\\n Explanation: \\n [Detailed Synth. Explanation]`\n",
    "\n",
    "#### Version 3: Light Touch Reformatter & Preserver\n",
    "\n",
    "*   **Action:** Answer first, then *original* `thinking` block.\n",
    "*   **`thinking`:** Preserved in output (within ``` ```).\n",
    "*   **Output:** `Answer \\n\\n ```thinking\\n[Original Thinking]\\n``` `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc5d20",
   "metadata": {},
   "source": [
    "# Basic Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563fba21",
   "metadata": {},
   "source": [
    "## Initialize Google Cloud Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a54202",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = None\n",
    "ai_client = None\n",
    "\n",
    "# --- Initialize Generative AI Client (`google.genai`) --- #\n",
    "display(Markdown(\"### Initializing Generative AI Client (`google.genai`)\"))\n",
    "try:\n",
    "    if USE_VERTEX:\n",
    "        display(Markdown(f\"Vertex AI backend (Project: {PROJECT_ID}, Loc: {LOCATION})...\"))\n",
    "        if not PROJECT_ID or not LOCATION or PROJECT_ID == \"your-gcp-project-id\":\n",
    "             raise ValueError(\"PROJECT_ID/LOCATION invalid for Vertex AI.\")\n",
    "        # Initialize Client for Vertex\n",
    "        ai_client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
    "        display(Markdown(f\"✅ Vertex AI Client Initialized.\"))\n",
    "    else: # Gemini API Mode\n",
    "        display(Markdown(\"Gemini API backend (using API Key)...\"))\n",
    "        effective_api_key = GEMINI_API_KEY if GEMINI_API_KEY != \"YOUR_API_KEY_HERE\" else os.environ.get(\"GOOGLE_API_KEY\")\n",
    "        if not effective_api_key:\n",
    "             if os.environ.get(\"GOOGLE_API_KEY\"): effective_api_key = None # Client uses env var\n",
    "             else: raise ValueError(\"Gemini API Key required but not found.\")\n",
    "        # Initialize Client for Gemini API\n",
    "        ai_client = genai.Client(api_key=effective_api_key, vertexai=False)\n",
    "        display(Markdown(f\"✅ Gemini API Client Initialized.\"))\n",
    "\n",
    "except ValueError as ve: display(Markdown(f\"❌ **Config Error:** {ve}\")); ai_client = None\n",
    "except Exception as e: display(Markdown(f\"❌ **AI Client Error:** {e}.\")); logger.error(\"AI Client Init Failed\", exc_info=True); ai_client = None\n",
    "\n",
    "# --- Initialize Storage Client (ONLY for Vertex AI mode) --- #\n",
    "if USE_VERTEX:\n",
    "    display(Markdown(\"### Initializing GCS Client (Vertex Mode Only)\"))\n",
    "    if not GCS_AVAILABLE: display(Markdown(\"❌ GCS lib missing.\")); raise RuntimeError(\"Missing GCS lib.\")\n",
    "    if not GCS_BUCKET or GCS_BUCKET == \"your-gcs-bucket-name\": display(Markdown(\"❌ GCS_BUCKET needed.\")); raise ValueError(\"GCS_BUCKET required.\")\n",
    "    try:\n",
    "        storage_client = storage.Client(project=PROJECT_ID)\n",
    "        if not storage_client.bucket(GCS_BUCKET).exists(): display(Markdown(f\"⚠️ GCS Bucket `{GCS_BUCKET}` inaccessible.\"))\n",
    "        else: display(Markdown(f\"✅ GCS Client Initialized (Bucket: '{GCS_BUCKET}').\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **GCS Client Error:** {e}.\")); logger.error(\"GCS Client Init Failed\", exc_info=True)\n",
    "        if not SKIP_PREPARE: raise RuntimeError(\"GCS client failed.\")\n",
    "        else: display(Markdown(\"⚠️ GCS client failed, but skipping prep.\"))\n",
    "else:\n",
    "    display(Markdown(\"### Initializing Gemini API Client (File API)\"))\n",
    "    try:\n",
    "        storage_client = ai_client.files\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **Gemini File API Client Error:** {e}.\")); logger.error(\"Gemini API Client Init Failed\", exc_info=True)\n",
    "    display(Markdown(f\"✅ Gemini File API Client Initialized.\"))\n",
    "\n",
    "# --- Final Checks --- #\n",
    "if ai_client is None: raise RuntimeError(\"AI client failed.\")\n",
    "if USE_VERTEX and storage_client is None and not SKIP_PREPARE: raise RuntimeError(\"GCS client failed for Vertex prep.\")\n",
    "display(Markdown(\"✅ Client initialization complete.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6c267",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File/Data Handling ---\n",
    "def load_processed_qids(filename: str) -> Set[str]:\n",
    "    processed_qids = set()\n",
    "    if Path(filename).is_file():\n",
    "        try:\n",
    "            df = pd.read_csv(filename, usecols=['qid'], dtype={'qid': str}, on_bad_lines='warn')\n",
    "            processed_qids = set(df['qid'].dropna().unique())\n",
    "            logger.info(f\"Loaded {len(processed_qids)} processed QIDs from {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not read QIDs from {filename}: {e}. Assuming zero processed.\")\n",
    "    return processed_qids\n",
    "\n",
    "def download_file_with_progress(url: str, destination: Path):\n",
    "    logger.info(f\"Downloading {url} to {destination}...\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=600)\n",
    "        response.raise_for_status()\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024 * 1024\n",
    "        with open(destination, 'wb') as f, tqdm(\n",
    "            desc=f\"Downloading {destination.name}\", total=total_size, unit='iB', unit_scale=True, unit_divisor=1024\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                size = f.write(data)\n",
    "                bar.update(size)\n",
    "        if total_size != 0 and bar.n != total_size:\n",
    "            destination.unlink(missing_ok=True)\n",
    "            raise RuntimeError(f\"Download size mismatch for {destination.name}.\")\n",
    "        logger.info(f\"Successfully downloaded {destination}\")\n",
    "    except Exception as e:\n",
    "        destination.unlink(missing_ok=True)\n",
    "        logger.error(f\"Download failed for {url}: {e}\")\n",
    "        raise\n",
    "\n",
    "def extract_zip(zip_path: Path, extract_to: Path):\n",
    "    logger.info(f\"Extracting {zip_path.name} to {extract_to}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            members = [m for m in zip_ref.namelist() if not m.startswith('__MACOSX/') and not m.endswith('.DS_Store')]\n",
    "            with tqdm(total=len(members), desc=f\"Extracting {zip_path.name}\") as pbar:\n",
    "                for member in members:\n",
    "                    zip_ref.extract(member=member, path=extract_to)\n",
    "                    pbar.update(1)\n",
    "        logger.info(f\"Successfully extracted {zip_path} to {extract_to}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Extraction error: {e}\")\n",
    "        raise\n",
    "    \n",
    "def move_videos_to_main_directory(base_path):\n",
    "    \"\"\"Find all MP4 files in subdirectories and move them to the main directory.\"\"\"\n",
    "    logger.info(f\"Moving all videos to main directory: {base_path}\")\n",
    "    moved_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    # Find all MP4 files in subdirectories (but not in the main directory)\n",
    "    for file_path in list(base_path.glob('**/*.mp4')):\n",
    "        # Skip files already in the main directory or hidden Mac files\n",
    "        if file_path.parent == base_path or file_path.name.startswith('._'):\n",
    "            continue\n",
    "            \n",
    "        # Destination in the main directory\n",
    "        dest_path = base_path / file_path.name\n",
    "        \n",
    "        try:\n",
    "            # Move the file\n",
    "            shutil.move(str(file_path), str(dest_path))\n",
    "            moved_count += 1\n",
    "            if moved_count % 50 == 0:\n",
    "                logger.info(f\"Moved {moved_count} videos so far...\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error moving {file_path}: {e}\")\n",
    "            failed_count += 1\n",
    "    \n",
    "    logger.info(f\"Moved {moved_count} videos to main directory. Failed: {failed_count}\")\n",
    "    \n",
    "\n",
    "def create_or_update_metadata(metadata_path: str, dataset_df: pd.DataFrame, video_updates: Dict[str, Dict]):\n",
    "    try:\n",
    "        required_cols = ['video_id', 'qid']\n",
    "        update_cols = ['local_path', 'gcs_uri', 'file_api_name', 'status']\n",
    "        dtype_map = {'video_id': str, 'qid': str} # Ensure IDs are strings\n",
    "\n",
    "        if not Path(metadata_path).is_file():\n",
    "            logger.info(f\"Creating metadata file: {metadata_path}\")\n",
    "            meta_df = dataset_df.copy()\n",
    "            for col in update_cols: meta_df[col] = pd.NA\n",
    "            meta_df['status'] = 'pending'\n",
    "        else:\n",
    "            logger.debug(f\"Loading existing metadata: {metadata_path}\")\n",
    "            meta_df = pd.read_csv(metadata_path, dtype=dtype_map)\n",
    "            for col in update_cols: # Add missing update columns if needed\n",
    "                 if col not in meta_df.columns: meta_df[col] = pd.NA\n",
    "\n",
    "        if not all(col in meta_df.columns for col in required_cols):\n",
    "            raise ValueError(f\"Metadata missing required columns ({required_cols}).\")\n",
    "\n",
    "        updates_df = pd.DataFrame.from_dict(video_updates, orient='index')\n",
    "        updates_df.index.name = 'video_id'\n",
    "        updates_df.reset_index(inplace=True)\n",
    "        updates_df['video_id'] = updates_df['video_id'].astype(str)\n",
    "\n",
    "        # Use merge for robust updating across potentially multiple rows per video_id\n",
    "        # First, prepare updates DF with only the necessary columns (video_id + update_cols)\n",
    "        merge_cols = ['video_id'] + [col for col in update_cols if col in updates_df.columns]\n",
    "        updates_to_merge = updates_df[merge_cols].drop_duplicates(subset=['video_id'], keep='last')\n",
    "\n",
    "        # Merge, prioritizing updates\n",
    "        # Suffixes help identify original vs update cols if needed, but update will overwrite\n",
    "        merged_df = pd.merge(meta_df, updates_to_merge, on='video_id', how='left', suffixes=('', '_update'))\n",
    "\n",
    "        # Apply the updates\n",
    "        for col in update_cols:\n",
    "            update_col_name = col + '_update'\n",
    "            if update_col_name in merged_df.columns:\n",
    "                # Fill NAs in original col with update col, then drop update col\n",
    "                meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n",
    "                # Alternative: Directly update where update is not NA\n",
    "                # meta_df[col] = np.where(merged_df[update_col_name].notna(), merged_df[update_col_name], merged_df[col])\n",
    "\n",
    "        meta_df.to_csv(metadata_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Metadata file '{metadata_path}' updated with {len(video_updates)} video records.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating metadata {metadata_path}: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def load_metadata_for_inference(metadata_file: str = METADATA_FILE) -> Dict[str, List[Dict]]:\n",
    "    if not Path(metadata_file).is_file(): return {}\n",
    "    video_questions = defaultdict(list)\n",
    "    required_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "    try:\n",
    "        df = pd.read_csv(metadata_file, dtype=str).fillna('')\n",
    "        if 'video_id' not in df.columns or required_col not in df.columns:\n",
    "            logger.error(f\"Metadata missing 'video_id' or '{required_col}'.\")\n",
    "            return {}\n",
    "        valid_df = df[df['video_id'].astype(bool) & df[required_col].astype(bool)]\n",
    "        if len(valid_df) == 0:\n",
    "             logger.warning(f\"No videos found with '{required_col}' in {metadata_file}. Check Step 4.\")\n",
    "             return {}\n",
    "        for video_id, group in valid_df.groupby('video_id'):\n",
    "             video_questions[video_id] = group.to_dict('records')\n",
    "        logger.info(f\"Loaded {len(video_questions)} videos ({len(valid_df)} questions) with valid IDs for inference.\")\n",
    "        return dict(video_questions)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading metadata for inference: {e}\", exc_info=True)\n",
    "        return {}\n",
    "\n",
    "# --- Upload/Verification Helpers ---\n",
    "def upload_to_gcs(storage_client, bucket_name: str, source_file_path: Path, destination_blob_name: str) -> Optional[str]:\n",
    "    if not GCS_AVAILABLE or storage_client is None or not source_file_path.is_file(): return None\n",
    "    try:\n",
    "        blob = storage_client.bucket(bucket_name).blob(destination_blob_name)\n",
    "        blob.upload_from_filename(str(source_file_path))\n",
    "        gcs_uri = f\"gs://{bucket_name}/{destination_blob_name}\"\n",
    "        logger.debug(f\"GCS OK: {source_file_path} -> {gcs_uri}\")\n",
    "        return gcs_uri\n",
    "    except Exception as e:\n",
    "        logger.error(f\"GCS Fail: {source_file_path}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def upload_via_file_api(storage_client, local_path: Path, display_name: str) -> Optional[str]:\n",
    "    if storage_client is None or not local_path.is_file(): return None\n",
    "    try:\n",
    "        logger.debug(f\"Uploading {local_path} via File API...\")\n",
    "        uploaded_file = storage_client.upload(file=local_path)\n",
    "        logger.info(f\"File API OK: {local_path} -> {uploaded_file.name}\")\n",
    "        return uploaded_file.name\n",
    "    except Exception as e:\n",
    "        logger.error(f\"File API Fail: {local_path}. Error: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def verify_gcs_file_exists(storage_client, gcs_uri: str) -> bool:\n",
    "    if not GCS_AVAILABLE or storage_client is None or not gcs_uri: return False\n",
    "    try:\n",
    "        exists = storage.Blob.from_string(gcs_uri, client=storage_client).exists()\n",
    "        if not exists: logger.warning(f\"GCS verify failed: {gcs_uri}\")\n",
    "        return exists\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error verifying GCS {gcs_uri}: {e}\")\n",
    "        return False\n",
    "\n",
    "def verify_file_api_resource_exists(storage_client, file_api_name: str) -> bool:\n",
    "    if not storage_client or not file_api_name: return False\n",
    "    try:\n",
    "        _ = storage_client.get(name=file_api_name) # Sync get for verification\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error verifying File API {file_api_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def verify_local_file_exists(local_path: str) -> bool:\n",
    "    exists = Path(local_path).is_file() if local_path else False\n",
    "    if not exists: logger.warning(f\"Local verify failed: {local_path}\")\n",
    "    return exists\n",
    "\n",
    "# --- Prompt Building ---\n",
    "def build_prompt(question_info: dict) -> str:\n",
    "    question = question_info.get(\"question\", \"\")\n",
    "    q_type = question_info.get(\"question_type\", \"default\")\n",
    "    template = PROMPT_TEMPLATES.get(q_type, PROMPT_TEMPLATES[\"default\"])\n",
    "    # if q_type is MCQ\n",
    "    if q_type == \"Multiple-choice Question with a Single Correct Answer\":\n",
    "        return template.format(question=question).strip() + \"\\n\" + \"E. None of the above\"\n",
    "    return template.format(question=question).strip() + \"\\n\" + question_info.get(\"question_prompt\").strip()\n",
    "\n",
    "# --- Rate Limiter ---\n",
    "class AsyncRateLimiter:\n",
    "    \"\"\"\n",
    "    An asyncio-compatible token bucket rate limiter.\n",
    "\n",
    "    Args:\n",
    "        rate (int): The maximum number of requests allowed per period.\n",
    "        period (float): The time period in seconds (default: 60 for RPM).\n",
    "        capacity (int, optional): The maximum burst capacity. Defaults to `rate`.\n",
    "    \"\"\"\n",
    "    def __init__(self, rate: int, period: float = 60.0, capacity: Optional[int] = None):\n",
    "        if rate <= 0:\n",
    "            raise ValueError(\"Rate must be positive\")\n",
    "        if period <= 0:\n",
    "            raise ValueError(\"Period must be positive\")\n",
    "\n",
    "        self.rate = rate\n",
    "        self.period = float(period)\n",
    "        self.capacity = float(capacity if capacity is not None else rate)\n",
    "        self._tokens = self.capacity # Start full\n",
    "        self._last_refill_time = time.monotonic()\n",
    "        self._lock = asyncio.Lock()\n",
    "\n",
    "    def _get_tokens_per_second(self) -> float:\n",
    "        return self.rate / self.period\n",
    "\n",
    "    async def _refill(self):\n",
    "        \"\"\"Replenishes tokens based on elapsed time. Must be called under lock.\"\"\"\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self._last_refill_time\n",
    "        if elapsed > 0:\n",
    "            tokens_to_add = elapsed * self._get_tokens_per_second()\n",
    "            self._tokens = min(self.capacity, self._tokens + tokens_to_add)\n",
    "            self._last_refill_time = now\n",
    "\n",
    "    async def acquire(self):\n",
    "        \"\"\"\n",
    "        Acquires a token, waiting if necessary.\n",
    "        \"\"\"\n",
    "        async with self._lock:\n",
    "            await self._refill() # Refill based on time since last acquire/refill\n",
    "\n",
    "            while self._tokens < 1:\n",
    "                # Calculate how long to wait for 1 token\n",
    "                tokens_needed = 1.0 - self._tokens\n",
    "                wait_time = tokens_needed / self._get_tokens_per_second()\n",
    "\n",
    "                # Release the lock before sleeping\n",
    "                lock_released = True\n",
    "                try:\n",
    "                    self._lock.release()\n",
    "                    logger.debug(f\"Rate limit hit. Waiting for {wait_time:.3f}s for next token.\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                finally:\n",
    "                    # Re-acquire the lock if it was released\n",
    "                    if lock_released:\n",
    "                        await self._lock.acquire()\n",
    "\n",
    "                # Refill again after waiting, as more time has passed\n",
    "                await self._refill()\n",
    "\n",
    "            # Consume a token\n",
    "            self._tokens -= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79b7f8",
   "metadata": {},
   "source": [
    "# Download, Extract & Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5cee60",
   "metadata": {},
   "source": [
    "## Fetch Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(DATASET_CSV)\n",
    "\n",
    "if dataset_path.is_file() and SKIP_FETCH:\n",
    "    logger.info(f\"Dataset file '{DATASET_CSV}' exists and SKIP_FETCH is True. Skipping.\")\n",
    "    display(Markdown(f\"✅ Skipping fetch: Found existing `{DATASET_CSV}`.\"))\n",
    "    # Load the existing dataframe for use in Step 2\n",
    "    try:\n",
    "        dataset_df = pd.read_csv(dataset_path, dtype=str) # Load all as string initially\n",
    "        logger.info(f\"Loaded existing dataset from {DATASET_CSV} ({len(dataset_df)} rows).\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load existing dataset file {DATASET_CSV}: {e}\")\n",
    "        display(Markdown(f\"❌ Error loading existing `{DATASET_CSV}`: {e}. Please delete the file or set SKIP_FETCH=False.\"))\n",
    "        raise\n",
    "else:\n",
    "    logger.info(f\"Fetching dataset '{HF_DATASET_NAME}' (split: '{HF_DATASET_SPLIT}') from HuggingFace...\")\n",
    "    try:\n",
    "        dataset = load_dataset(HF_DATASET_NAME, split=HF_DATASET_SPLIT, cache_dir=HF_CACHE_DIR)\n",
    "        dataset_df = dataset.to_pandas()\n",
    "        # Ensure key columns are strings\n",
    "        for col in ['qid', 'video_id', 'question', 'question_type']:\n",
    "             if col in dataset_df.columns:\n",
    "                 dataset_df[col] = dataset_df[col].astype(str)\n",
    "        dataset_df.to_csv(dataset_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Successfully fetched dataset and saved to {DATASET_CSV} ({len(dataset_df)} rows).\")\n",
    "        display(Markdown(f\"✅ Dataset fetched and saved to `{DATASET_CSV}` ({len(dataset_df)} rows).\"))\n",
    "        display(dataset_df.head())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch or save dataset: {e}\", exc_info=True)\n",
    "        display(Markdown(f\"❌ **Error fetching dataset:** {e}. Check connection, dataset name/split, cache dir permissions.\"))\n",
    "        raise RuntimeError(\"Dataset fetching failed. Cannot continue.\")\n",
    "\n",
    "# Ensure dataset_df is loaded if skipping fetch didn't load it (e.g., first run with skip=True and no file)\n",
    "if 'dataset_df' not in locals():\n",
    "    if dataset_path.is_file():\n",
    "        try:\n",
    "            dataset_df = pd.read_csv(dataset_path, dtype=str)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Critical error: Could not load dataset from {DATASET_CSV} after attempting fetch/skip: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise RuntimeError(f\"Critical error: Dataset DataFrame not loaded and file {DATASET_CSV} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fab26",
   "metadata": {},
   "source": [
    "## Download, Extract, and Prepare Videos\n",
    "\n",
    "Downloads, extracts, and uploads videos (to GCS or File API). Updates `video_metadata.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c752de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(DATASET_CSV)\n",
    "dataset_df = pd.read_csv(dataset_path, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826fbc54",
   "metadata": {},
   "source": [
    "### Download Video Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Downloading Archive\"))\n",
    "if zip_file_path.is_file() and SKIP_DOWNLOAD_ZIP:\n",
    "    display(Markdown(f\"✅ Skipping download: Found `{zip_file_path}`.\"))\n",
    "else:\n",
    "    try: download_file_with_progress(VIDEO_ZIP_URL, zip_file_path); display(Markdown(f\"✅ Downloaded: `{zip_file_path}`.\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **Download Error:** {e}.\"))\n",
    "        if not SKIP_EXTRACT or not SKIP_PREPARE: raise RuntimeError(f\"Download failed.\")\n",
    "        else: display(Markdown(\"⚠️ Download failed, skipping steps.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d8ad5",
   "metadata": {},
   "source": [
    "### Extract Video Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2055cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Extracting Archive\"))\n",
    "if any(extracted_videos_path.glob('*.mp4')) and SKIP_EXTRACT:\n",
    "    display(Markdown(f\"✅ Skipping extraction: Files in `{extracted_videos_path}`.\"))\n",
    "elif not zip_file_path.is_file():\n",
    "    display(Markdown(f\"❌ Cannot extract: `{zip_file_path}` missing.\"))\n",
    "    if not SKIP_PREPARE: raise RuntimeError(f\"Zip missing.\")\n",
    "    else: display(Markdown(\"⚠️ Extraction skipped (no zip).\"))\n",
    "else:\n",
    "    try: \n",
    "        extract_zip(zip_file_path, extracted_videos_path)\n",
    "        # Move all videos to main directory\n",
    "        move_videos_to_main_directory(extracted_videos_path)\n",
    "        display(Markdown(f\"✅ Extracted to `{extracted_videos_path}` and moved all videos to main directory.\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **Extraction Error:** {e}.\"))\n",
    "        if not SKIP_PREPARE: raise RuntimeError(\"Extraction failed.\")\n",
    "        else: display(Markdown(\"⚠️ Extraction failed, skipping prep.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca34116",
   "metadata": {},
   "source": [
    "### Slow/Speed Up Videos\n",
    "Losslessly change video speed while also re-encoding audio to maintain pitch. As\n",
    "a result, is super fast. Could be made faster if using asyncio to concurrently run\n",
    "ffmpeg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_subprocess(cmd, check=True, capture_output=False):\n",
    "    \"\"\"Helper function to run subprocess asynchronously.\"\"\"\n",
    "    stdout_pipe = asyncio.subprocess.PIPE if capture_output else asyncio.subprocess.DEVNULL\n",
    "    # Capture stderr only if check is True or capture_output is True, otherwise DEVNULL\n",
    "    stderr_pipe = asyncio.subprocess.PIPE if check or capture_output else asyncio.subprocess.DEVNULL\n",
    "\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        *cmd,\n",
    "        stdout=stdout_pipe,\n",
    "        stderr=stderr_pipe\n",
    "    )\n",
    "    stdout, stderr = await process.communicate()\n",
    "\n",
    "    if check and process.returncode != 0:\n",
    "        error_msg = f\"Command '{' '.join(cmd)}' failed with return code {process.returncode}\"\n",
    "        stderr_decoded = stderr.decode(errors='ignore') if stderr else \"\"\n",
    "        if stderr_decoded:\n",
    "            error_msg += f\"\\nStderr: {stderr_decoded}\"\n",
    "        # Raise specific exception to potentially capture stderr later\n",
    "        raise subprocess.CalledProcessError(process.returncode, cmd, output=stdout, stderr=stderr)\n",
    "\n",
    "    return stdout, stderr, process.returncode\n",
    "\n",
    "async def process_single_video(vid_path, speed_videos_path, VIDEO_SPEED_FACTOR, semaphore):\n",
    "    \"\"\"Asynchronously processes a single video. Returns status string.\"\"\"\n",
    "    vid_path_str = str(vid_path.resolve())\n",
    "    out_path = speed_videos_path / vid_path.name\n",
    "    out_path_str = str(out_path.resolve())\n",
    "\n",
    "    async with semaphore: # Limit concurrency\n",
    "        if out_path.is_file():\n",
    "            return 'skipped'\n",
    "\n",
    "        if VIDEO_SPEED_FACTOR == 1.0:\n",
    "            try:\n",
    "                # Use asyncio.to_thread for potentially blocking I/O\n",
    "                await asyncio.to_thread(shutil.copy, vid_path_str, out_path_str)\n",
    "                return 'processed'\n",
    "            except Exception as e:\n",
    "                try:\n",
    "                    logger.error(f\"Error copying {vid_path.name}: {e}\")\n",
    "                except NameError:\n",
    "                    print(f\"Error copying {vid_path.name}: {e}\")\n",
    "                return 'error'\n",
    "\n",
    "        # --- Process video with speed change ---\n",
    "        tf_bitstream_path = None\n",
    "        tf_audio_path = None\n",
    "        tf_final_path = None\n",
    "        try:\n",
    "            # Create temporary files (synchronous part is okay here)\n",
    "            # Context manager ensures files are closed before ffmpeg uses them\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".h264\") as tf_b, \\\n",
    "                 tempfile.NamedTemporaryFile(delete=False, suffix=\".aac\") as tf_a, \\\n",
    "                 tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as tf_f:\n",
    "                tf_bitstream_name = tf_b.name\n",
    "                tf_audio_name = tf_a.name\n",
    "                tf_final_name = tf_f.name\n",
    "            # Store paths for cleanup\n",
    "            tf_bitstream_path = Path(tf_bitstream_name)\n",
    "            tf_audio_path = Path(tf_audio_name)\n",
    "            tf_final_path = Path(tf_final_name)\n",
    "\n",
    "\n",
    "            # Get original FPS\n",
    "            ffprobe_cmd = [\n",
    "                \"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"v\", \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n",
    "                \"-show_entries\", \"stream=r_frame_rate\", vid_path_str\n",
    "            ]\n",
    "            stdout, _, _ = await run_subprocess(ffprobe_cmd, check=True, capture_output=True)\n",
    "            fps = float(fractions.Fraction(stdout.decode().strip()))\n",
    "            new_fps = fps * VIDEO_SPEED_FACTOR\n",
    "\n",
    "            # Extract and speed up audio\n",
    "            factor = VIDEO_SPEED_FACTOR\n",
    "            filter_parts = []\n",
    "            while factor > 2.0:\n",
    "                filter_parts.append(\"atempo=2.0\")\n",
    "                factor /= 2.0\n",
    "            while factor < 0.5:\n",
    "                filter_parts.append(\"atempo=0.5\")\n",
    "                factor /= 0.5\n",
    "            if abs(factor - 1.0) > 1e-6:\n",
    "                 filter_parts.append(f\"atempo={factor:.6f}\")\n",
    "\n",
    "            if not filter_parts:\n",
    "                 audio_cmd = [\"ffmpeg\", \"-y\", \"-i\", vid_path_str, \"-vn\", \"-c:a\", \"copy\", tf_audio_name]\n",
    "            else:\n",
    "                audio_filter = \",\".join(filter_parts)\n",
    "                audio_cmd = [\"ffmpeg\", \"-y\", \"-i\", vid_path_str, \"-vn\", \"-filter:a\", audio_filter, \"-c:a\", \"aac\", \"-b:a\", \"128k\", tf_audio_name]\n",
    "            await run_subprocess(audio_cmd, check=True)\n",
    "\n",
    "\n",
    "            # Extract h264 bitstream\n",
    "            extract_cmd = [\"ffmpeg\", \"-y\", \"-i\", vid_path_str, \"-map\", \"0:v\", \"-c:v\", \"copy\", \"-bsf:v\", \"h264_mp4toannexb\", tf_bitstream_name]\n",
    "            await run_subprocess(extract_cmd, check=True)\n",
    "\n",
    "            # Remux bitstream with new audio and FPS\n",
    "            remux_cmd = [\"ffmpeg\", \"-y\", \"-fflags\", \"+genpts\", \"-r\", f\"{new_fps:.6f}\", \"-i\", tf_bitstream_name, \"-i\", tf_audio_name, \"-map\", \"0:v\", \"-map\", \"1:a\", \"-c:v\", \"copy\", \"-c:a\", \"copy\", tf_final_name]\n",
    "            await run_subprocess(remux_cmd, check=True)\n",
    "\n",
    "            # Move final file (use asyncio.to_thread)\n",
    "            await asyncio.to_thread(shutil.move, tf_final_name, out_path_str)\n",
    "            return 'processed'\n",
    "\n",
    "        except Exception as e:\n",
    "            err_msg = f\"Error processing {vid_path.name}: {e}\"\n",
    "            # Include ffmpeg stderr if available\n",
    "            if isinstance(e, subprocess.CalledProcessError) and e.stderr:\n",
    "                 err_msg += f\"\\nFFmpeg/FFprobe Stderr:\\n{e.stderr.decode(errors='ignore')}\"\n",
    "            try:\n",
    "                logger.error(err_msg)\n",
    "            except NameError:\n",
    "                print(err_msg)\n",
    "            return 'error'\n",
    "        finally:\n",
    "            # Clean up temporary files asynchronously using to_thread\n",
    "            async def _cleanup():\n",
    "                if tf_bitstream_path and tf_bitstream_path.exists():\n",
    "                    tf_bitstream_path.unlink(missing_ok=True)\n",
    "                if tf_audio_path and tf_audio_path.exists():\n",
    "                    tf_audio_path.unlink(missing_ok=True)\n",
    "                # tf_final is moved, only delete if error occurred before move\n",
    "                if tf_final_path and tf_final_path.exists():\n",
    "                    tf_final_path.unlink(missing_ok=True)\n",
    "            # Run sync cleanup in thread only if paths were assigned\n",
    "            if tf_bitstream_path or tf_audio_path or tf_final_path:\n",
    "                 await asyncio.to_thread(_cleanup)\n",
    "\n",
    "\n",
    "# --- Main Cell Logic ---\n",
    "\n",
    "async def run_processing(): # Wrap in an async function to use await\n",
    "    display(Markdown(\"### Preparing Videos\"))\n",
    "    if dataset_df is None:\n",
    "        raise RuntimeError(\"Dataset DF unavailable.\")\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    speed_videos_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_video_ids = sorted(list(dataset_df['video_id'].dropna().unique()))\n",
    "    # Use logging if available, otherwise print\n",
    "    try:\n",
    "        logger.info(f\"Processing {len(all_video_ids)} unique video IDs.\")\n",
    "    except NameError:\n",
    "        print(f\"Processing {len(all_video_ids)} unique video IDs.\")\n",
    "\n",
    "    vid_paths = list(extracted_videos_path.glob(\"*.mp4\"))\n",
    "\n",
    "    # Limit concurrency\n",
    "    concurrency_limit = MAX_ASYNC_WORKERS\n",
    "    try:\n",
    "        logger.info(f\"Using concurrency limit: {concurrency_limit}\")\n",
    "    except NameError:\n",
    "        print(f\"Using concurrency limit: {concurrency_limit}\")\n",
    "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
    "\n",
    "    tasks = []\n",
    "    # Keep the familiar loop structure for creating tasks\n",
    "    print(f\"Preparing tasks for {len(vid_paths)} videos...\")\n",
    "    for vid_path in vid_paths:\n",
    "         # Create a task for each video processing job\n",
    "         # Pass necessary arguments to the task creator\n",
    "         task = asyncio.create_task(process_single_video(vid_path, speed_videos_path, VIDEO_SPEED_FACTOR, semaphore))\n",
    "         tasks.append(task)\n",
    "\n",
    "    # Now, run all the created tasks concurrently and display progress\n",
    "    # Use asyncio.as_completed with a standard tqdm progress bar\n",
    "    print(f\"Transforming {len(tasks)} Videos...\")\n",
    "    results = []\n",
    "    # Use the imported tqdm (now tqdm.auto) to create a standard progress bar instance\n",
    "    with tqdm(total=len(tasks), desc=\"Transforming Videos\", unit=\"video\") as pbar:\n",
    "        for future in asyncio.as_completed(tasks):\n",
    "            try:\n",
    "                result = await future # Get result from completed task\n",
    "                results.append(result)\n",
    "            except Exception as exc:\n",
    "                # Log errors from tasks that failed internally if not caught by process_single_video\n",
    "                # (process_single_video should ideally return 'error' status instead of raising)\n",
    "                try:\n",
    "                    logger.error(f\"Task for a video failed: {exc}\")\n",
    "                except NameError:\n",
    "                    print(f\"Task for a video failed: {exc}\")\n",
    "                results.append('error') # Count as error if task itself fails unexpectedly\n",
    "            finally:\n",
    "                 pbar.update(1) # Increment progress bar regardless of outcome\n",
    "\n",
    "\n",
    "    # Count results\n",
    "    processed = results.count('processed')\n",
    "    skipped = results.count('skipped')\n",
    "    errors = results.count('error')\n",
    "\n",
    "    print(f\"\\n\\n{skipped} videos skipped, {processed} videos processed, {errors} errors, {len(vid_paths)} total.\")\n",
    "\n",
    "# --- Execute the async processing ---\n",
    "# In a Jupyter Notebook, you usually need to await the top-level async function.\n",
    "# If top-level await isn't enabled, you might need nest_asyncio or run manually.\n",
    "# Using await directly is the most common way in modern notebooks.\n",
    "await run_processing()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92696e74",
   "metadata": {},
   "source": [
    "### Preparing and Upload Videos to GCS or File API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare Videos (Upload GCS/File API) & Update Metadata --- #\n",
    "#-- Can set SKIP_PREPARE to True to skip this step if you already uploaded it into bucket through Vertex AI Mode/ into files through Gemini API key --#\n",
    "display(Markdown(\"### Preparing Videos & Updating Metadata\"))\n",
    "if SKIP_PREPARE:\n",
    "    display(Markdown(\"✅ Skipping video preparation.\"))\n",
    "elif storage_client is None:\n",
    "     display(Markdown(\"❌ Cannot prepare: Client not ready.\")); raise RuntimeError(\"Client missing.\")\n",
    "else:\n",
    "    if dataset_df is None: raise RuntimeError(\"Dataset DF unavailable.\")\n",
    "    all_video_ids = sorted(list(dataset_df['video_id'].dropna().unique()))\n",
    "    logger.info(f\"Processing {len(all_video_ids)} unique video IDs.\")\n",
    "\n",
    "    videos_to_process_ids = all_video_ids\n",
    "    if MAX_VIDEOS_TO_PROCESS is not None:\n",
    "        videos_to_process_ids = all_video_ids[:MAX_VIDEOS_TO_PROCESS]\n",
    "        logger.info(f\"Limiting to {len(videos_to_process_ids)} videos.\")\n",
    "\n",
    "    # Load existing metadata to check status\n",
    "    existing_statuses = {}\n",
    "    resource_ids = {}\n",
    "    required_id_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "    if Path(METADATA_FILE).is_file():\n",
    "        try:\n",
    "            existing_df = pd.read_csv(METADATA_FILE, dtype=str)\n",
    "            if 'video_id' in existing_df.columns and 'status' in existing_df.columns:\n",
    "                existing_statuses = pd.Series(existing_df.status.values, index=existing_df.video_id).to_dict()\n",
    "            if 'video_id' in existing_df.columns and required_id_col in existing_df.columns:\n",
    "                 resource_ids = pd.Series(existing_df[required_id_col].values, index=existing_df.video_id).dropna().to_dict()\n",
    "            logger.info(\"Checked existing metadata statuses/IDs.\")\n",
    "        except Exception as e: logger.warning(f\"Could not load existing metadata: {e}\")\n",
    "\n",
    "    video_metadata_updates = {}\n",
    "    processed_count, upload_failures, missing_local, skipped_count = 0, 0, 0, 0\n",
    "    num_batches = math.ceil(len(videos_to_process_ids) / UPLOAD_BATCH_SIZE_GCS)\n",
    "    prep_mode = \"GCS Upload\" if USE_VERTEX else \"File API Upload\"\n",
    "\n",
    "    with tqdm(total=len(videos_to_process_ids), desc=f\"Preparing ({prep_mode})\") as pbar:\n",
    "        for i in range(0, len(videos_to_process_ids), UPLOAD_BATCH_SIZE_GCS):\n",
    "            batch_ids = videos_to_process_ids[i : i + UPLOAD_BATCH_SIZE_GCS]\n",
    "            batch_num = (i // UPLOAD_BATCH_SIZE_GCS) + 1\n",
    "            logger.info(f\"Prep Batch {batch_num}/{num_batches}...\")\n",
    "            current_batch_updates = {}\n",
    "\n",
    "            for video_id in batch_ids:\n",
    "                pbar.set_postfix_str(f\"ID: {video_id}\")\n",
    "                update_data = {\"local_path\": None, \"gcs_uri\": None, \"file_api_name\": None, \"status\": \"error_unknown\"}\n",
    "                local_video_path = speed_videos_path / f\"{video_id}.mp4\"\n",
    "                current_status = existing_statuses.get(video_id, 'pending')\n",
    "                existing_resource_id = resource_ids.get(video_id)\n",
    "                is_already_processed = False\n",
    "\n",
    "                # Check if already uploaded and verified\n",
    "                if current_status in ['uploaded_gcs', 'uploaded_file_api'] and existing_resource_id:\n",
    "                     verified = False\n",
    "                     if USE_VERTEX: verified = verify_gcs_file_exists(storage_client, existing_resource_id)\n",
    "                     else: verified = verify_file_api_resource_exists(storage_client, existing_resource_id)\n",
    "                     if verified:\n",
    "                         logger.debug(f\"Skipping verified video {video_id} ('{current_status}').\")\n",
    "                         is_already_processed = True\n",
    "                         skipped_count += 1\n",
    "                         update_data.update({ # Ensure metadata is consistent\n",
    "                             'local_path': str(local_video_path) if local_video_path.is_file() else None,\n",
    "                             'status': current_status,\n",
    "                             required_id_col: existing_resource_id\n",
    "                         })\n",
    "                     else:\n",
    "                         logger.warning(f\"Video {video_id} ({current_status}) needs re-processing (verification failed).\")\n",
    "                elif current_status != 'pending':\n",
    "                     logger.debug(f\"Video {video_id} has non-pending status '{current_status}' but no verified resource ID. Re-processing.\")\n",
    "\n",
    "                if is_already_processed:\n",
    "                    processed_count += 1\n",
    "                    current_batch_updates[video_id] = update_data\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Process if needed\n",
    "                if local_video_path.is_file():\n",
    "                    update_data[\"local_path\"] = str(local_video_path)\n",
    "                    resource_id_result = None\n",
    "                    if USE_VERTEX:\n",
    "                        blob_name = f\"videos/{video_id}.mp4\"\n",
    "                        resource_id_result = upload_to_gcs(storage_client, GCS_BUCKET, local_video_path, blob_name)\n",
    "                        if resource_id_result: update_data.update({\"gcs_uri\": resource_id_result, \"status\": \"uploaded_gcs\"})\n",
    "                        else: update_data[\"status\"] = \"gcs_upload_failed\"; upload_failures += 1\n",
    "                    else: # Gemini API\n",
    "                        resource_id_result = upload_via_file_api(storage_client, local_video_path, f\"vid_{video_id}\")\n",
    "                        if resource_id_result: update_data.update({\"file_api_name\": resource_id_result, \"status\": \"uploaded_file_api\"})\n",
    "                        else: update_data[\"status\"] = \"file_api_upload_failed\"; upload_failures += 1\n",
    "                else:\n",
    "                    logger.warning(f\"Local file missing: {local_video_path}\")\n",
    "                    missing_local += 1\n",
    "                    update_data[\"status\"] = \"local_missing\"\n",
    "\n",
    "                current_batch_updates[video_id] = update_data\n",
    "                processed_count += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Update metadata after batch\n",
    "            if current_batch_updates:\n",
    "                 try: create_or_update_metadata(METADATA_FILE, dataset_df, current_batch_updates)\n",
    "                 except Exception as e: logger.error(f\"Metadata update failed batch {batch_num}: {e}\")\n",
    "                 video_metadata_updates.update(current_batch_updates)\n",
    "\n",
    "    logger.info(f\"Prep finished. Checked: {processed_count}, Skipped(verified): {skipped_count}, Missing Local: {missing_local}, Upload Failures: {upload_failures}\")\n",
    "    display(Markdown(f\"✅ Video preparation complete. See logs. Metadata: `{METADATA_FILE}`.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6cd69d",
   "metadata": {},
   "source": [
    "# Bulk Inference \n",
    "## (Agentic) (Turn by turn) (Generated Questions) Bulk inference CoCoT reasoning with summary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ac68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FINAL_DIR = os.path.join(f\"all_results/full_inference_CoCoT_generated_questions/{MODEL_NAME}\", RESULTS_FILE)\n",
    "os.makedirs(os.path.dirname(RESULTS_FINAL_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21403c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWERS_DIR = f\"generated_questions/{QUESTIONS_MODEL_NAME}/chat_history\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c86296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_chat(chat_json):\n",
    "    return [\n",
    "        types.Content(\n",
    "            role=msg[\"role\"],\n",
    "            parts=[types.Part.from_text(text=msg[\"parts\"][0])]\n",
    "        ) for msg in chat_json\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da74e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference Functions ---\n",
    "\n",
    "async def perform_inference_single_async(\n",
    "    question_info: Dict,\n",
    "    client: Any,\n",
    "    semaphore: asyncio.Semaphore,\n",
    "    rate_limiter: Optional[AsyncRateLimiter],\n",
    "    results_queue: asyncio.Queue\n",
    ") -> None: # Return None as result is put in queue\n",
    "    \"\"\"\n",
    "    Async inference for one question, putting the result into a queue.\n",
    "    \"\"\"\n",
    "    qid = question_info.get(\"qid\", \"?\")\n",
    "    video_id = question_info.get(\"video_id\", \"?\")\n",
    "    prompt_text = build_prompt(question_info)\n",
    "    gcs_uri = question_info.get(\"gcs_uri\")\n",
    "    file_api_name = question_info.get(\"file_api_name\")\n",
    "    start_time = time.time()\n",
    "    result = None # Default result in case of early exit\n",
    "    \n",
    "    chat_path = os.path.join(ANSWERS_DIR,video_id+\".json\")\n",
    "    if not os.path.isfile(chat_path):\n",
    "        logger.error(f\"QID {qid} (Async): Chat file not found for video ID {video_id}.\")\n",
    "        result = {\"qid\": qid, \"pred\": f\"ERROR: Chat file not found for video ID {video_id}.\", \"duration\": 0, \"finish_reason\": \"N/A\", \"status\": \"Failed (Chat File Not Found)\"}\n",
    "        await results_queue.put(result)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Load chat history with generated questions from JSON file\n",
    "    with open(chat_path) as f:\n",
    "        chat_json = json.load(f)\n",
    "        chat = deserialize_chat(chat_json)\n",
    "\n",
    "    # --- Prepare Inputs (Same as before) ---\n",
    "    try:\n",
    "        video_part = None\n",
    "        if USE_VERTEX:\n",
    "            # ... (GCS URI logic) ...\n",
    "            if not gcs_uri: raise ValueError(\"Missing GCS URI.\")\n",
    "            video_part = types.Part.from_uri(mime_type='video/mp4', file_uri=gcs_uri)\n",
    "        else: # Gemini API Mode\n",
    "            # ... (File API get logic) ...\n",
    "             if not file_api_name: raise ValueError(\"Missing File API name.\")\n",
    "             try:\n",
    "                 file_object = await client.aio.files.get(name=file_api_name)\n",
    "                 video_part = file_object\n",
    "             except genai_errors.NotFoundError: raise FileNotFoundError(f\"File API '{file_api_name}' not found.\")\n",
    "             except Exception as e: raise RuntimeError(f\"Failed get File API obj: {e}\")\n",
    "        \n",
    "        user_msg = types.Content(role=\"user\",\n",
    "                            parts=[types.Part.from_text(text=prompt_text)])\n",
    "\n",
    "        # Merge video part, chat history, and user message\n",
    "        if video_part is None: raise RuntimeError(\"Video part preparation failed.\")\n",
    "        contents = [video_part] + chat + [user_msg]\n",
    "\n",
    "    except (ValueError, FileNotFoundError, RuntimeError) as e:\n",
    "        logger.error(f\"QID {qid} (Async): Input Error - {e}\")\n",
    "        # Put error result in queue\n",
    "        result = {\"qid\": qid, \"pred\": f\"ERROR: Input Fail - {e}\", \"duration\": 0, \"finish_reason\": \"N/A\", \"status\": \"Failed (Input)\"}\n",
    "        await results_queue.put(result)\n",
    "        return # Exit the function\n",
    "    except Exception as e:\n",
    "         logger.error(f\"QID {qid} (Async): Unexpected Input Prep Error: {e}\", exc_info=True)\n",
    "         result = {\"qid\": qid, \"pred\": f\"ERROR: Input Prep Failed Unexpectedly - {e}\", \"duration\": 0, \"finish_reason\": \"N/A\", \"status\": \"Failed (Input Prep)\"}\n",
    "         await results_queue.put(result)\n",
    "         return # Exit the function\n",
    "\n",
    "    # --- Perform Inference with Retries, Semaphore, and Rate Limiting ---\n",
    "    async with semaphore:\n",
    "        for attempt in range(MAX_RETRIES + 1):\n",
    "            try:\n",
    "                if rate_limiter: await rate_limiter.acquire()\n",
    "                api_start = time.time()\n",
    "                logger.debug(f\"QID {qid} (Async): Attempt {attempt + 1} sending request...\")\n",
    "                response = await client.aio.models.generate_content(\n",
    "                    model=MODEL_NAME,\n",
    "                    contents=contents,\n",
    "                    config=CONFIG\n",
    "                )\n",
    "\n",
    "                # Process Response\n",
    "                answer, reason, status, err_detail = \"ERROR\", \"UNKNOWN\", \"Success\", \"\"\n",
    "                try:\n",
    "                    answer = response.text.strip()\n",
    "                    if response.candidates and hasattr(response.candidates[0], 'finish_reason') and response.candidates[0].finish_reason is not None:\n",
    "                        reason = response.candidates[0].finish_reason.name\n",
    "                except ValueError as ve:\n",
    "                    status, err_detail = \"Blocked/Empty\", f\"ValueError: {ve}. \"\n",
    "                    # ... (block/safety reason extraction) ...\n",
    "                    answer = f\"ERROR: {status}. {err_detail}\"\n",
    "\n",
    "                result = {\"qid\": qid, \"pred\": answer, \"duration\": time.time()-start_time, \"finish_reason\": reason, \"status\": status}\n",
    "                await results_queue.put(result)\n",
    "                logger.debug(f\"QID {qid} (Async): Attempt {attempt + 1} {status} ({time.time()-api_start:.2f}s API / {time.time()-start_time:.2f}s Total). Result queued.\")\n",
    "                return # Exit function after success\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                 result = {\"qid\": qid, \"pred\": f\"ERROR: - {e}\", \"duration\": time.time()-start_time, \"finish_reason\": reason, \"status\": \"Failed (Unexpected Error)\"}\n",
    "                 await results_queue.put(result)\n",
    "                 return\n",
    "\n",
    "        # Fallback (Should not be reached if logic above is correct)\n",
    "        logger.error(f\"QID {qid} (Async): Exited retry loop unexpectedly.\")\n",
    "        result = {\"qid\": qid, \"pred\": \"ERROR: Unknown after retries\", \"duration\": time.time()-start_time, \"finish_reason\": \"UNKNOWN\", \"status\": \"Failed (Unknown)\"}\n",
    "        await results_queue.put(result)\n",
    "\n",
    "async def results_writer_task(\n",
    "    queue: asyncio.Queue,\n",
    "    filename: str,\n",
    "    write_batch_size: int = 20, # How many results to buffer before writing\n",
    "    write_interval_sec: float = 10.0 # Max time between writes\n",
    "):\n",
    "    \"\"\"Gets results from queue and writes them to CSV in batches.\"\"\"\n",
    "    results_buffer = []\n",
    "    last_write_time = time.monotonic()\n",
    "    # Define expected header based on the dict keys put in the queue\n",
    "    fieldnames = [\"qid\", \"pred\", \"status\", \"duration_sec\", \"finish_reason\"]\n",
    "    file_exists = Path(filename).is_file()\n",
    "\n",
    "    logger.info(f\"Writer task started. Writing results to {filename}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Wait for an item with a timeout\n",
    "            result = await asyncio.wait_for(queue.get(), timeout=write_interval_sec)\n",
    "\n",
    "            if result is None: # Signal to terminate\n",
    "                logger.info(\"Writer task received termination signal.\")\n",
    "                break\n",
    "\n",
    "            if isinstance(result, dict):\n",
    "                 # Ensure duration is rounded here before adding to buffer\n",
    "                 result[\"duration_sec\"] = round(result.get(\"duration\", -1), 2)\n",
    "                 # Remove the original 'duration' key if desired\n",
    "                 result.pop('duration', None)\n",
    "                 results_buffer.append(result)\n",
    "            else:\n",
    "                logger.warning(f\"Writer task received non-dict item: {result}\")\n",
    "\n",
    "            queue.task_done() # Signal that the item was processed\n",
    "\n",
    "        except asyncio.TimeoutError:\n",
    "            # Timeout occurred, write buffer if not empty, even if batch size not reached\n",
    "            logger.debug(\"Writer task timeout reached.\")\n",
    "            pass # Continue to buffer check below\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Writer task encountered error getting from queue: {e}\", exc_info=True)\n",
    "            # Decide if this is fatal or if we should try to continue\n",
    "            await asyncio.sleep(1) # Prevent fast spinning on error\n",
    "            continue # Try to continue processing\n",
    "\n",
    "        # Check if we should write the buffer\n",
    "        buffer_size = len(results_buffer)\n",
    "        time_since_last_write = time.monotonic() - last_write_time\n",
    "        should_write = (\n",
    "             buffer_size > 0 and\n",
    "             (buffer_size >= write_batch_size or time_since_last_write >= write_interval_sec)\n",
    "        )\n",
    "\n",
    "        if should_write:\n",
    "            logger.info(f\"Writing batch of {buffer_size} results to {filename}...\")\n",
    "            try:\n",
    "                # Use 'with open' for proper handling\n",
    "                with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                    # Write header only if file didn't exist at the start\n",
    "                    if not file_exists:\n",
    "                        writer.writeheader()\n",
    "                        file_exists = True # Prevent writing header again\n",
    "                    writer.writerows(results_buffer)\n",
    "\n",
    "                results_buffer = [] # Clear buffer after successful write\n",
    "                last_write_time = time.monotonic()\n",
    "                logger.info(f\"Batch written successfully.\")\n",
    "            except IOError as e:\n",
    "                logger.error(f\"IOError writing results batch to {filename}: {e}. Results may be lost.\")\n",
    "                # Optional: Add retry logic here, or store failed batch elsewhere\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error writing results batch: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "    # --- Cleanup: Write any remaining items after receiving None signal ---\n",
    "    if results_buffer:\n",
    "        logger.info(f\"Writing final remaining {len(results_buffer)} results...\")\n",
    "        try:\n",
    "            with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                if not file_exists: writer.writeheader() # Check again in case file was deleted mid-run\n",
    "                writer.writerows(results_buffer)\n",
    "            logger.info(\"Final results written.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error writing final results batch: {e}\")\n",
    "\n",
    "    logger.info(\"Writer task finished.\")\n",
    "\n",
    "# --- Modified Main Execution Function with UI ---\n",
    "async def run_bulk_inference_async():\n",
    "    \"\"\"Runs the bulk inference process asynchronously with queue writer, rate limiting, and UI.\"\"\"\n",
    "\n",
    "    # --- Standard Setup ---\n",
    "    if ai_client is None: logger.error(\"AI Client not initialized.\"); display(Markdown(\"❌ AI Client not initialized.\")); return\n",
    "\n",
    "    # --- Load Data and Prepare Tasks ---\n",
    "    logger.info(\"Loading metadata for inference...\")\n",
    "    video_questions_for_inference = load_metadata_for_inference(METADATA_FILE)\n",
    "    if not video_questions_for_inference: logger.warning(\"No video data ready.\"); return\n",
    "    processed_qids = load_processed_qids(RESULTS_FINAL_DIR)\n",
    "\n",
    "    inference_tasks_input = []\n",
    "    tasks_skipped = 0\n",
    "    required_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "    for video_id, questions in video_questions_for_inference.items():\n",
    "        for question_info in questions:\n",
    "             qid = question_info.get('qid')\n",
    "             if not qid or qid in processed_qids or not question_info.get(required_col):\n",
    "                 tasks_skipped += 1; continue\n",
    "             inference_tasks_input.append(question_info)\n",
    "\n",
    "    total_tasks = len(inference_tasks_input)\n",
    "    logger.info(f\"Prepared {total_tasks} new inference tasks. Skipped {tasks_skipped}.\" )\n",
    "    if total_tasks == 0: logger.info(\"No new questions to process.\"); return\n",
    "\n",
    "    # --- Setup Rate Limiter ---\n",
    "    rate_limiter = None\n",
    "    if REQUESTS_PER_MINUTE is not None and REQUESTS_PER_MINUTE > 0:\n",
    "        capacity = 10 # Example: Keep burst capacity moderate\n",
    "        rate_limiter = AsyncRateLimiter(rate=REQUESTS_PER_MINUTE, period=60.0, capacity=capacity)\n",
    "        logger.info(f\"Rate limiting enabled: {REQUESTS_PER_MINUTE} RPM, Capacity: {capacity}\")\n",
    "    else:\n",
    "        logger.info(\"Rate limiting disabled.\")\n",
    "\n",
    "    # --- Create Queue and Start Writer Task ---\n",
    "    results_queue = asyncio.Queue()\n",
    "    writer_handle = asyncio.create_task(\n",
    "        results_writer_task(results_queue, RESULTS_FINAL_DIR)\n",
    "    )\n",
    "\n",
    "    # --- Schedule and Run Inference Tasks with Progress Bar ---\n",
    "    logger.info(f\"Starting async inference for {total_tasks} questions (Concurrency: {MAX_ASYNC_WORKERS})...\")\n",
    "    semaphore = asyncio.Semaphore(MAX_ASYNC_WORKERS)\n",
    "    start_bulk_time = time.time()\n",
    "\n",
    "    # Create coroutines\n",
    "    inference_coroutines = [\n",
    "        perform_inference_single_async(q_info, ai_client, semaphore, rate_limiter, results_queue)\n",
    "        for q_info in inference_tasks_input\n",
    "    ]\n",
    "\n",
    "    # --- Use asyncio.as_completed with tqdm_notebook for live progress ---\n",
    "    completed_count = 0\n",
    "    gather_exception = None\n",
    "    try:\n",
    "        # Create a future for each coroutine to track completion\n",
    "        tasks = [asyncio.ensure_future(coro) for coro in inference_coroutines]\n",
    "        # Use tqdm with as_completed\n",
    "        for future in tqdm(asyncio.as_completed(tasks), total=total_tasks, desc=\"Async Inference\"):\n",
    "            try:\n",
    "                await future # Wait for the next task to complete, raise exception if task failed\n",
    "                completed_count += 1\n",
    "            except Exception as task_exc:\n",
    "                 # Log error from individual task if it wasn't caught inside\n",
    "                 logger.error(f\"Error surfaced from an inference task: {task_exc}\", exc_info=False)\n",
    "                 # Optionally decide if you want to stop processing other tasks\n",
    "                 # gather_exception = task_exc # Store first exception\n",
    "                 # break # Or continue processing others\n",
    "    except Exception as outer_exc:\n",
    "        # Catch errors during the setup/iteration of as_completed itself\n",
    "        logger.error(f\"Error during as_completed processing: {outer_exc}\", exc_info=True)\n",
    "        gather_exception = outer_exc\n",
    "\n",
    "    # Ensure all tasks are awaited even if we broke early due to an error in one task\n",
    "    # (This might be redundant if as_completed handles cancellation correctly, but safer)\n",
    "    await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    bulk_duration = time.time() - start_bulk_time\n",
    "    logger.info(f\"Async inference task processing finished in {bulk_duration:.2f} seconds. Completed: {completed_count}/{total_tasks}.\")\n",
    "    if gather_exception:\n",
    "         logger.error(f\"Bulk inference encountered errors: {gather_exception}\")\n",
    "\n",
    "    # --- Signal Writer Task to Finish (ALWAYS DO THIS) ---\n",
    "    logger.info(\"Signaling writer task to complete...\")\n",
    "    await results_queue.put(None)\n",
    "\n",
    "    # --- Wait for Writer Task to Finish (ALWAYS DO THIS) ---\n",
    "    logger.info(\"Waiting for writer task to finish writing remaining results...\")\n",
    "    try:\n",
    "        await writer_handle # Wait until the writer processes the None signal and exits\n",
    "        logger.info(\"Writer task has finished.\")\n",
    "    except Exception as writer_exc:\n",
    "        logger.error(f\"Error waiting for writer task: {writer_exc}\", exc_info=True)\n",
    "\n",
    "    # --- Final Summary ---\n",
    "    logger.info(f\"Bulk inference process complete. See logs and {RESULTS_FINAL_DIR}.\")\n",
    "    # Remove handler to prevent duplicate logs on re-run\n",
    "\n",
    "# --- Run the async function ---\n",
    "asyncio.run(run_bulk_inference_async())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec2cc9c",
   "metadata": {},
   "source": [
    "Check with the clean up again after finishing the Bulk Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47abf5",
   "metadata": {},
   "source": [
    "## Interim Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26add203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interim_cleanup_and_report():\n",
    "    \"\"\"Final cleanup and report of results.\"\"\"\n",
    "\n",
    "    try:\n",
    "        results_df = pd.read_csv(RESULTS_FINAL_DIR, dtype=str)\n",
    "        logger.info(f\"Loaded results from {RESULTS_FINAL_DIR} ({len(results_df)} rows).\")\n",
    "        \n",
    "        # Correctly identify failed entries\n",
    "        failed = results_df[results_df['CoT'].isna() | (results_df['status'] != 'Success')]\n",
    "        \n",
    "        # Success is everything not in failed\n",
    "        success = results_df[~results_df.index.isin(failed.index)]\n",
    "\n",
    "        display(Markdown(f\"### Results Summary: {len(success)} Success, {len(failed)} Failed.\"))\n",
    "\n",
    "        # Sort by 'qid'\n",
    "        results_df.sort_values(by=['qid'], inplace=True)\n",
    "\n",
    "        # remove duplicates based on 'qid' and keep the first occurrence\n",
    "        results_df.drop_duplicates(subset=['qid'], keep='first', inplace=True)\n",
    "        logger.info(f\"Removed duplicates, remaining {len(results_df)} unique QIDs.\")\n",
    "\n",
    "        # Save the cleaned-up results to a new file with datetime suffix\n",
    "        original_filename = Path(RESULTS_FINAL_DIR).stem\n",
    "        # Get the directory of the original file\n",
    "        original_dir = Path(RESULTS_FINAL_DIR).parent\n",
    "        # Create the new file names\n",
    "        success_filename = f\"{original_filename}.csv\"\n",
    "        failed_filename = f\"{original_filename}_failed.csv\"\n",
    "        # Create the full paths for the new files\n",
    "        success_filename = original_dir / success_filename\n",
    "        failed_filename = original_dir / failed_filename\n",
    "        # Save the success and failed DataFrames to separate CSV files\n",
    "        success.to_csv(success_filename, index=False, encoding='utf-8')\n",
    "        failed.to_csv(failed_filename, index=False, encoding='utf-8')\n",
    "     \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing results file: {e}\", exc_info=True)\n",
    "        display(Markdown(f\"❌ Error processing results file: {e}.\"))\n",
    "        return\n",
    "\n",
    "interim_cleanup_and_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d89e511",
   "metadata": {},
   "source": [
    "# Cleanup and reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2845acd",
   "metadata": {},
   "source": [
    "### Report With Summary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_OUTPUT_CSV = RESULTS_FINAL_DIR.replace(\".csv\", \"_cot_summary.csv\")\n",
    "\n",
    "\n",
    "def results_cleanup_and_report():\n",
    "    \"\"\"Final cleanup and report of results.\"\"\"\n",
    "    if not Path(RESULTS_FINAL_DIR).is_file(): display(Markdown(f\"❌ Results file `{RESULTS_FINAL_DIR}` not found.\")); return\n",
    "\n",
    "    try:\n",
    "        results_df = pd.read_csv(RESULTS_FINAL_DIR, dtype=str)\n",
    "        logger.info(f\"Loaded results from {RESULTS_FINAL_DIR} ({len(results_df)} rows).\")\n",
    "        \n",
    "        success = results_df[results_df['status'] == 'Success']\n",
    "        failed = results_df[results_df['status'] != 'Success']\n",
    "        logger.info(f\"Results Summary: {len(success)} Success, {len(failed)} Failed.\")\n",
    "        display(Markdown(f\"### Results Summary: {len(success)} Success, {len(failed)} Failed.\"))\n",
    "\n",
    "        avg_duration = results_df['duration_sec'].astype(float).mean()\n",
    "        logger.info(f\"Average duration for successful tasks: {avg_duration:.2f} seconds.\")\n",
    "        display(Markdown(f\"### Average Duration: {avg_duration:.2f} seconds.\"))\n",
    "\n",
    "        # Sort by 'qid'\n",
    "        results_df.sort_values(by=['qid'], inplace=True)\n",
    "\n",
    "        # remove duplicates based on 'qid' and keep the first occurrence\n",
    "        results_df.drop_duplicates(subset=['qid'], keep='first', inplace=True)\n",
    "        logger.info(f\"Removed duplicates, remaining {len(results_df)} unique QIDs.\")\n",
    "\n",
    "        # remove all unnecessary columns\n",
    "        columns_to_keep = ['qid', 'pred']\n",
    "        results_df = results_df[columns_to_keep]\n",
    "\n",
    "        new_name = SUMMARY_OUTPUT_CSV.replace('.csv', \"_\") +  time.strftime(\"%Y%m%d_%H%M%S\") + \".csv\"\n",
    "        # Save the cleaned DataFrame to a CSV file\n",
    "        results_df.to_csv(new_name, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Cleaned results saved to {new_name}.\")\n",
    "        display(Markdown(f\"✅ Cleaned results saved to `{new_name}`.\"))\n",
    "     \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing results file: {e}\", exc_info=True)\n",
    "        display(Markdown(f\"❌ Error processing results file: {e}.\"))\n",
    "        return\n",
    "  \n",
    "results_cleanup_and_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956479e",
   "metadata": {},
   "source": [
    "### Getting COT Thinking Process Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a43841",
   "metadata": {},
   "outputs": [],
   "source": [
    "COT_ONLY_OUTPUT_CSV = RESULTS_FINAL_DIR.replace(\".csv\", \"_cot_only.csv\")\n",
    "\n",
    "def results_cleanup_and_report():\n",
    "    \"\"\"Final cleanup and report of results.\"\"\"\n",
    "    if not Path(RESULTS_FINAL_DIR).is_file(): display(Markdown(f\"❌ Results file `{RESULTS_FINAL_DIR}` not found.\")); return\n",
    "\n",
    "    try:\n",
    "        results_df = pd.read_csv(RESULTS_FINAL_DIR, dtype=str)\n",
    "        logger.info(f\"Loaded results from {RESULTS_FINAL_DIR} ({len(results_df)} rows).\")\n",
    "        \n",
    "        success = results_df[results_df['status'] == 'Success']\n",
    "        failed = results_df[results_df['status'] != 'Success']\n",
    "        logger.info(f\"Results Summary: {len(success)} Success, {len(failed)} Failed.\")\n",
    "        display(Markdown(f\"### Results Summary: {len(success)} Success, {len(failed)} Failed.\"))\n",
    "\n",
    "        avg_duration = results_df['duration_sec'].astype(float).mean()\n",
    "        logger.info(f\"Average duration for successful tasks: {avg_duration:.2f} seconds.\")\n",
    "        display(Markdown(f\"### Average Duration: {avg_duration:.2f} seconds.\"))\n",
    "\n",
    "        # Sort by 'qid'\n",
    "        results_df.sort_values(by=['qid'], inplace=True)\n",
    "\n",
    "        # remove duplicates based on 'qid' and keep the first occurrence\n",
    "        results_df.drop_duplicates(subset=['qid'], keep='first', inplace=True)\n",
    "        logger.info(f\"Removed duplicates, remaining {len(results_df)} unique QIDs.\")\n",
    "\n",
    "        # select only the relevant columns\n",
    "        results_df = results_df[['qid', 'CoT']]\n",
    "\n",
    "        # Rename column\n",
    "        results_df.rename(columns={'CoT': 'pred'}, inplace=True)\n",
    "\n",
    "        new_name = COT_ONLY_OUTPUT_CSV.replace('.csv', \"_\") +  time.strftime(\"%Y%m%d_%H%M%S\") + \".csv\"\n",
    "    \n",
    "        # Save the cleaned DataFrame to a CSV file\n",
    "        results_df.to_csv(new_name, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Cleaned results saved to {new_name}.\")\n",
    "        display(Markdown(f\"✅ Cleaned results saved to `{new_name}`.\"))\n",
    "     \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing results file: {e}\", exc_info=True)\n",
    "        display(Markdown(f\"❌ Error processing results file: {e}.\"))\n",
    "        return\n",
    "  \n",
    "results_cleanup_and_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f961d7",
   "metadata": {},
   "source": [
    "### Delete original results file if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e124e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(RESULTS_FINAL_DIR) # Remove the original results file\n",
    "os.remove(RESULTS_FINAL_DIR.replace(\".csv\", \"_failed.csv\")) # Remove the failed results file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
