{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9874197",
   "metadata": {},
   "source": [
    "#  Vertex Inference Unified\n",
    "\n",
    "This notebook uses the unified `google-genai` library (imported as `from google import genai`). It supports:\n",
    "- **Vertex AI Backend:** Uploads videos to GCS during the 'Prepare' step. Using your own Cloud Account\n",
    "- **Gemini API Backend:** Uploads videos using the **File API** during the 'Prepare' step. Using API key\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1.  **Import Libraries & Configure.**\n",
    "2.  **Config** - Set up the configuration for the pipeline, including the model, model config, prompts, and prompt config.\n",
    "2.  **Initialize Clients:** Set up AI client and Storage client.\n",
    "3.  **(Only the first time) Fetch Dataset:** Downloads metadata from HuggingFace.\n",
    "4.  **(Only the first time on each API type) Download, Extract & Prepare Videos:** Downloads, extracts, uploads (GCS/File API). Updates metadata.\n",
    "5.  **Bulk Inference (Async):** Performs inference using pre-uploaded video resources using baseline models (Gemini 2.5 Pro/ Gemini 2.5 Flash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19484ef",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "1. After switching from Vertex to Gemini and vice versa, be sure to follow the steps:\n",
    "    - Run all cells in order to re-upload the videos to the correct storage client, you can enable the SKIP_DOWNLOAD and SKIP_EXTRACT flags to skip the download and extraction steps. Only the upload step is needed\n",
    "\n",
    "2. Gemini API's file client has a expiry time of 1 day or so for the uploaded files. You may need to follow the steps above to re-upload the files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c07e40",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55d7dd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`google.genai` SDK and helpers imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports (Corrected for `google.genai`)\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import datetime\n",
    "import zipfile\n",
    "import math\n",
    "import sys\n",
    "import asyncio\n",
    "from typing import Dict, List, Optional, Set, Tuple, Any\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import fractions\n",
    "\n",
    "# Google Cloud & AI Libraries (Unified SDK)\n",
    "try:\n",
    "    import google.genai as genai\n",
    "    from google.genai import types\n",
    "    from google.genai import errors as genai_errors\n",
    "    from google.api_core import exceptions as api_core_exceptions\n",
    "    # GCS Client (Optional, for Vertex Mode)\n",
    "    try:\n",
    "        from google.cloud import storage\n",
    "        GCS_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"INFO: google-cloud-storage not found. Vertex AI GCS operations unavailable.\")\n",
    "        storage = None\n",
    "        GCS_AVAILABLE = False\n",
    "    print(\"`google.genai` SDK and helpers imported successfully.\")\n",
    "except ImportError as e:\n",
    "     print(f\"ERROR: Failed to import Google libraries: {e}. Install: pip install google-genai google-api-core google-cloud-storage\")\n",
    "     genai = None; types = None; genai_errors = None; api_core_exceptions = None\n",
    "     storage = None; GCS_AVAILABLE = False\n",
    "     raise ImportError(\"FATAL: `google.genai` or `google-api-core` SDK not found.\")\n",
    "\n",
    "# Data Handling & Progress\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# UI Elements\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "\n",
    "# Async in Notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbce404",
   "metadata": {},
   "source": [
    "## Config Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d5e7cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:28:47,460 - INFO - Gemini API mode configured. Videos will be uploaded via File API.\n"
     ]
    }
   ],
   "source": [
    "# --- GCP Configuration ---\n",
    "\n",
    "# PROJECT_ID = \"YOUR_PROJECT_ID_HERE\" # Your Google Cloud Project ID (Needed for GCS and Vertex AI mode)\n",
    "# LOCATION = \"us-central1\"      # Your Google Cloud Region (Needed for Vertex AI mode)\n",
    "# GCS_BUCKET = \"YOUR_GCS_BUCKET_HERE\" # Your GCS bucket name (Needed for video storage)\n",
    "\n",
    "PROJECT_ID = \"tiktokllm\" # Your Google Cloud Project ID (Needed for GCS and Vertex AI mode)\n",
    "LOCATION = \"us-central1\"      # Your Google Cloud Region (Needed for Vertex AI mode)\n",
    "GCS_BUCKET = \"seekdeepr4-ml-storage\" # Your GCS bucket name (Needed for video storage)\n",
    "\n",
    "# --- Choose Backend Mode ---\n",
    "# Set USE_VERTEX to True to use the Vertex AI backend (requires ADC or service account auth).\n",
    "# Set USE_VERTEX to False to use the Gemini API backend (requires GEMINI_API_KEY).\n",
    "USE_VERTEX = False  # <-- CHANGE THIS TO True TO USE VERTEX AI\n",
    "\n",
    "# --- Gemini API Key (Only required if USE_VERTEX is False) ---\n",
    "# IMPORTANT: Replace with your actual Gemini API Key if USE_VERTEX is False.\n",
    "# Consider loading from environment variables (GOOGLE_API_KEY) or a secure secrets manager.\n",
    "GEMINI_API_KEY = \"AIzaSyCtTXQV55XJqZRQb8hXUsFFfI3dJHK_5nE\"  # Replace with your actual Gemini API Key\n",
    "\n",
    "\n",
    "# --- File Paths ---\n",
    "DATASET_CSV = \"dataset.csv\"               # Input dataset metadata from HuggingFace\n",
    "METADATA_FILE = \"video_metadata_vertex.csv\" if USE_VERTEX else \"video_metadata_non_vertex.csv\"      # Stores video info: video_id, local_path, gcs_uri (if Vertex), question data\n",
    "RESULTS_FILE = \"results_noncot_full_inference.csv\"              # Output file for inference predictions\n",
    "DOWNLOADS_DIR = \"downloads\"               # Directory for downloaded zip file\n",
    "EXTRACTED_VIDEOS_DIR = \"extracted_videos\" # Directory storing extracted .mp4 files locally\n",
    "SPEED_VIDEOS_DIR = \"speed_videos\"         # Stores sped up/slowed down videos\n",
    "HF_CACHE_DIR = \"./hf_cache\"               # Cache directory for HuggingFace datasets\n",
    "\n",
    "# --- Step 1: Fetch Dataset Configuration ---\n",
    "HF_DATASET_NAME = \"lmms-lab/AISG_Challenge\" # HuggingFace dataset identifier\n",
    "HF_DATASET_SPLIT = \"test\"                 # Dataset split to use\n",
    "SKIP_FETCH = False                        # Set True to skip fetching if DATASET_CSV exists\n",
    "\n",
    "# --- Step 2: Download & Prepare Videos Configuration ---\n",
    "VIDEO_ZIP_URL = \"https://huggingface.co/datasets/lmms-lab/AISG_Challenge/resolve/main/Benchmark-AllVideos-HQ-Encoded-challenge.zip?download=true\"\n",
    "ZIP_FILE_NAME = \"all_videos.zip\"\n",
    "SKIP_DOWNLOAD_ZIP = True                 # Set True to skip downloading if zip exists\n",
    "SKIP_EXTRACT = True                      # Set True to skip extraction if videos exist locally\n",
    "SKIP_PREPARE = True                     # Set True to skip video preparation (GCS upload for Vertex, metadata update)\n",
    "MAX_VIDEOS_TO_PROCESS = None              # Limit videos for testing (e.g., 5), None for all\n",
    "UPLOAD_BATCH_SIZE_GCS = 10                # Batch size for GCS uploads (Vertex mode only)\n",
    "\n",
    "# --- Inference Configuration ---\n",
    "# Choose a model name compatible with your selected method (Vertex AI or Gemini API)\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# Vertex AI: gemini-2.0-flash, gemini-2.0-flash-lite, gemini-2.0-pro-exp-02-05, gemini-2.0-flash-thinking-exp-01-21\n",
    "# Rate limits: https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#gemini-2.0-flash\n",
    "# Basically 500 requests per minute for 2.0-flash and 2.0-flash-lite (unlimited), 10 requests per minute for 2.0-pro-exp-02-05, gemini-2.5-flash-preview-04-17\n",
    "\n",
    "# Gemini API: gemini-2.0-flash, gemini-2.0-flash-lite, gemini-2.0-flash-thinking-exp-01-21, gemini-2.5-pro-exp-03-25\n",
    "# Rate limits: https://ai.google.dev/gemini-api/docs/rate-limits#tier-1\n",
    "# For free tier: 30 requests per minute for 2.0-flash and 2.0-flash-lite, 10 requests per minute for 2.0-pro-exp-02-05\n",
    "# For tier-1: 2000 requests per minute for 2.0-flash and 2.0-flash-lite (have to pay), 10 requests per minute for 2.0-pro-exp-02-05 and gemini-2.0-flash-thinking-exp-01-21, gemini-2.5-flash-preview-04-17\n",
    "\n",
    "# 1.0=normal speed, 0.5=half speed, etc.\n",
    "VIDEO_SPEED_FACTOR = 0.5\n",
    "\n",
    "# --- Setup Derived Paths & Directories ---\n",
    "zip_file_path = Path(DOWNLOADS_DIR) / ZIP_FILE_NAME\n",
    "extracted_videos_path = Path(EXTRACTED_VIDEOS_DIR)\n",
    "speed_videos_path = Path(SPEED_VIDEOS_DIR) / str(VIDEO_SPEED_FACTOR)\n",
    "Path(DOWNLOADS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "extracted_videos_path.mkdir(parents=True, exist_ok=True)\n",
    "speed_videos_path.mkdir(parents=True, exist_ok=True)\n",
    "Path(HF_CACHE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler(sys.stdout)])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Configuration Validation & Display --- #\n",
    "warnings_found = False\n",
    "if USE_VERTEX:\n",
    "    if not PROJECT_ID or PROJECT_ID == \"your-gcp-project-id\":\n",
    "        logger.error(\"Vertex AI mode requires PROJECT_ID to be set.\")\n",
    "        warnings_found = True\n",
    "    if not LOCATION:\n",
    "        logger.error(\"Vertex AI mode requires LOCATION to be set.\")\n",
    "        warnings_found = True\n",
    "    if not GCS_BUCKET or GCS_BUCKET == \"your-gcs-bucket-name\":\n",
    "        logger.error(\"Vertex AI mode requires GCS_BUCKET for video uploads.\")\n",
    "        warnings_found = True\n",
    "    if not GCS_AVAILABLE:\n",
    "        logger.error(\"Vertex AI mode requires 'google-cloud-storage', but it's not installed.\")\n",
    "        warnings_found = True\n",
    "else: # Gemini API Mode\n",
    "    # Check API Key (explicit or env var)\n",
    "    effective_api_key = GEMINI_API_KEY if GEMINI_API_KEY != \"YOUR_API_KEY_HERE\" else os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not effective_api_key:\n",
    "        logger.error(\"Gemini API mode requires GEMINI_API_KEY or GOOGLE_API_KEY environment variable.\")\n",
    "        warnings_found = True\n",
    "    else:\n",
    "        # Don't store the key in the config display if loaded from env\n",
    "        if GEMINI_API_KEY == \"YOUR_API_KEY_HERE\" and os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "            GEMINI_API_KEY = \"(Loaded from GOOGLE_API_KEY env var)\"\n",
    "        logger.info(\"Gemini API mode configured. Videos will be uploaded via File API.\")\n",
    "\n",
    "if warnings_found:\n",
    "     print(\"\\n\\n************************* WARNING *************************\")\n",
    "     print(\"Configuration errors detected above. Execution might fail.\")\n",
    "     print(\"***********************************************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d8e3e7",
   "metadata": {},
   "source": [
    "# Main Model Selection For Bulk Inference Generated Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406aa4d",
   "metadata": {},
   "source": [
    "### Generating Questions Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a566765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To See All Available Models, Go into the Generating_Questions_models.py file\n",
    "# Models are used to generate the questions\n",
    "from models.Generating_Questions_models import get_brainstorm_prompt\n",
    "CoT_model_list = [\"gemini-2.0-flash\",\"gemini-2.5-pro-preview-05-06\"]\n",
    "QUESTIONS_MODEL_NAME, QUESTIONS_PROMPT, QUESTIONS_SCHEMA, QUESTIONS_CONFIG, QUESTIONS_REQUESTS_PER_MINUTE, QUESTIONS_MAX_RETRIES, QUESTIONS_MAX_ASYNC_WORKERS = get_brainstorm_prompt(CoT_model_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420dc6f",
   "metadata": {},
   "source": [
    "### COT output models for Bulk Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dfdd5a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To See All Available Models, Go into the CoT_ouput_models.py file\n",
    "# CoT models are used for Bulk Inference\n",
    "from models.CoT_ouput_models import get_cot_model\n",
    "CoT_model_list = [\"gemini-2.0-flash\",\"gemini-2.5-pro-preview-05-06\", \"gemini-2.5-pro-preview-03-25\"]\n",
    "MODEL_NAME, SYSTEM_PROMPT, PROMPT_TEMPLATES, CONFIG, REQUESTS_PER_MINUTE, MAX_RETRIES, MAX_ASYNC_WORKERS = get_cot_model(CoT_model_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc5d20",
   "metadata": {},
   "source": [
    "# Basic Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563fba21",
   "metadata": {},
   "source": [
    "## Initialize Google Cloud Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65a54202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Initializing Generative AI Client (`google.genai`)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Gemini API backend (using API Key)..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✅ Gemini API Client Initialized."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Initializing Gemini API Client (File API)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✅ Gemini File API Client Initialized."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✅ Client initialization complete."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "storage_client = None\n",
    "ai_client = None\n",
    "\n",
    "# --- Initialize Generative AI Client (`google.genai`) --- #\n",
    "display(Markdown(\"### Initializing Generative AI Client (`google.genai`)\"))\n",
    "try:\n",
    "    if USE_VERTEX:\n",
    "        display(Markdown(f\"Vertex AI backend (Project: {PROJECT_ID}, Loc: {LOCATION})...\"))\n",
    "        if not PROJECT_ID or not LOCATION or PROJECT_ID == \"your-gcp-project-id\":\n",
    "             raise ValueError(\"PROJECT_ID/LOCATION invalid for Vertex AI.\")\n",
    "        # Initialize Client for Vertex\n",
    "        ai_client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
    "        display(Markdown(f\"✅ Vertex AI Client Initialized.\"))\n",
    "    else: # Gemini API Mode\n",
    "        display(Markdown(\"Gemini API backend (using API Key)...\"))\n",
    "        effective_api_key = GEMINI_API_KEY if GEMINI_API_KEY != \"YOUR_API_KEY_HERE\" else os.environ.get(\"GOOGLE_API_KEY\")\n",
    "        if not effective_api_key:\n",
    "             if os.environ.get(\"GOOGLE_API_KEY\"): effective_api_key = None # Client uses env var\n",
    "             else: raise ValueError(\"Gemini API Key required but not found.\")\n",
    "        # Initialize Client for Gemini API\n",
    "        ai_client = genai.Client(api_key=effective_api_key, vertexai=False)\n",
    "        display(Markdown(f\"✅ Gemini API Client Initialized.\"))\n",
    "\n",
    "except ValueError as ve: display(Markdown(f\"❌ **Config Error:** {ve}\")); ai_client = None\n",
    "except Exception as e: display(Markdown(f\"❌ **AI Client Error:** {e}.\")); logger.error(\"AI Client Init Failed\", exc_info=True); ai_client = None\n",
    "\n",
    "# --- Initialize Storage Client (ONLY for Vertex AI mode) --- #\n",
    "if USE_VERTEX:\n",
    "    display(Markdown(\"### Initializing GCS Client (Vertex Mode Only)\"))\n",
    "    if not GCS_AVAILABLE: display(Markdown(\"❌ GCS lib missing.\")); raise RuntimeError(\"Missing GCS lib.\")\n",
    "    if not GCS_BUCKET or GCS_BUCKET == \"your-gcs-bucket-name\": display(Markdown(\"❌ GCS_BUCKET needed.\")); raise ValueError(\"GCS_BUCKET required.\")\n",
    "    try:\n",
    "        storage_client = storage.Client(project=PROJECT_ID)\n",
    "        if not storage_client.bucket(GCS_BUCKET).exists(): display(Markdown(f\"⚠️ GCS Bucket `{GCS_BUCKET}` inaccessible.\"))\n",
    "        else: display(Markdown(f\"✅ GCS Client Initialized (Bucket: '{GCS_BUCKET}').\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **GCS Client Error:** {e}.\")); logger.error(\"GCS Client Init Failed\", exc_info=True)\n",
    "        if not SKIP_PREPARE: raise RuntimeError(\"GCS client failed.\")\n",
    "        else: display(Markdown(\"⚠️ GCS client failed, but skipping prep.\"))\n",
    "else:\n",
    "    display(Markdown(\"### Initializing Gemini API Client (File API)\"))\n",
    "    try:\n",
    "        storage_client = ai_client.files\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **Gemini File API Client Error:** {e}.\")); logger.error(\"Gemini API Client Init Failed\", exc_info=True)\n",
    "    display(Markdown(f\"✅ Gemini File API Client Initialized.\"))\n",
    "\n",
    "# --- Final Checks --- #\n",
    "if ai_client is None: raise RuntimeError(\"AI client failed.\")\n",
    "\n",
    "if USE_VERTEX and storage_client is None and not SKIP_PREPARE: raise RuntimeError(\"GCS client failed for Vertex prep.\")\n",
    "display(Markdown(\"✅ Client initialization complete.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6c267",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4fc9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File/Data Handling ---\n",
    "def load_processed_video_ids(filename: str) -> Set[str]:\n",
    "    processed_video_ids = set()\n",
    "    if Path(filename).is_file():\n",
    "        try:\n",
    "            df = pd.read_csv(filename, usecols=['video_id'], dtype={'video_id': str}, on_bad_lines='warn')\n",
    "            processed_video_ids = set(df['video_id'].dropna().unique())\n",
    "            logger.info(f\"Loaded {len(processed_video_ids)} processed video IDs from {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not read video IDs from {filename}: {e}. Assuming zero processed.\")\n",
    "    return processed_video_ids\n",
    "\n",
    "\n",
    "def download_file_with_progress(url: str, destination: Path):\n",
    "    logger.info(f\"Downloading {url} to {destination}...\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=600)\n",
    "        response.raise_for_status()\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024 * 1024\n",
    "        with open(destination, 'wb') as f, tqdm(\n",
    "            desc=f\"Downloading {destination.name}\", total=total_size, unit='iB', unit_scale=True, unit_divisor=1024\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                size = f.write(data)\n",
    "                bar.update(size)\n",
    "        if total_size != 0 and bar.n != total_size:\n",
    "            destination.unlink(missing_ok=True)\n",
    "            raise RuntimeError(f\"Download size mismatch for {destination.name}.\")\n",
    "        logger.info(f\"Successfully downloaded {destination}\")\n",
    "    except Exception as e:\n",
    "        destination.unlink(missing_ok=True)\n",
    "        logger.error(f\"Download failed for {url}: {e}\")\n",
    "        raise\n",
    "\n",
    "def extract_zip(zip_path: Path, extract_to: Path):\n",
    "    logger.info(f\"Extracting {zip_path.name} to {extract_to}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            members = [m for m in zip_ref.namelist() if not m.startswith('__MACOSX/') and not m.endswith('.DS_Store')]\n",
    "            with tqdm(total=len(members), desc=f\"Extracting {zip_path.name}\") as pbar:\n",
    "                for member in members:\n",
    "                    zip_ref.extract(member=member, path=extract_to)\n",
    "                    pbar.update(1)\n",
    "        logger.info(f\"Successfully extracted {zip_path} to {extract_to}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Extraction error: {e}\")\n",
    "        raise\n",
    "    \n",
    "def move_videos_to_main_directory(base_path):\n",
    "    \"\"\"Find all MP4 files in subdirectories and move them to the main directory.\"\"\"\n",
    "    logger.info(f\"Moving all videos to main directory: {base_path}\")\n",
    "    moved_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    # Find all MP4 files in subdirectories (but not in the main directory)\n",
    "    for file_path in list(base_path.glob('**/*.mp4')):\n",
    "        # Skip files already in the main directory or hidden Mac files\n",
    "        if file_path.parent == base_path or file_path.name.startswith('._'):\n",
    "            continue\n",
    "            \n",
    "        # Destination in the main directory\n",
    "        dest_path = base_path / file_path.name\n",
    "        \n",
    "        try:\n",
    "            # Move the file\n",
    "            shutil.move(str(file_path), str(dest_path))\n",
    "            moved_count += 1\n",
    "            if moved_count % 50 == 0:\n",
    "                logger.info(f\"Moved {moved_count} videos so far...\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error moving {file_path}: {e}\")\n",
    "            failed_count += 1\n",
    "    \n",
    "    logger.info(f\"Moved {moved_count} videos to main directory. Failed: {failed_count}\")\n",
    "    \n",
    "\n",
    "def create_or_update_metadata(metadata_path: str, dataset_df: pd.DataFrame, video_updates: Dict[str, Dict]):\n",
    "    try:\n",
    "        required_cols = ['video_id', 'qid']\n",
    "        update_cols = ['local_path', 'gcs_uri', 'file_api_name', 'status']\n",
    "        dtype_map = {'video_id': str, 'qid': str} # Ensure IDs are strings\n",
    "\n",
    "        if not Path(metadata_path).is_file():\n",
    "            logger.info(f\"Creating metadata file: {metadata_path}\")\n",
    "            meta_df = dataset_df.copy()\n",
    "            for col in update_cols: meta_df[col] = pd.NA\n",
    "            meta_df['status'] = 'pending'\n",
    "        else:\n",
    "            logger.debug(f\"Loading existing metadata: {metadata_path}\")\n",
    "            meta_df = pd.read_csv(metadata_path, dtype=dtype_map)\n",
    "            for col in update_cols: # Add missing update columns if needed\n",
    "                 if col not in meta_df.columns: meta_df[col] = pd.NA\n",
    "\n",
    "        if not all(col in meta_df.columns for col in required_cols):\n",
    "            raise ValueError(f\"Metadata missing required columns ({required_cols}).\")\n",
    "\n",
    "        updates_df = pd.DataFrame.from_dict(video_updates, orient='index')\n",
    "        updates_df.index.name = 'video_id'\n",
    "        updates_df.reset_index(inplace=True)\n",
    "        updates_df['video_id'] = updates_df['video_id'].astype(str)\n",
    "\n",
    "        # Use merge for robust updating across potentially multiple rows per video_id\n",
    "        # First, prepare updates DF with only the necessary columns (video_id + update_cols)\n",
    "        merge_cols = ['video_id'] + [col for col in update_cols if col in updates_df.columns]\n",
    "        updates_to_merge = updates_df[merge_cols].drop_duplicates(subset=['video_id'], keep='last')\n",
    "\n",
    "        # Merge, prioritizing updates\n",
    "        # Suffixes help identify original vs update cols if needed, but update will overwrite\n",
    "        merged_df = pd.merge(meta_df, updates_to_merge, on='video_id', how='left', suffixes=('', '_update'))\n",
    "\n",
    "        # Apply the updates\n",
    "        for col in update_cols:\n",
    "            update_col_name = col + '_update'\n",
    "            if update_col_name in merged_df.columns:\n",
    "                # Fill NAs in original col with update col, then drop update col\n",
    "                meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n",
    "                # Alternative: Directly update where update is not NA\n",
    "                # meta_df[col] = np.where(merged_df[update_col_name].notna(), merged_df[update_col_name], merged_df[col])\n",
    "\n",
    "        meta_df.to_csv(metadata_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Metadata file '{metadata_path}' updated with {len(video_updates)} video records.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating metadata {metadata_path}: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def load_metadata_questions_generation(metadata_file: str = METADATA_FILE) -> Dict[str, Dict]:\n",
    "    \"\"\"Loads video metadata where each video_id is unique. Returns Dict[video_id, metadata_dict].\"\"\"\n",
    "    if not Path(metadata_file).is_file():\n",
    "        return {}\n",
    "\n",
    "    required_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "    try:\n",
    "        df = pd.read_csv(metadata_file, dtype=str).fillna('')\n",
    "        if 'video_id' not in df.columns or required_col not in df.columns:\n",
    "            logger.error(f\"Metadata missing 'video_id' or '{required_col}'.\")\n",
    "            return {}\n",
    "        # Keep only valid rows\n",
    "        valid_df = df[df['video_id'].astype(bool) & df[required_col].astype(bool)]\n",
    "        if len(valid_df) == 0:\n",
    "            logger.warning(f\"No videos found with '{required_col}' in {metadata_file}. Check Step 4.\")\n",
    "            return {}\n",
    "        # Drop duplicates to ensure uniqueness\n",
    "        unique_df = valid_df.drop_duplicates(subset='video_id', keep='first')\n",
    "        # Create the final dictionary\n",
    "        video_metadata = {\n",
    "            row['video_id']: row.to_dict()\n",
    "            for _, row in unique_df.iterrows()\n",
    "        }\n",
    "        logger.info(f\"Loaded {len(video_metadata)} unique videos for inference.\")\n",
    "        return video_metadata\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading metadata for inference: {e}\", exc_info=True)\n",
    "        return {}\n",
    "\n",
    "\n",
    "# --- Upload/Verification Helpers ---\n",
    "def upload_to_gcs(storage_client, bucket_name: str, source_file_path: Path, destination_blob_name: str) -> Optional[str]:\n",
    "    if not GCS_AVAILABLE or storage_client is None or not source_file_path.is_file(): return None\n",
    "    try:\n",
    "        blob = storage_client.bucket(bucket_name).blob(destination_blob_name)\n",
    "        blob.upload_from_filename(str(source_file_path))\n",
    "        gcs_uri = f\"gs://{bucket_name}/{destination_blob_name}\"\n",
    "        logger.debug(f\"GCS OK: {source_file_path} -> {gcs_uri}\")\n",
    "        return gcs_uri\n",
    "    except Exception as e:\n",
    "        logger.error(f\"GCS Fail: {source_file_path}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def upload_via_file_api(storage_client, local_path: Path, display_name: str) -> Optional[str]:\n",
    "    if storage_client is None or not local_path.is_file(): return None\n",
    "    try:\n",
    "        logger.debug(f\"Uploading {local_path} via File API...\")\n",
    "        uploaded_file = storage_client.upload(file=local_path)\n",
    "        logger.info(f\"File API OK: {local_path} -> {uploaded_file.name}\")\n",
    "        return uploaded_file.name\n",
    "    except Exception as e:\n",
    "        logger.error(f\"File API Fail: {local_path}. Error: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def verify_gcs_file_exists(storage_client, gcs_uri: str) -> bool:\n",
    "    if not GCS_AVAILABLE or storage_client is None or not gcs_uri: return False\n",
    "    try:\n",
    "        exists = storage.Blob.from_string(gcs_uri, client=storage_client).exists()\n",
    "        if not exists: logger.warning(f\"GCS verify failed: {gcs_uri}\")\n",
    "        return exists\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error verifying GCS {gcs_uri}: {e}\")\n",
    "        return False\n",
    "\n",
    "def verify_file_api_resource_exists(storage_client, file_api_name: str) -> bool:\n",
    "    if not storage_client or not file_api_name: return False\n",
    "    try:\n",
    "        _ = storage_client.get(name=file_api_name) # Sync get for verification\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error verifying File API {file_api_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def verify_local_file_exists(local_path: str) -> bool:\n",
    "    exists = Path(local_path).is_file() if local_path else False\n",
    "    if not exists: logger.warning(f\"Local verify failed: {local_path}\")\n",
    "    return exists\n",
    "\n",
    "# --- Prompt Building ---\n",
    "def build_prompt(question_info: dict) -> str:\n",
    "    question = question_info.get(\"question\", \"\")\n",
    "    q_type = question_info.get(\"question_type\", \"default\")\n",
    "    template = PROMPT_TEMPLATES.get(q_type, PROMPT_TEMPLATES[\"default\"])\n",
    "    # if q_type is MCQ\n",
    "    if q_type == \"Multiple-choice Question with a Single Correct Answer\":\n",
    "        return template.format(question=question).strip() + \"\\n\" + \"E. None of the above\"\n",
    "    return template.format(question=question).strip() + \"\\n\" + question_info.get(\"question_prompt\").strip()\n",
    "\n",
    "# --- Rate Limiter ---\n",
    "class AsyncRateLimiter:\n",
    "    \"\"\"\n",
    "    An asyncio-compatible token bucket rate limiter.\n",
    "\n",
    "    Args:\n",
    "        rate (int): The maximum number of requests allowed per period.\n",
    "        period (float): The time period in seconds (default: 60 for RPM).\n",
    "        capacity (int, optional): The maximum burst capacity. Defaults to `rate`.\n",
    "    \"\"\"\n",
    "    def __init__(self, rate: int, period: float = 60.0, capacity: Optional[int] = None):\n",
    "        if rate <= 0:\n",
    "            raise ValueError(\"Rate must be positive\")\n",
    "        if period <= 0:\n",
    "            raise ValueError(\"Period must be positive\")\n",
    "\n",
    "        self.rate = rate\n",
    "        self.period = float(period)\n",
    "        self.capacity = float(capacity if capacity is not None else rate)\n",
    "        self._tokens = self.capacity # Start full\n",
    "        self._last_refill_time = time.monotonic()\n",
    "        self._lock = asyncio.Lock()\n",
    "\n",
    "    def _get_tokens_per_second(self) -> float:\n",
    "        return self.rate / self.period\n",
    "\n",
    "    async def _refill(self):\n",
    "        \"\"\"Replenishes tokens based on elapsed time. Must be called under lock.\"\"\"\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self._last_refill_time\n",
    "        if elapsed > 0:\n",
    "            tokens_to_add = elapsed * self._get_tokens_per_second()\n",
    "            self._tokens = min(self.capacity, self._tokens + tokens_to_add)\n",
    "            self._last_refill_time = now\n",
    "\n",
    "    async def acquire(self):\n",
    "        \"\"\"\n",
    "        Acquires a token, waiting if necessary.\n",
    "        \"\"\"\n",
    "        async with self._lock:\n",
    "            await self._refill() # Refill based on time since last acquire/refill\n",
    "\n",
    "            while self._tokens < 1:\n",
    "                # Calculate how long to wait for 1 token\n",
    "                tokens_needed = 1.0 - self._tokens\n",
    "                wait_time = tokens_needed / self._get_tokens_per_second()\n",
    "\n",
    "                # Release the lock before sleeping\n",
    "                lock_released = True\n",
    "                try:\n",
    "                    self._lock.release()\n",
    "                    logger.debug(f\"Rate limit hit. Waiting for {wait_time:.3f}s for next token.\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                finally:\n",
    "                    # Re-acquire the lock if it was released\n",
    "                    if lock_released:\n",
    "                        await self._lock.acquire()\n",
    "\n",
    "                # Refill again after waiting, as more time has passed\n",
    "                await self._refill()\n",
    "\n",
    "            # Consume a token\n",
    "            self._tokens -= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79b7f8",
   "metadata": {},
   "source": [
    "# Download, Extract & Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5cee60",
   "metadata": {},
   "source": [
    "## Fetch Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12da9974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:18:13,098 - INFO - Fetching dataset 'lmms-lab/AISG_Challenge' (split: 'test') from HuggingFace...\n",
      "2025-05-09 21:18:17,802 - INFO - Successfully fetched dataset and saved to dataset.csv (1500 rows).\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ Dataset fetched and saved to `dataset.csv` (1500 rows)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>video_id</th>\n",
       "      <th>question_type</th>\n",
       "      <th>capability</th>\n",
       "      <th>question</th>\n",
       "      <th>duration</th>\n",
       "      <th>question_prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>youtube_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008-0</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Primary Open-ended Question</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>What is the difference between the action of t...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008-1</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Paraphrased Open-ended Question</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>Can you describe how the actions of the last p...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0008-2</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Correctly-led Open-ended Question</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>Did the last person open the bottle without us...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0008-3</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Wrongly-led Open-ended Question</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>Did the last person in the video open the bott...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>Please state your answer with a brief explanat...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0008-7</td>\n",
       "      <td>sj81PWrerDk</td>\n",
       "      <td>Multiple-choice Question with a Single Correct...</td>\n",
       "      <td>Plot Attribute (Montage)</td>\n",
       "      <td>How does the last person in the video open the...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>E. None of the above\\nSelect one best answer t...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.youtube.com/shorts/sj81PWrerDk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid     video_id                                      question_type  \\\n",
       "0  0008-0  sj81PWrerDk                        Primary Open-ended Question   \n",
       "1  0008-1  sj81PWrerDk                    Paraphrased Open-ended Question   \n",
       "2  0008-2  sj81PWrerDk                  Correctly-led Open-ended Question   \n",
       "3  0008-3  sj81PWrerDk                    Wrongly-led Open-ended Question   \n",
       "4  0008-7  sj81PWrerDk  Multiple-choice Question with a Single Correct...   \n",
       "\n",
       "                 capability  \\\n",
       "0  Plot Attribute (Montage)   \n",
       "1  Plot Attribute (Montage)   \n",
       "2  Plot Attribute (Montage)   \n",
       "3  Plot Attribute (Montage)   \n",
       "4  Plot Attribute (Montage)   \n",
       "\n",
       "                                            question duration  \\\n",
       "0  What is the difference between the action of t...     8.85   \n",
       "1  Can you describe how the actions of the last p...     8.85   \n",
       "2  Did the last person open the bottle without us...     8.85   \n",
       "3  Did the last person in the video open the bott...     8.85   \n",
       "4  How does the last person in the video open the...     8.85   \n",
       "\n",
       "                                     question_prompt answer  \\\n",
       "0  Please state your answer with a brief explanat...          \n",
       "1  Please state your answer with a brief explanat...          \n",
       "2  Please state your answer with a brief explanat...          \n",
       "3  Please state your answer with a brief explanat...          \n",
       "4  E. None of the above\\nSelect one best answer t...          \n",
       "\n",
       "                                  youtube_url  \n",
       "0  https://www.youtube.com/shorts/sj81PWrerDk  \n",
       "1  https://www.youtube.com/shorts/sj81PWrerDk  \n",
       "2  https://www.youtube.com/shorts/sj81PWrerDk  \n",
       "3  https://www.youtube.com/shorts/sj81PWrerDk  \n",
       "4  https://www.youtube.com/shorts/sj81PWrerDk  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = Path(DATASET_CSV)\n",
    "\n",
    "if dataset_path.is_file() and SKIP_FETCH:\n",
    "    logger.info(f\"Dataset file '{DATASET_CSV}' exists and SKIP_FETCH is True. Skipping.\")\n",
    "    display(Markdown(f\"✅ Skipping fetch: Found existing `{DATASET_CSV}`.\"))\n",
    "    # Load the existing dataframe for use in Step 2\n",
    "    try:\n",
    "        dataset_df = pd.read_csv(dataset_path, dtype=str) # Load all as string initially\n",
    "        logger.info(f\"Loaded existing dataset from {DATASET_CSV} ({len(dataset_df)} rows).\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load existing dataset file {DATASET_CSV}: {e}\")\n",
    "        display(Markdown(f\"❌ Error loading existing `{DATASET_CSV}`: {e}. Please delete the file or set SKIP_FETCH=False.\"))\n",
    "        raise\n",
    "else:\n",
    "    logger.info(f\"Fetching dataset '{HF_DATASET_NAME}' (split: '{HF_DATASET_SPLIT}') from HuggingFace...\")\n",
    "    try:\n",
    "        dataset = load_dataset(HF_DATASET_NAME, split=HF_DATASET_SPLIT, cache_dir=HF_CACHE_DIR)\n",
    "        dataset_df = dataset.to_pandas()\n",
    "        # Ensure key columns are strings\n",
    "        for col in ['qid', 'video_id', 'question', 'question_type']:\n",
    "             if col in dataset_df.columns:\n",
    "                 dataset_df[col] = dataset_df[col].astype(str)\n",
    "        dataset_df.to_csv(dataset_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Successfully fetched dataset and saved to {DATASET_CSV} ({len(dataset_df)} rows).\")\n",
    "        display(Markdown(f\"✅ Dataset fetched and saved to `{DATASET_CSV}` ({len(dataset_df)} rows).\"))\n",
    "        display(dataset_df.head())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch or save dataset: {e}\", exc_info=True)\n",
    "        display(Markdown(f\"❌ **Error fetching dataset:** {e}. Check connection, dataset name/split, cache dir permissions.\"))\n",
    "        raise RuntimeError(\"Dataset fetching failed. Cannot continue.\")\n",
    "\n",
    "# Ensure dataset_df is loaded if skipping fetch didn't load it (e.g., first run with skip=True and no file)\n",
    "if 'dataset_df' not in locals():\n",
    "    if dataset_path.is_file():\n",
    "        try:\n",
    "            dataset_df = pd.read_csv(dataset_path, dtype=str)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Critical error: Could not load dataset from {DATASET_CSV} after attempting fetch/skip: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise RuntimeError(f\"Critical error: Dataset DataFrame not loaded and file {DATASET_CSV} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fab26",
   "metadata": {},
   "source": [
    "## Download, Extract, and Prepare Videos\n",
    "\n",
    "Downloads, extracts, and uploads videos (to GCS or File API). Updates `video_metadata.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c752de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(DATASET_CSV)\n",
    "dataset_df = pd.read_csv(dataset_path, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826fbc54",
   "metadata": {},
   "source": [
    "### Download Video Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e9d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Downloading Archive"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✅ Skipping download: Found `downloads/all_videos.zip`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"### Downloading Archive\"))\n",
    "# Remember to set SKIP_DOWNLOAD_ZIP to True if you want to skip the download and you already have the zip file\n",
    "if zip_file_path.is_file() and SKIP_DOWNLOAD_ZIP:\n",
    "    display(Markdown(f\"✅ Skipping download: Found `{zip_file_path}`.\"))\n",
    "else:\n",
    "    try: download_file_with_progress(VIDEO_ZIP_URL, zip_file_path); display(Markdown(f\"✅ Downloaded: `{zip_file_path}`.\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **Download Error:** {e}.\"))\n",
    "        if not SKIP_EXTRACT or not SKIP_PREPARE: raise RuntimeError(f\"Download failed.\")\n",
    "        else: display(Markdown(\"⚠️ Download failed, skipping steps.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d8ad5",
   "metadata": {},
   "source": [
    "### Extract Video Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2055cbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Extracting Archive"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✅ Skipping extraction: Files in `extracted_videos`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"### Extracting Archive\"))\n",
    "# Check if the zip file exists and if we should skip extraction\n",
    "if any(extracted_videos_path.glob('*.mp4')) and SKIP_EXTRACT:\n",
    "    display(Markdown(f\"✅ Skipping extraction: Files in `{extracted_videos_path}`.\"))\n",
    "elif not zip_file_path.is_file():\n",
    "    display(Markdown(f\"❌ Cannot extract: `{zip_file_path}` missing.\"))\n",
    "    if not SKIP_PREPARE: raise RuntimeError(f\"Zip missing.\")\n",
    "    else: display(Markdown(\"⚠️ Extraction skipped (no zip).\"))\n",
    "else:\n",
    "    try: \n",
    "        extract_zip(zip_file_path, extracted_videos_path)\n",
    "        # Move all videos to main directory\n",
    "        move_videos_to_main_directory(extracted_videos_path)\n",
    "        display(Markdown(f\"✅ Extracted to `{extracted_videos_path}` and moved all videos to main directory.\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **Extraction Error:** {e}.\"))\n",
    "        if not SKIP_PREPARE: raise RuntimeError(\"Extraction failed.\")\n",
    "        else: display(Markdown(\"⚠️ Extraction failed, skipping prep.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca34116",
   "metadata": {},
   "source": [
    "### Slow/Speed Up Videos\n",
    "Losslessly change video speed while also re-encoding audio to maintain pitch. As\n",
    "a result, is super fast. Could be made faster if using asyncio to concurrently run\n",
    "ffmpeg. The video results are saved into **speed_videos/0.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2b773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Preparing Videos"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:49,041 - INFO - Processing 289 unique video IDs.\n",
      "2025-05-09 16:57:49,042 - INFO - Using concurrency limit: 25\n",
      "Preparing tasks for 289 videos...\n",
      "Transforming 289 Videos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7c1850f5784fd4b54007fa9b19d06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transforming Videos:   0%|          | 0/289 [00:00<?, ?video/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "289 videos skipped, 0 videos processed, 0 errors, 289 total.\n"
     ]
    }
   ],
   "source": [
    "async def run_subprocess(cmd, check=True, capture_output=False):\n",
    "    \"\"\"Helper function to run subprocess asynchronously.\"\"\"\n",
    "    stdout_pipe = asyncio.subprocess.PIPE if capture_output else asyncio.subprocess.DEVNULL\n",
    "    # Capture stderr only if check is True or capture_output is True, otherwise DEVNULL\n",
    "    stderr_pipe = asyncio.subprocess.PIPE if check or capture_output else asyncio.subprocess.DEVNULL\n",
    "\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        *cmd,\n",
    "        stdout=stdout_pipe,\n",
    "        stderr=stderr_pipe\n",
    "    )\n",
    "    stdout, stderr = await process.communicate()\n",
    "\n",
    "    if check and process.returncode != 0:\n",
    "        error_msg = f\"Command '{' '.join(cmd)}' failed with return code {process.returncode}\"\n",
    "        stderr_decoded = stderr.decode(errors='ignore') if stderr else \"\"\n",
    "        if stderr_decoded:\n",
    "            error_msg += f\"\\nStderr: {stderr_decoded}\"\n",
    "        # Raise specific exception to potentially capture stderr later\n",
    "        raise subprocess.CalledProcessError(process.returncode, cmd, output=stdout, stderr=stderr)\n",
    "\n",
    "    return stdout, stderr, process.returncode\n",
    "\n",
    "async def process_single_video(vid_path, speed_videos_path, VIDEO_SPEED_FACTOR, semaphore):\n",
    "    \"\"\"Asynchronously processes a single video. Returns status string.\"\"\"\n",
    "    vid_path_str = str(vid_path.resolve())\n",
    "    out_path = speed_videos_path / vid_path.name\n",
    "    out_path_str = str(out_path.resolve())\n",
    "\n",
    "    async with semaphore: # Limit concurrency\n",
    "        if out_path.is_file():\n",
    "            return 'skipped'\n",
    "\n",
    "        if VIDEO_SPEED_FACTOR == 1.0:\n",
    "            try:\n",
    "                # Use asyncio.to_thread for potentially blocking I/O\n",
    "                await asyncio.to_thread(shutil.copy, vid_path_str, out_path_str)\n",
    "                return 'processed'\n",
    "            except Exception as e:\n",
    "                try:\n",
    "                    logger.error(f\"Error copying {vid_path.name}: {e}\")\n",
    "                except NameError:\n",
    "                    print(f\"Error copying {vid_path.name}: {e}\")\n",
    "                return 'error'\n",
    "\n",
    "        # --- Process video with speed change ---\n",
    "        tf_bitstream_path = None\n",
    "        tf_audio_path = None\n",
    "        tf_final_path = None\n",
    "        try:\n",
    "            # Create temporary files (synchronous part is okay here)\n",
    "            # Context manager ensures files are closed before ffmpeg uses them\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".h264\") as tf_b, \\\n",
    "                 tempfile.NamedTemporaryFile(delete=False, suffix=\".aac\") as tf_a, \\\n",
    "                 tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as tf_f:\n",
    "                tf_bitstream_name = tf_b.name\n",
    "                tf_audio_name = tf_a.name\n",
    "                tf_final_name = tf_f.name\n",
    "            # Store paths for cleanup\n",
    "            tf_bitstream_path = Path(tf_bitstream_name)\n",
    "            tf_audio_path = Path(tf_audio_name)\n",
    "            tf_final_path = Path(tf_final_name)\n",
    "\n",
    "\n",
    "            # Get original FPS\n",
    "            ffprobe_cmd = [\n",
    "                \"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"v\", \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n",
    "                \"-show_entries\", \"stream=r_frame_rate\", vid_path_str\n",
    "            ]\n",
    "            stdout, _, _ = await run_subprocess(ffprobe_cmd, check=True, capture_output=True)\n",
    "            fps = float(fractions.Fraction(stdout.decode().strip()))\n",
    "            new_fps = fps * VIDEO_SPEED_FACTOR\n",
    "\n",
    "            # Extract and speed up audio\n",
    "            factor = VIDEO_SPEED_FACTOR\n",
    "            filter_parts = []\n",
    "            while factor > 2.0:\n",
    "                filter_parts.append(\"atempo=2.0\")\n",
    "                factor /= 2.0\n",
    "            while factor < 0.5:\n",
    "                filter_parts.append(\"atempo=0.5\")\n",
    "                factor /= 0.5\n",
    "            if abs(factor - 1.0) > 1e-6:\n",
    "                 filter_parts.append(f\"atempo={factor:.6f}\")\n",
    "\n",
    "            if not filter_parts:\n",
    "                 audio_cmd = [\"ffmpeg\", \"-y\", \"-i\", vid_path_str, \"-vn\", \"-c:a\", \"copy\", tf_audio_name]\n",
    "            else:\n",
    "                audio_filter = \",\".join(filter_parts)\n",
    "                audio_cmd = [\"ffmpeg\", \"-y\", \"-i\", vid_path_str, \"-vn\", \"-filter:a\", audio_filter, \"-c:a\", \"aac\", \"-b:a\", \"128k\", tf_audio_name]\n",
    "            await run_subprocess(audio_cmd, check=True)\n",
    "\n",
    "\n",
    "            # Extract h264 bitstream\n",
    "            extract_cmd = [\"ffmpeg\", \"-y\", \"-i\", vid_path_str, \"-map\", \"0:v\", \"-c:v\", \"copy\", \"-bsf:v\", \"h264_mp4toannexb\", tf_bitstream_name]\n",
    "            await run_subprocess(extract_cmd, check=True)\n",
    "\n",
    "            # Remux bitstream with new audio and FPS\n",
    "            remux_cmd = [\"ffmpeg\", \"-y\", \"-fflags\", \"+genpts\", \"-r\", f\"{new_fps:.6f}\", \"-i\", tf_bitstream_name, \"-i\", tf_audio_name, \"-map\", \"0:v\", \"-map\", \"1:a\", \"-c:v\", \"copy\", \"-c:a\", \"copy\", tf_final_name]\n",
    "            await run_subprocess(remux_cmd, check=True)\n",
    "\n",
    "            # Move final file (use asyncio.to_thread)\n",
    "            await asyncio.to_thread(shutil.move, tf_final_name, out_path_str)\n",
    "            return 'processed'\n",
    "\n",
    "        except Exception as e:\n",
    "            err_msg = f\"Error processing {vid_path.name}: {e}\"\n",
    "            # Include ffmpeg stderr if available\n",
    "            if isinstance(e, subprocess.CalledProcessError) and e.stderr:\n",
    "                 err_msg += f\"\\nFFmpeg/FFprobe Stderr:\\n{e.stderr.decode(errors='ignore')}\"\n",
    "            try:\n",
    "                logger.error(err_msg)\n",
    "            except NameError:\n",
    "                print(err_msg)\n",
    "            return 'error'\n",
    "        finally:\n",
    "            # Clean up temporary files asynchronously using to_thread\n",
    "            async def _cleanup():\n",
    "                if tf_bitstream_path and tf_bitstream_path.exists():\n",
    "                    tf_bitstream_path.unlink(missing_ok=True)\n",
    "                if tf_audio_path and tf_audio_path.exists():\n",
    "                    tf_audio_path.unlink(missing_ok=True)\n",
    "                # tf_final is moved, only delete if error occurred before move\n",
    "                if tf_final_path and tf_final_path.exists():\n",
    "                    tf_final_path.unlink(missing_ok=True)\n",
    "            # Run sync cleanup in thread only if paths were assigned\n",
    "            if tf_bitstream_path or tf_audio_path or tf_final_path:\n",
    "                 await asyncio.to_thread(_cleanup)\n",
    "\n",
    "\n",
    "# --- Main Cell Logic ---\n",
    "\n",
    "async def run_processing(): # Wrap in an async function to use await\n",
    "    display(Markdown(\"### Preparing Videos\"))\n",
    "    if dataset_df is None:\n",
    "        raise RuntimeError(\"Dataset DF unavailable.\")\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    speed_videos_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_video_ids = sorted(list(dataset_df['video_id'].dropna().unique()))\n",
    "    # Use logging if available, otherwise print\n",
    "    try:\n",
    "        logger.info(f\"Processing {len(all_video_ids)} unique video IDs.\")\n",
    "    except NameError:\n",
    "        print(f\"Processing {len(all_video_ids)} unique video IDs.\")\n",
    "\n",
    "    vid_paths = list(extracted_videos_path.glob(\"*.mp4\"))\n",
    "\n",
    "    # Limit concurrency\n",
    "    concurrency_limit = MAX_ASYNC_WORKERS\n",
    "    try:\n",
    "        logger.info(f\"Using concurrency limit: {concurrency_limit}\")\n",
    "    except NameError:\n",
    "        print(f\"Using concurrency limit: {concurrency_limit}\")\n",
    "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
    "\n",
    "    tasks = []\n",
    "    # Keep the familiar loop structure for creating tasks\n",
    "    print(f\"Preparing tasks for {len(vid_paths)} videos...\")\n",
    "    for vid_path in vid_paths:\n",
    "         # Create a task for each video processing job\n",
    "         # Pass necessary arguments to the task creator\n",
    "         task = asyncio.create_task(process_single_video(vid_path, speed_videos_path, VIDEO_SPEED_FACTOR, semaphore))\n",
    "         tasks.append(task)\n",
    "\n",
    "    # Now, run all the created tasks concurrently and display progress\n",
    "    # Use asyncio.as_completed with a standard tqdm progress bar\n",
    "    print(f\"Transforming {len(tasks)} Videos...\")\n",
    "    results = []\n",
    "    # Use the imported tqdm (now tqdm.auto) to create a standard progress bar instance\n",
    "    with tqdm(total=len(tasks), desc=\"Transforming Videos\", unit=\"video\") as pbar:\n",
    "        for future in asyncio.as_completed(tasks):\n",
    "            try:\n",
    "                result = await future # Get result from completed task\n",
    "                results.append(result)\n",
    "            except Exception as exc:\n",
    "                # Log errors from tasks that failed internally if not caught by process_single_video\n",
    "                # (process_single_video should ideally return 'error' status instead of raising)\n",
    "                try:\n",
    "                    logger.error(f\"Task for a video failed: {exc}\")\n",
    "                except NameError:\n",
    "                    print(f\"Task for a video failed: {exc}\")\n",
    "                results.append('error') # Count as error if task itself fails unexpectedly\n",
    "            finally:\n",
    "                 pbar.update(1) # Increment progress bar regardless of outcome\n",
    "\n",
    "\n",
    "    # Count results\n",
    "    processed = results.count('processed')\n",
    "    skipped = results.count('skipped')\n",
    "    errors = results.count('error')\n",
    "\n",
    "    print(f\"\\n\\n{skipped} videos skipped, {processed} videos processed, {errors} errors, {len(vid_paths)} total.\")\n",
    "\n",
    "# --- Execute the async processing ---\n",
    "# In a Jupyter Notebook, you usually need to await the top-level async function.\n",
    "# If top-level await isn't enabled, you might need nest_asyncio or run manually.\n",
    "# Using await directly is the most common way in modern notebooks.\n",
    "\n",
    "await run_processing()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92696e74",
   "metadata": {},
   "source": [
    "### Preparing and Upload Videos to GCS or File API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c59f9",
   "metadata": {},
   "source": [
    "Upload all videos into File API, and record the link to that video inside video_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c86fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Preparing Videos & Updating Metadata"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:56:30,359 - INFO - Processing 289 unique video IDs.\n",
      "2025-05-09 16:56:30,368 - INFO - Checked existing metadata statuses/IDs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583a7a97e5084894b28db52605f932bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing (File API Upload):   0%|          | 0/289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:56:30,373 - INFO - Prep Batch 1/29...\n",
      "2025-05-09 16:56:31,635 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/w39suyzut56n \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:31,897 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/41kf9ogzg8j0 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:32,947 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ve0u1v50aefp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:33,200 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/0l5gb60u48l2 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:34,238 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/miph0amkhkls \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:34,502 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/w3ip4tz6eciq \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:35,511 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/tpvog0tailnp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:35,814 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/zjmh9a1u8pwp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:36,759 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/o72i26ppnxee \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:37,061 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/z1zgf2hh9q2l \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:37,088 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:56:37,089 - INFO - Prep Batch 2/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:56:38,022 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/o331ocqdkfkp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:38,274 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/l4ya82g93xzp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:38,532 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/95rsvlque3n6 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:38,788 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/eez9invg1g17 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:39,045 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/vctx6ncsmds3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:39,299 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/uhuu1lext2td \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:39,555 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/tkwluqfg0wku \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:39,807 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/tno2c3ags7vj \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:40,080 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/v9gt54dwu3zh \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:40,363 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ktf7jp2y83h1 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:40,390 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:56:40,391 - INFO - Prep Batch 3/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:56:40,630 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/850utvj99uhm \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:40,878 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/0zp1rnmesj76 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:41,133 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/f7qb6ggy1adz \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:41,388 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/f7b0q7ybt777 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:41,636 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/h3tp5l56v26c \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:41,884 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ra6pu7q12dgu \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:42,135 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/yxpcpnqtme35 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:42,387 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/9hnojrj1sogl \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:42,638 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/2tbw5rqmlsjo \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:42,883 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/fkb2eui0kilk \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:42,910 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:56:42,911 - INFO - Prep Batch 4/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:56:43,187 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/r5zikoquxv18 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:43,479 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/6z3528vj1395 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:43,715 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/wcbt54nw1f9k \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:43,958 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/7nkufe223z8c \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:44,213 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/xa20qmgsshox \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:44,454 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/en9ic7loesdj \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:44,695 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/yqveoimn6onv \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:44,939 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/t6gpe4cl281s \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:45,190 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/1ttgurnqare3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:45,421 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ib78bsx7kp8q \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:45,449 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:56:45,449 - INFO - Prep Batch 5/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:56:45,700 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/k3yvdgy1no7g \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:45,958 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/xvsy1an2lhlv \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:46,259 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/zctd8wq4eju6 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:46,573 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/wbznq1bzp6na \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:46,819 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/dep2ay5syolh \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:47,105 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/4cgaw9bvnqoh \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:47,358 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/v97uxv5emiqi \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:47,605 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/kkatwuzg61xy \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:47,862 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/3se1pgt8uhrx \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:48,113 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/wpq78d5vv3ov \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:48,139 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:56:48,139 - INFO - Prep Batch 6/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:56:48,384 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/w8dn1phzsxjc \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:48,622 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/imma063kra4k \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:48,878 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/egc3cmvttnjc \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:49,125 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/grzg97gufm0j \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:49,390 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/1a2zpnt932tr \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:49,657 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/avqnwigwnbmx \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:49,900 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/rttv291eulcp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:50,146 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/5t2xw3kk6oo5 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:50,390 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/uljerxiufz77 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:50,635 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/f7wng1h2gkzm \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:50,663 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:56:50,663 - INFO - Prep Batch 7/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:56:50,917 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/hz5da1rc8wq9 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:51,168 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/i4u9iuibcuja \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:51,417 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ezep5lpjlhhr \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:51,657 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/piij58us0sjv \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:51,908 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/qjyl5vv8hq7r \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:52,157 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/386r61cvh8ny \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:52,397 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/gn8t0qz2u8n7 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:52,648 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/83hql8gw3mky \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:52,900 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/xlgehhyk840z \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:53,143 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/h6uckwqf3cum \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:53,170 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:56:53,171 - INFO - Prep Batch 8/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:56:53,402 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/9yor6cwv4ezw \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:53,645 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/b6g8xcvjdb27 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:53,894 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ww1qpxrzy8ql \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:54,138 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/57l2qmgo1h74 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:54,385 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ys8kx3o8opbu \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:54,631 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/9ao2iwy3us1f \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:54,874 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/3h6s5wxn5f6k \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:55,118 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/u2s4crill25j \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:55,403 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/vv13uajitgmt \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:55,678 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/gz5rsdpbzx04 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:55,706 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:56:55,706 - INFO - Prep Batch 9/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:56:55,951 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/qk9xg2155t6z \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:56,198 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/2xy44ytetltq \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:56,439 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/nqgqryebwlcr \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:56,680 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/l64ocrrte2wx \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:56,935 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/zf3wogc0vuqk \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:57,188 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/megi1xkk7ywg \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:57,445 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/1wb6dcakr1xz \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:57,696 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/mx7i04jzz0a5 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:57,949 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/21r9r0x2fsot \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:58,195 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/xkxvz2vxuh5b \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:58,224 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:56:58,225 - INFO - Prep Batch 10/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:56:58,470 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/8544ulcewne3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:58,767 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/m3jf7mzrcctj \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:59,012 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/7p7jia4uic8i \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:59,255 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/kynzzmyjlb7y \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:59,508 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/fqchbh2e671j \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:56:59,754 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/zk2v39e967vq \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:00,000 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/g9cboa9af30f \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:00,247 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/vcw685ej9zm3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:00,509 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/5q4jd9royb9n \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:00,757 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/d69h3jvkthoi \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:00,785 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:00,786 - INFO - Prep Batch 11/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:01,039 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/v4s94ljlxdk3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:01,299 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/gj7uemw49sa7 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:01,549 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/vfw9z5w5c93y \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:01,829 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/kr3x7ic2fev4 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:02,072 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/euh0i9eu2hco \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:02,333 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/q1612r9x0obr \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:02,588 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/c40r3ucyxmdr \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:02,826 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/3xfqimc2ug1c \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:03,070 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/dt4pq43r40el \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:03,313 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/9iku7e9kco2y \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:03,342 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:03,343 - INFO - Prep Batch 12/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:03,602 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/7cgupmr0qckp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:03,836 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/7a4rmks2sn07 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:04,089 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/5teru7glwp15 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:04,335 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/17aol3hrpsp0 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:04,591 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/bo4yhl3y6ikb \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:04,880 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/uh012ar70nx0 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:05,133 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/j3i58jftpzge \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:05,381 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/f74zc0ze88k4 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:05,632 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/904sr8wqce2a \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:05,889 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/54fnnzlbigb7 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:05,918 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:05,919 - INFO - Prep Batch 13/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:06,175 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ysegdo3gejz9 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:06,423 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/4ausvv5th7pz \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:06,677 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/5jty59n0m2rq \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:06,926 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/h0rbedwgiyf4 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:07,186 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/oywph0404air \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:07,446 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/2bpybh3ldtg3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:07,692 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ilo72cp5qg05 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:07,972 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/2btzatnizvo6 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:08,212 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ym210vsbjxce \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:08,461 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/0bqy4mt2yvd0 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:08,487 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:08,488 - INFO - Prep Batch 14/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:08,727 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/bwz2bf0h6f8d \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:08,977 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/cu82zg5agwwx \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:09,227 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/g5w50kv57m10 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:09,466 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/vquplbx2vkr6 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:09,718 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/cgqwnzgjxie5 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:09,959 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/wlon0ix4nh74 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:10,216 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/3icelho8pss0 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:10,467 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ga7watoe6r9a \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:10,736 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/nd3yeg7cq8f4 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:11,004 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/wi8g87xemz7v \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:11,033 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:11,033 - INFO - Prep Batch 15/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:11,269 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/lq01sle000zk \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:11,520 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/1lojcwe0fc9x \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:11,775 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/s8io01dkfs9a \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:12,020 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/6qhargnlkbsc \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:12,269 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/i0f596v6i26q \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:12,505 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/2hi8wri3w5x1 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:12,760 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/a8c3pe0jcr9t \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:13,001 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/f5z6bjmsk189 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:13,257 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/s09cg24g6ymt \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:13,500 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/8d2znj0u8zsy \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:13,528 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:13,528 - INFO - Prep Batch 16/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:13,784 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/15b911pr2jqi \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:14,075 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/g0ck73kjuagx \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:14,330 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/8rq9humq5yu7 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:14,577 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/g1nsxqxxuogw \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:14,834 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/2a793o0hi94y \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:15,081 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/85e9rogej6kp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:15,323 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/d23uc9dsybox \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:15,557 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/npq8mp63t7i4 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:15,805 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ii0yyv54xx33 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:16,049 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/9qgl5tf7erly \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:16,076 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:16,077 - INFO - Prep Batch 17/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:16,323 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/82lk06s1tgar \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:16,571 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/buix7z0gz6wt \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:16,883 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/o6ind10d5u84 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:17,174 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/zdevhd2kdobm \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:17,403 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/egtu044i2pgq \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:17,642 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/x90zg16j04uk \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:17,907 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/hyme0p59iq82 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:18,157 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/01gf7ribs1ef \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:18,412 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/wv3yw45je5ga \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:18,662 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/erjhamo0bdq2 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:18,691 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:18,691 - INFO - Prep Batch 18/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:18,939 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/xpznelj2xm4q \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:19,183 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/s3spo0my9b00 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:19,448 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ftfoiwkd27yp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:19,695 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/5y21918ikeqt \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:19,944 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ducboy43ruad \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:20,222 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/7zof77ujsuy3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:20,465 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/aj479161t11i \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:20,706 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/o1tmd4irn35u \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:20,941 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ci10ioa6c3qj \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:21,179 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/nswvqch0pxr6 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:21,207 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:21,208 - INFO - Prep Batch 19/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:21,449 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/4yih81l9tvhv \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:21,695 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/edkb2geri5bi \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:21,961 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/dmd3mnaj7e5b \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:22,201 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/flh0sis7ucr1 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:22,445 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/6sxh49xz4ttm \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:22,686 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/bp9y9tst4074 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:22,982 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/e5tz6c1ittpq \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:23,234 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/dwf5127ezeno \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:23,485 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/g0sf7e4qrjmx \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:23,736 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/4r9pg0rvrmuu \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:23,763 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:23,764 - INFO - Prep Batch 20/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:24,003 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/0dfnxj2nvims \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:24,269 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/0vkijfkw03sz \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:24,509 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/nuihn5uprctp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:24,764 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ivp540zfo2fp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:25,018 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/8kxm3b5fti2d \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:25,260 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/n8mvz6g3h7y0 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:25,512 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/xigtoa2hpgv4 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:25,759 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/4i154mm8abre \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:26,068 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/bam8hipdql7r \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:26,362 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/5z8klnevw3di \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:26,391 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:26,392 - INFO - Prep Batch 21/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:26,640 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/s62z792fjsdx \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:26,891 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/399a5m74lnoy \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:27,149 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/wi2g45y0d40k \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:27,402 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/rpegl8ercnba \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:27,667 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/304m7x4dlr8d \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:27,917 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/wxddeojrd6h3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:28,171 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/5ennj6om85m8 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:28,411 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/aajrym2qltla \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:28,663 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/33kim85a76pz \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:28,920 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/6ysxdkagfnfk \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:28,947 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:28,948 - INFO - Prep Batch 22/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:29,199 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/8wmdemzu3l26 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:29,485 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/6qqfmnmzm09n \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:29,722 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/fhp8zxr6pf4y \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:29,966 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ajy8v7h34cu4 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:30,223 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/4w201q6u4gma \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:30,469 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/g610d1rerqd3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:30,714 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/cxuh321hxjz6 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:30,961 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ejqgj7fxhm8u \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:31,221 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/xevg1wkaam73 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:31,469 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/926c6a40sme3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:31,496 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:31,496 - INFO - Prep Batch 23/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:31,738 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/nbc9aiksod2i \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:31,981 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/x2zy599obeea \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:32,265 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/427h676cwvcd \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:32,510 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/cphcqjexflqu \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:32,754 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/91w9cb8z2qde \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:32,996 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/8p7j77bnkdv3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:33,241 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/i623nuayrg3y \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:33,487 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/zq6kt1mn5wth \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:33,742 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/41qdzte5tqh4 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:33,976 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/smpy2qorbau8 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:34,003 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:34,003 - INFO - Prep Batch 24/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:34,257 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/fu2xlhq5tpqa \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:34,501 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/sj7ykzpstrrq \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:34,759 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/b9dkoyljk04n \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:34,998 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/6uqcu4m8y536 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:35,240 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ff1p4g96dnvp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:35,509 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/gt9uuplm440n \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:35,753 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/x7kdiwu2lysn \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:35,998 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/hughoumnk6gi \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:36,244 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/vf516z61o03r \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:36,503 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/3uby6i85a3lo \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:36,530 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:36,531 - INFO - Prep Batch 25/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:36,782 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/gyyzvwexuawu \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:37,017 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/2dqryfjvcdcn \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:37,258 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/q2rlo21hk51u \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:37,488 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/r0jxwzxoju9i \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:37,747 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/9q75b11nkumv \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:38,025 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/z8wka6izb87o \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:38,269 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/kylpi0w0eohc \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:38,516 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/u5fdvery45vo \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:38,757 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/3hne8kjrb1os \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:39,015 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/d08mw4frsbdd \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:39,044 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:39,045 - INFO - Prep Batch 26/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:39,284 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/mivgufg9sf2v \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:39,536 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/qdo9uz9hh5fm \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:39,780 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/4h3uut56oxrn \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:40,013 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/4megq9lqu3zt \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:40,264 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/hdolj8budn5y \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:40,507 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/gffl9rgiudup \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:40,755 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/yhvmvysyd4on \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:41,008 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/efaaazspdynf \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:41,257 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/36nij71qp2n3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:41,503 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/bwrdhg5xzqlh \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:41,532 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:41,533 - INFO - Prep Batch 27/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:41,773 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/6ukp3ka0v2i7 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:42,027 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/krcuwkl1r7d0 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:42,269 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/t1qxyu7n4v4t \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:42,518 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/x4b8ni01b7hk \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:42,767 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/1g4yv3b0fzpr \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:43,012 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/dbgduyhs8xes \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:43,262 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/qtwtb5f4a865 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:43,505 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/yf7ejguwfv7z \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:43,753 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/dvjhnz8pqnd0 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:43,997 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/en58vwbr5wl5 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:44,026 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:44,027 - INFO - Prep Batch 28/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:44,318 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/wzhqg1npvno9 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:44,556 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/1ms27o0mgnd8 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:44,788 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/3wg34rb3bmeu \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:45,023 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/sn9cog94x05k \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:45,268 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/memmvh0po3rn \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:45,512 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/5v2pfqljdrbk \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:45,764 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/2v21gokzi5xs \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:46,027 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/bdi04c3uiy53 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:46,301 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/rtpucpm90hpo \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:46,579 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/r7iaioq369vn \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:46,608 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 10 video records.\n",
      "2025-05-09 16:57:46,608 - INFO - Prep Batch 29/29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 16:57:46,860 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/pv2tcwf0urv7 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:47,101 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/stzfw4bzq0o6 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:47,361 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/174bq6d7nkja \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:47,637 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ai7s0wpxv68v \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:47,874 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/6l62emtc0x0a \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:48,123 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/hdoaritov1wq \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:48,371 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ibpib2zm6b3e \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:48,609 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/hi6qjoicoaph \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:48,860 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/36lw8vcjptn4 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 16:57:48,887 - INFO - Metadata file 'video_metadata_non_vertex.csv' updated with 9 video records.\n",
      "2025-05-09 16:57:48,888 - INFO - Prep finished. Checked: 289, Skipped(verified): 289, Missing Local: 0, Upload Failures: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1557101/1819856703.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ Video preparation complete. See logs. Metadata: `video_metadata_non_vertex.csv`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Prepare Videos (Upload GCS/File API) & Update Metadata --- #\n",
    "display(Markdown(\"### Preparing Videos & Updating Metadata\"))\n",
    "if SKIP_PREPARE:\n",
    "    display(Markdown(\"✅ Skipping video preparation.\"))\n",
    "elif storage_client is None:\n",
    "     display(Markdown(\"❌ Cannot prepare: Client not ready.\")); raise RuntimeError(\"Client missing.\")\n",
    "else:\n",
    "    if dataset_df is None: raise RuntimeError(\"Dataset DF unavailable.\")\n",
    "    all_video_ids = sorted(list(dataset_df['video_id'].dropna().unique()))\n",
    "    logger.info(f\"Processing {len(all_video_ids)} unique video IDs.\")\n",
    "\n",
    "    videos_to_process_ids = all_video_ids\n",
    "    if MAX_VIDEOS_TO_PROCESS is not None:\n",
    "        videos_to_process_ids = all_video_ids[:MAX_VIDEOS_TO_PROCESS]\n",
    "        logger.info(f\"Limiting to {len(videos_to_process_ids)} videos.\")\n",
    "\n",
    "    # Load existing metadata to check status\n",
    "    existing_statuses = {}\n",
    "    resource_ids = {}\n",
    "    required_id_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "    if Path(METADATA_FILE).is_file():\n",
    "        try:\n",
    "            existing_df = pd.read_csv(METADATA_FILE, dtype=str)\n",
    "            if 'video_id' in existing_df.columns and 'status' in existing_df.columns:\n",
    "                existing_statuses = pd.Series(existing_df.status.values, index=existing_df.video_id).to_dict()\n",
    "            if 'video_id' in existing_df.columns and required_id_col in existing_df.columns:\n",
    "                 resource_ids = pd.Series(existing_df[required_id_col].values, index=existing_df.video_id).dropna().to_dict()\n",
    "            logger.info(\"Checked existing metadata statuses/IDs.\")\n",
    "        except Exception as e: logger.warning(f\"Could not load existing metadata: {e}\")\n",
    "\n",
    "    video_metadata_updates = {}\n",
    "    processed_count, upload_failures, missing_local, skipped_count = 0, 0, 0, 0\n",
    "    num_batches = math.ceil(len(videos_to_process_ids) / UPLOAD_BATCH_SIZE_GCS)\n",
    "    prep_mode = \"GCS Upload\" if USE_VERTEX else \"File API Upload\"\n",
    "\n",
    "    with tqdm(total=len(videos_to_process_ids), desc=f\"Preparing ({prep_mode})\") as pbar:\n",
    "        for i in range(0, len(videos_to_process_ids), UPLOAD_BATCH_SIZE_GCS):\n",
    "            batch_ids = videos_to_process_ids[i : i + UPLOAD_BATCH_SIZE_GCS]\n",
    "            batch_num = (i // UPLOAD_BATCH_SIZE_GCS) + 1\n",
    "            logger.info(f\"Prep Batch {batch_num}/{num_batches}...\")\n",
    "            current_batch_updates = {}\n",
    "\n",
    "            for video_id in batch_ids:\n",
    "                pbar.set_postfix_str(f\"ID: {video_id}\")\n",
    "                update_data = {\"local_path\": None, \"gcs_uri\": None, \"file_api_name\": None, \"status\": \"error_unknown\"}\n",
    "                local_video_path = speed_videos_path / f\"{video_id}.mp4\"\n",
    "                current_status = existing_statuses.get(video_id, 'pending')\n",
    "                existing_resource_id = resource_ids.get(video_id)\n",
    "                is_already_processed = False\n",
    "\n",
    "                # Check if already uploaded and verified\n",
    "                if current_status in ['uploaded_gcs', 'uploaded_file_api'] and existing_resource_id:\n",
    "                     verified = False\n",
    "                     if USE_VERTEX: verified = verify_gcs_file_exists(storage_client, existing_resource_id)\n",
    "                     else: verified = verify_file_api_resource_exists(storage_client, existing_resource_id)\n",
    "                     if verified:\n",
    "                         logger.debug(f\"Skipping verified video {video_id} ('{current_status}').\")\n",
    "                         is_already_processed = True\n",
    "                         skipped_count += 1\n",
    "                         update_data.update({ # Ensure metadata is consistent\n",
    "                             'local_path': str(local_video_path) if local_video_path.is_file() else None,\n",
    "                             'status': current_status,\n",
    "                             required_id_col: existing_resource_id\n",
    "                         })\n",
    "                     else:\n",
    "                         logger.warning(f\"Video {video_id} ({current_status}) needs re-processing (verification failed).\")\n",
    "                elif current_status != 'pending':\n",
    "                     logger.debug(f\"Video {video_id} has non-pending status '{current_status}' but no verified resource ID. Re-processing.\")\n",
    "\n",
    "                if is_already_processed:\n",
    "                    processed_count += 1\n",
    "                    current_batch_updates[video_id] = update_data\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Process if needed\n",
    "                if local_video_path.is_file():\n",
    "                    update_data[\"local_path\"] = str(local_video_path)\n",
    "                    resource_id_result = None\n",
    "                    if USE_VERTEX:\n",
    "                        blob_name = f\"videos/{video_id}.mp4\"\n",
    "                        resource_id_result = upload_to_gcs(storage_client, GCS_BUCKET, local_video_path, blob_name)\n",
    "                        if resource_id_result: update_data.update({\"gcs_uri\": resource_id_result, \"status\": \"uploaded_gcs\"})\n",
    "                        else: update_data[\"status\"] = \"gcs_upload_failed\"; upload_failures += 1\n",
    "                    else: # Gemini API\n",
    "                        resource_id_result = upload_via_file_api(storage_client, local_video_path, f\"vid_{video_id}\")\n",
    "                        if resource_id_result: update_data.update({\"file_api_name\": resource_id_result, \"status\": \"uploaded_file_api\"})\n",
    "                        else: update_data[\"status\"] = \"file_api_upload_failed\"; upload_failures += 1\n",
    "                else:\n",
    "                    logger.warning(f\"Local file missing: {local_video_path}\")\n",
    "                    missing_local += 1\n",
    "                    update_data[\"status\"] = \"local_missing\"\n",
    "\n",
    "                current_batch_updates[video_id] = update_data\n",
    "                processed_count += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Update metadata after batch\n",
    "            if current_batch_updates:\n",
    "                 try: create_or_update_metadata(METADATA_FILE, dataset_df, current_batch_updates)\n",
    "                 except Exception as e: logger.error(f\"Metadata update failed batch {batch_num}: {e}\")\n",
    "                 video_metadata_updates.update(current_batch_updates)\n",
    "\n",
    "    logger.info(f\"Prep finished. Checked: {processed_count}, Skipped(verified): {skipped_count}, Missing Local: {missing_local}, Upload Failures: {upload_failures}\")\n",
    "    display(Markdown(f\"✅ Video preparation complete. See logs. Metadata: `{METADATA_FILE}`.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16dd3c6",
   "metadata": {},
   "source": [
    "# Perform Bulk Inference To Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e84cd939",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS_DIR = os.path.join(f\"generated_questions/{QUESTIONS_MODEL_NAME}\", \"questions.csv\")\n",
    "os.makedirs(os.path.dirname(QUESTIONS_DIR), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28719bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:29:11,214 - INFO - Loading metadata for inference...\n",
      "2025-05-09 21:29:11,238 - INFO - Loaded 289 unique videos for inference.\n",
      "Loaded 289 video questions for inference.\n",
      "2025-05-09 21:29:11,241 - INFO - Loaded 289 processed video IDs from generated_questions/gemini-2.0-flash/questions.csv\n",
      "2025-05-09 21:29:11,241 - INFO - Prepared 0 new inference tasks. Skipped 289.\n",
      "2025-05-09 21:29:11,242 - INFO - No new questions to process.\n"
     ]
    }
   ],
   "source": [
    "# --- Inference Functions ---\n",
    "\n",
    "async def perform_inference_single_async(\n",
    "    video_info: Dict,\n",
    "    client: Any,\n",
    "    semaphore: asyncio.Semaphore,\n",
    "    rate_limiter: Optional[AsyncRateLimiter],\n",
    "    results_queue: asyncio.Queue\n",
    ") -> None: # Return None as result is put in queue\n",
    "    \"\"\"\n",
    "    Async inference for one question, putting the result into a queue.\n",
    "    \"\"\"\n",
    "    video_id = video_info.get(\"video_id\", \"?\")\n",
    "    # prompt_text = build_prompt(question_info)\n",
    "    prompt_text = QUESTIONS_PROMPT\n",
    "    gcs_uri = video_info.get(\"gcs_uri\")\n",
    "    file_api_name = video_info.get(\"file_api_name\")\n",
    "    start_time = time.time()\n",
    "    result = None # Default result in case of early exit\n",
    "\n",
    "    # --- Prepare Inputs (Same as before) ---\n",
    "    try:\n",
    "        video_part = None\n",
    "        if USE_VERTEX:\n",
    "            # ... (GCS URI logic) ...\n",
    "            if not gcs_uri: raise ValueError(\"Missing GCS URI.\")\n",
    "            video_part = types.Part.from_uri(mime_type='video/mp4', file_uri=gcs_uri)\n",
    "        else: # Gemini API Mode\n",
    "            # ... (File API get logic) ...\n",
    "             if not file_api_name: raise ValueError(\"Missing File API name.\")\n",
    "             try:\n",
    "                 file_object = await client.aio.files.get(name=file_api_name)\n",
    "                 video_part = file_object\n",
    "             except genai_errors.NotFoundError: raise FileNotFoundError(f\"File API '{file_api_name}' not found.\")\n",
    "             except Exception as e: raise RuntimeError(f\"Failed get File API obj: {e}\")\n",
    "\n",
    "        video_content = types.Content(role=\"user\", parts=[types.Part.from_text(text=prompt_text)])\n",
    "        \n",
    "        if video_part is None: raise RuntimeError(\"Video part preparation failed.\")\n",
    "        contents = [video_content, video_part]\n",
    "\n",
    "    except (ValueError, FileNotFoundError, RuntimeError) as e:\n",
    "        logger.error(f\"QID {video_id} (Async): Input Error - {e}\")\n",
    "        # Put error result in queue\n",
    "        result = {\"video_id\": video_id, \"questions\": f\"ERROR: Input Fail - {e}\", \"duration\": 0, \"finish_reason\": \"N/A\", \"status\": \"Failed (Input)\"}\n",
    "        await results_queue.put(result)\n",
    "        return # Exit the function\n",
    "    except Exception as e:\n",
    "         logger.error(f\"Video_ID {video_id} (Async): Unexpected Input Prep Error: {e}\", exc_info=True)\n",
    "         result = {\"video_id\": video_id, \"questions\": f\"ERROR: Input Prep Failed Unexpectedly - {e}\", \"duration\": 0, \"finish_reason\": \"N/A\", \"status\": \"Failed (Input Prep)\"}\n",
    "         await results_queue.put(result)\n",
    "         return # Exit the function\n",
    "\n",
    "    # --- Perform Inference with Retries, Semaphore, and Rate Limiting ---\n",
    "    async with semaphore:\n",
    "        for attempt in range(QUESTIONS_MAX_RETRIES + 1):\n",
    "            try:\n",
    "                if rate_limiter: await rate_limiter.acquire()\n",
    "                api_start = time.time()\n",
    "                logger.debug(f\"QID {video_id} (Async): Attempt {attempt + 1} sending request...\")\n",
    "                response = await client.aio.models.generate_content(\n",
    "                    model=QUESTIONS_MODEL_NAME,\n",
    "                    contents=contents,\n",
    "                    config=QUESTIONS_CONFIG\n",
    "                )\n",
    "                print(f\"video_id: {video_id}, response: {response}\")\n",
    "\n",
    "                # Process Response\n",
    "                answer, reason, status, err_detail = \"ERROR\", \"UNKNOWN\", \"Success\", \"\"\n",
    "                try:\n",
    "                    answer = response.parsed.questions\n",
    "                    if response.candidates and hasattr(response.candidates[0], 'finish_reason') and response.candidates[0].finish_reason is not None:\n",
    "                        reason = response.candidates[0].finish_reason.name\n",
    "                except ValueError as ve:\n",
    "                    status, err_detail = \"Blocked/Empty\", f\"ValueError: {ve}. \"\n",
    "                    # ... (block/safety reason extraction) ...\n",
    "                    answer = f\"ERROR: {status}. {err_detail}\"\n",
    "\n",
    "                result = {\"video_id\": video_id, \"questions\": answer, \"duration\": time.time()-start_time, \"finish_reason\": reason, \"status\": status}\n",
    "                await results_queue.put(result)\n",
    "                logger.debug(f\"QID {video_id} (Async): Attempt {attempt + 1} {status} ({time.time()-api_start:.2f}s API / {time.time()-start_time:.2f}s Total). Result queued.\")\n",
    "                return # Exit function after success\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                 result = {\"video_id\": video_id, \"questions\": f\"ERROR: - {e}\", \"duration\": time.time()-start_time, \"finish_reason\": reason, \"status\": \"Failed (Unexpected Error)\"}\n",
    "                 await results_queue.put(result)\n",
    "                 return\n",
    "\n",
    "        # Fallback (Should not be reached if logic above is correct)\n",
    "        logger.error(f\"VideoID {video_id} (Async): Exited retry loop unexpectedly.\")\n",
    "        result = {\"video_id\": video_id, \"questions\": \"ERROR: Unknown after retries\", \"duration\": time.time()-start_time, \"finish_reason\": \"UNKNOWN\", \"status\": \"Failed (Unknown)\"}\n",
    "        await results_queue.put(result)\n",
    "\n",
    "async def results_writer_task(\n",
    "    queue: asyncio.Queue,\n",
    "    filename: str,\n",
    "    write_batch_size: int = 20, # How many results to buffer before writing\n",
    "    write_interval_sec: float = 10.0 # Max time between writes\n",
    "):\n",
    "    \"\"\"Gets results from queue and writes them to CSV in batches.\"\"\"\n",
    "    results_buffer = []\n",
    "    last_write_time = time.monotonic()\n",
    "    # Define expected header based on the dict keys put in the queue\n",
    "    fieldnames = [\"video_id\", \"questions\", \"status\", \"duration_sec\", \"finish_reason\"]\n",
    "    file_exists = Path(filename).is_file()\n",
    "\n",
    "    logger.info(f\"Writer task started. Writing results to {filename}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Wait for an item with a timeout\n",
    "            result = await asyncio.wait_for(queue.get(), timeout=write_interval_sec)\n",
    "\n",
    "            if result is None: # Signal to terminate\n",
    "                logger.info(\"Writer task received termination signal.\")\n",
    "                break\n",
    "\n",
    "            if isinstance(result, dict):\n",
    "                 # Ensure duration is rounded here before adding to buffer\n",
    "                 result[\"duration_sec\"] = round(result.get(\"duration\", -1), 2)\n",
    "                 # Remove the original 'duration' key if desired\n",
    "                 result.pop('duration', None)\n",
    "                 results_buffer.append(result)\n",
    "            else:\n",
    "                logger.warning(f\"Writer task received non-dict item: {result}\")\n",
    "\n",
    "            queue.task_done() # Signal that the item was processed\n",
    "\n",
    "        except asyncio.TimeoutError:\n",
    "            # Timeout occurred, write buffer if not empty, even if batch size not reached\n",
    "            logger.debug(\"Writer task timeout reached.\")\n",
    "            pass # Continue to buffer check below\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Writer task encountered error getting from queue: {e}\", exc_info=True)\n",
    "            # Decide if this is fatal or if we should try to continue\n",
    "            await asyncio.sleep(1) # Prevent fast spinning on error\n",
    "            continue # Try to continue processing\n",
    "\n",
    "        # Check if we should write the buffer\n",
    "        buffer_size = len(results_buffer)\n",
    "        time_since_last_write = time.monotonic() - last_write_time\n",
    "        should_write = (\n",
    "             buffer_size > 0 and\n",
    "             (buffer_size >= write_batch_size or time_since_last_write >= write_interval_sec)\n",
    "        )\n",
    "\n",
    "        if should_write:\n",
    "            logger.info(f\"Writing batch of {buffer_size} results to {filename}...\")\n",
    "            try:\n",
    "                # Use 'with open' for proper handling\n",
    "                with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                    # Write header only if file didn't exist at the start\n",
    "                    if not file_exists:\n",
    "                        writer.writeheader()\n",
    "                        file_exists = True # Prevent writing header again\n",
    "                    writer.writerows(results_buffer)\n",
    "\n",
    "                results_buffer = [] # Clear buffer after successful write\n",
    "                last_write_time = time.monotonic()\n",
    "                logger.info(f\"Batch written successfully.\")\n",
    "            except IOError as e:\n",
    "                logger.error(f\"IOError writing results batch to {filename}: {e}. Results may be lost.\")\n",
    "                # Optional: Add retry logic here, or store failed batch elsewhere\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error writing results batch: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "    # --- Cleanup: Write any remaining items after receiving None signal ---\n",
    "    if results_buffer:\n",
    "        logger.info(f\"Writing final remaining {len(results_buffer)} results...\")\n",
    "        try:\n",
    "            with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                if not file_exists: writer.writeheader() # Check again in case file was deleted mid-run\n",
    "                writer.writerows(results_buffer)\n",
    "            logger.info(\"Final results written.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error writing final results batch: {e}\")\n",
    "\n",
    "    logger.info(\"Writer task finished.\")\n",
    "\n",
    "# --- Modified Main Execution Function with UI ---\n",
    "async def run_bulk_inference_async():\n",
    "    \"\"\"Runs the bulk inference process asynchronously with queue writer, rate limiting, and UI.\"\"\"\n",
    "\n",
    "    # --- Standard Setup ---\n",
    "    if ai_client is None: logger.error(\"AI Client not initialized.\"); display(Markdown(\"❌ AI Client not initialized.\")); return\n",
    "\n",
    "    # --- Load Data and Prepare Tasks ---\n",
    "    logger.info(\"Loading metadata for inference...\")\n",
    "    \n",
    "    # Load video questions for inference as a dictionaies of dicts\n",
    "    # Each dict contains video_id, gcs_uri, file_api_name, and other metadata\n",
    "    video_questions_for_inference = load_metadata_questions_generation(METADATA_FILE)\n",
    "    print(f\"Loaded {len(video_questions_for_inference)} video questions for inference.\")\n",
    "    \n",
    "    if not video_questions_for_inference: logger.warning(\"No video data ready.\"); return\n",
    "    processed_qids = load_processed_video_ids(QUESTIONS_DIR)\n",
    "\n",
    "    inference_tasks_input = []\n",
    "    tasks_skipped = 0\n",
    "    required_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "    for video_id, video_info in video_questions_for_inference.items():\n",
    "        # Check if video_id is correct:\n",
    "        if video_id == video_info.get('video_id'):\n",
    "            logger.debug(f\"Video ID {video_id} matches.\")\n",
    "        else:\n",
    "            logger.warning(f\"Video ID mismatch: {video_id} != {video_info.get('video_id')}.\")\n",
    "            continue\n",
    "        if not video_id or video_id in processed_qids:\n",
    "            tasks_skipped += 1; continue\n",
    "        inference_tasks_input.append(video_info)\n",
    "\n",
    "    total_tasks = len(inference_tasks_input)\n",
    "    logger.info(f\"Prepared {total_tasks} new inference tasks. Skipped {tasks_skipped}.\" )\n",
    "    if total_tasks == 0: logger.info(\"No new questions to process.\"); return\n",
    "\n",
    "    # --- Setup Rate Limiter ---\n",
    "    rate_limiter = None\n",
    "    if QUESTIONS_REQUESTS_PER_MINUTE is not None and QUESTIONS_REQUESTS_PER_MINUTE > 0:\n",
    "        capacity = 10 # Example: Keep burst capacity moderate\n",
    "        rate_limiter = AsyncRateLimiter(rate=QUESTIONS_REQUESTS_PER_MINUTE, period=60.0, capacity=capacity)\n",
    "        logger.info(f\"Rate limiting enabled: {QUESTIONS_REQUESTS_PER_MINUTE} RPM, Capacity: {capacity}\")\n",
    "    else:\n",
    "        logger.info(\"Rate limiting disabled.\")\n",
    "\n",
    "    # --- Create Queue and Start Writer Task ---\n",
    "    results_queue = asyncio.Queue()\n",
    "    writer_handle = asyncio.create_task(\n",
    "        results_writer_task(results_queue, QUESTIONS_DIR)\n",
    "    )\n",
    "\n",
    "    # --- Schedule and Run Inference Tasks with Progress Bar ---\n",
    "    logger.info(f\"Starting async inference for {total_tasks} questions (Concurrency: {QUESTIONS_MAX_ASYNC_WORKERS})...\")\n",
    "    semaphore = asyncio.Semaphore(QUESTIONS_MAX_ASYNC_WORKERS)\n",
    "    start_bulk_time = time.time()\n",
    "\n",
    "    # Create coroutines\n",
    "    inference_coroutines = [\n",
    "        perform_inference_single_async(q_info, ai_client, semaphore, rate_limiter, results_queue)\n",
    "        for q_info in inference_tasks_input\n",
    "    ]\n",
    "\n",
    "    # --- Use asyncio.as_completed with tqdm_notebook for live progress ---\n",
    "    completed_count = 0\n",
    "    gather_exception = None\n",
    "    try:\n",
    "        # Create a future for each coroutine to track completion\n",
    "        tasks = [asyncio.ensure_future(coro) for coro in inference_coroutines]\n",
    "        # Use tqdm with as_completed\n",
    "        for future in tqdm(asyncio.as_completed(tasks), total=total_tasks, desc=\"Async Inference\"):\n",
    "            try:\n",
    "                await future # Wait for the next task to complete, raise exception if task failed\n",
    "                completed_count += 1\n",
    "            except Exception as task_exc:\n",
    "                 # Log error from individual task if it wasn't caught inside\n",
    "                 logger.error(f\"Error surfaced from an inference task: {task_exc}\", exc_info=False)\n",
    "                 # Optionally decide if you want to stop processing other tasks\n",
    "                 # gather_exception = task_exc # Store first exception\n",
    "                 # break # Or continue processing others\n",
    "    except Exception as outer_exc:\n",
    "        # Catch errors during the setup/iteration of as_completed itself\n",
    "        logger.error(f\"Error during as_completed processing: {outer_exc}\", exc_info=True)\n",
    "        gather_exception = outer_exc\n",
    "\n",
    "    # Ensure all tasks are awaited even if we broke early due to an error in one task\n",
    "    # (This might be redundant if as_completed handles cancellation correctly, but safer)\n",
    "    await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    bulk_duration = time.time() - start_bulk_time\n",
    "    logger.info(f\"Async inference task processing finished in {bulk_duration:.2f} seconds. Completed: {completed_count}/{total_tasks}.\")\n",
    "    if gather_exception:\n",
    "         logger.error(f\"Bulk inference encountered errors: {gather_exception}\")\n",
    "\n",
    "    # --- Signal Writer Task to Finish (ALWAYS DO THIS) ---\n",
    "    logger.info(\"Signaling writer task to complete...\")\n",
    "    await results_queue.put(None)\n",
    "\n",
    "    # --- Wait for Writer Task to Finish (ALWAYS DO THIS) ---\n",
    "    logger.info(\"Waiting for writer task to finish writing remaining results...\")\n",
    "    try:\n",
    "        await writer_handle # Wait until the writer processes the None signal and exits\n",
    "        logger.info(\"Writer task has finished.\")\n",
    "    except Exception as writer_exc:\n",
    "        logger.error(f\"Error waiting for writer task: {writer_exc}\", exc_info=True)\n",
    "\n",
    "    # --- Final Summary ---\n",
    "    logger.info(f\"Bulk inference process complete. See logs and {QUESTIONS_DIR}.\")\n",
    "    # Remove handler to prevent duplicate logs on re-run\n",
    "\n",
    "# --- Run the async function ---\n",
    "asyncio.run(run_bulk_inference_async())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2076b7a",
   "metadata": {},
   "source": [
    "### Interim Clean Up for generated questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4d0ca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:29:16,688 - INFO - Loaded results from generated_questions/gemini-2.0-flash/questions.csv (289 rows).\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Results Summary: 289 Success, 0 Failed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:29:16,690 - INFO - Removed duplicates, remaining 289 unique QIDs.\n"
     ]
    }
   ],
   "source": [
    "def interim_cleanup_and_report(FILE_DIR):\n",
    "    \"\"\"Final cleanup and report of results.\"\"\"\n",
    "    if not Path(FILE_DIR).is_file(): display(Markdown(f\"❌ Results file `{FILE_DIR}` not found.\")); return\n",
    "\n",
    "    try:\n",
    "        results_df = pd.read_csv(FILE_DIR, dtype=str)\n",
    "        logger.info(f\"Loaded results from {FILE_DIR} ({len(results_df)} rows).\")\n",
    "        \n",
    "        # Correctly identify failed entries\n",
    "        failed = results_df[results_df['questions'].isna() | (results_df['status'] != 'Success')]\n",
    "        \n",
    "        # Success is everything not in failed\n",
    "        success = results_df[~results_df.index.isin(failed.index)]\n",
    "\n",
    "        display(Markdown(f\"### Results Summary: {len(success)} Success, {len(failed)} Failed.\"))\n",
    "\n",
    "        # Sort by 'video_id'\n",
    "        results_df.sort_values(by=['video_id'], inplace=True)\n",
    "\n",
    "        # remove duplicates based on 'video_id' and keep the first occurrence\n",
    "        results_df.drop_duplicates(subset=['video_id'], keep='first', inplace=True)\n",
    "        logger.info(f\"Removed duplicates, remaining {len(results_df)} unique QIDs.\")\n",
    "\n",
    "        # Save the cleaned-up results to a new file with datetime suffix\n",
    "        original_filename = FILE_DIR.replace('.csv', \"\")\n",
    "        success_filename = f\"{original_filename}.csv\"\n",
    "        failed_filename = f\"{original_filename}_failed.csv\"\n",
    "        failed.to_csv(failed_filename, index=False, encoding='utf-8')\n",
    "        success.drop_duplicates(subset=['video_id'], keep='first', inplace=True)\n",
    "        success.to_csv(success_filename, index=False, encoding='utf-8')\n",
    "     \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing results file: {e}\", exc_info=True)\n",
    "        display(Markdown(f\"❌ Error processing results file: {e}.\"))\n",
    "        return\n",
    "  \n",
    "interim_cleanup_and_report(QUESTIONS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4dcdc",
   "metadata": {},
   "source": [
    "# Perfrom chat conversation history on all Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7bf2a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWERS_DIR = f\"generated_questions/{QUESTIONS_MODEL_NAME}/chat_history\"\n",
    "os.makedirs(os.path.dirname(ANSWERS_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d523e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWERS_CHECK_DIR = os.path.join(f\"generated_questions/{QUESTIONS_MODEL_NAME}\", \"answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14296369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:29:21,729 - INFO - Loaded 289 videos (289 questions) with valid IDs for inference.\n"
     ]
    }
   ],
   "source": [
    "# ---Fetch Generated Questions for each videos ---\n",
    "import ast\n",
    "\n",
    "def get_questions_for_video(questions_file: str = QUESTIONS_DIR) -> Dict[str, List[str]]:\n",
    "    if not Path(questions_file).is_file(): return {}\n",
    "    video_questions = defaultdict(list)\n",
    "    try:\n",
    "        df = pd.read_csv(questions_file, dtype=str).fillna('')\n",
    "        if 'video_id' not in df.columns or 'questions' not in df.columns:\n",
    "            logger.error(f\"Questions file missing 'video_id' or 'question'.\")\n",
    "            return {}\n",
    "        valid_df = df[df['video_id'].astype(bool) & df['questions'].astype(bool)]\n",
    "        if len(valid_df) == 0:\n",
    "             logger.warning(f\"No videos found with questions in {questions_file}. Check Step 4.\")\n",
    "             return {}\n",
    "        for video_id in valid_df['video_id'].unique():\n",
    "            question_str = valid_df[valid_df['video_id'] == video_id]['questions'].iloc[0]\n",
    "            questions_list = ast.literal_eval(question_str)\n",
    "            video_questions[video_id] = questions_list\n",
    "        logger.info(f\"Loaded {len(video_questions)} videos ({len(valid_df)} questions) with valid IDs for inference.\")\n",
    "        return dict(video_questions)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading questions: {e}\", exc_info=True)\n",
    "        return {}\n",
    "\n",
    "generated_questions_dict = get_questions_for_video(QUESTIONS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39d017d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_chat(chat):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": msg.role,\n",
    "            \"parts\": [part.text for part in msg.parts]\n",
    "        } for msg in chat\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116c068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:29:26,702 - INFO - Loading metadata for inference...\n",
      "2025-05-09 21:29:26,724 - INFO - Loaded 289 unique videos for inference.\n",
      "Loaded 289 video questions for inference.\n",
      "2025-05-09 21:29:26,726 - INFO - Loaded 61 processed video IDs from generated_questions/gemini-2.0-flash/answers.csv\n",
      "2025-05-09 21:29:26,726 - INFO - Prepared 228 new inference tasks. Skipped 61.\n",
      "2025-05-09 21:29:26,727 - INFO - Rate limiting enabled: 7 RPM, Capacity: 10\n",
      "2025-05-09 21:29:26,727 - INFO - Starting async inference for 228 questions (Concurrency: 4)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a89c3ef4d994a1f97cb1981728c29be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Async Inference:   0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:29:26,735 - INFO - Writer task started. Writing results to generated_questions/gemini-2.0-flash/answers.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:29:26,739 - INFO - QID Se8JkJUt7YE (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,739 - INFO - QID RBz8pTO0ySw (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,740 - INFO - QID fyq0pd_pFvE (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,741 - INFO - QID FkNdEfxYYc4 (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,741 - INFO - QID SdvMPe6XF7w (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,742 - INFO - QID rujdEa3MVYo (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,742 - INFO - QID Ai4fCLMGL9Y (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,742 - INFO - QID Z5jKO_Qql30 (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,743 - INFO - QID yzlFm0LpRpM (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,744 - INFO - QID m8tfdmm3g2A (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,744 - INFO - QID HsXS1Qt11cU (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,745 - INFO - QID lAmfy5J8iEA (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,746 - INFO - QID jyXFmTycx_k (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,746 - INFO - QID xW8pfZ5WpNQ (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,747 - INFO - QID 3bisVkxCA0c (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,747 - INFO - QID _hkdNurB1Fk (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,748 - INFO - QID 85l_AvYt-XE (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,748 - INFO - QID y5lL-l01ujA (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,751 - INFO - QID Ldcs4Venq1Q (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,756 - INFO - QID wsPyV3igUG4 (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:26,770 - INFO - QID fD02ePuiOtU (Async): Already processed. Skipping.\n",
      "2025-05-09 21:29:27,276 - INFO - Writing batch of 20 results to generated_questions/gemini-2.0-flash/answers.csv...\n",
      "2025-05-09 21:29:27,277 - INFO - Batch written successfully.\n",
      "2025-05-09 21:29:27,625 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/57l2qmgo1h74 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:29:27,639 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:29:54,283 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:29:54,288 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:29:54,289 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:30:38,797 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:38,800 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:30:38,801 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:30:56,471 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,474 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:30:56,476 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/hdolj8budn5y \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:30:56,489 - ERROR - Video_ID trYIAMCLeOs (Async): Unexpected Input Prep Error: module 'google.genai.errors' has no attribute 'NotFoundError'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1593784/3084300841.py\", line 39, in perform_inference_single_async\n",
      "    file_object = await client.aio.files.get(name=file_api_name)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/files.py\", line 985, in get\n",
      "    response_dict = await self._api_client.async_request(\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/_api_client.py\", line 776, in async_request\n",
      "    result = await self._async_request(http_request=http_request, stream=False)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/_api_client.py\", line 720, in _async_request\n",
      "    await errors.APIError.raise_for_async_response(response)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/errors.py\", line 131, in raise_for_async_response\n",
      "    raise ServerError(status_code, response_json, response)\n",
      "google.genai.errors.ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The service is currently unavailable.', 'status': 'UNAVAILABLE'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1593784/3084300841.py\", line 41, in perform_inference_single_async\n",
      "    except genai_errors.NotFoundError: raise FileNotFoundError(f\"File API '{file_api_name}' not found.\")\n",
      "AttributeError: module 'google.genai.errors' has no attribute 'NotFoundError'\n",
      "2025-05-09 21:30:56,491 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/k3yvdgy1no7g \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,492 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/oywph0404air \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,493 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/f7qb6ggy1adz \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,494 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/vfw9z5w5c93y \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,496 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/rttv291eulcp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,496 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/dmd3mnaj7e5b \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,497 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/1a2zpnt932tr \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,498 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/f7wng1h2gkzm \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,499 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/imma063kra4k \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,500 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/1g4yv3b0fzpr \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,501 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/5y21918ikeqt \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,502 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/wbznq1bzp6na \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:30:56,503 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/3xfqimc2ug1c \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,504 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/hughoumnk6gi \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,505 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/fu2xlhq5tpqa \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,505 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/hi6qjoicoaph \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,506 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/1ttgurnqare3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,507 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ysegdo3gejz9 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,508 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/o1tmd4irn35u \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,509 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/xevg1wkaam73 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,510 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/8d2znj0u8zsy \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,511 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/r0jxwzxoju9i \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,512 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/n8mvz6g3h7y0 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,513 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/bam8hipdql7r \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:30:56,514 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/6qhargnlkbsc \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,515 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/euh0i9eu2hco \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,515 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/7p7jia4uic8i \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,516 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/xkxvz2vxuh5b \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,517 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/stzfw4bzq0o6 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,518 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/qdo9uz9hh5fm \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,519 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/4i154mm8abre \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,520 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/904sr8wqce2a \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,521 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/9hnojrj1sogl \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:30:56,522 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/o331ocqdkfkp \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,523 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/gyyzvwexuawu \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,524 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/2xy44ytetltq \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,525 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/m3jf7mzrcctj \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,526 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/5teru7glwp15 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,527 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/gt9uuplm440n \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,528 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/en58vwbr5wl5 \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:30:56,529 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/3se1pgt8uhrx \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,530 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/wi8g87xemz7v \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,531 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ff1p4g96dnvp \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:30:56,532 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/kkatwuzg61xy \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,533 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/nuihn5uprctp \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:30:56,534 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ii0yyv54xx33 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,535 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/3uby6i85a3lo \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,536 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/xa20qmgsshox \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,537 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/sn9cog94x05k \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,538 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/41qdzte5tqh4 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,538 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/cu82zg5agwwx \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,539 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/54fnnzlbigb7 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,540 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/tno2c3ags7vj \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,541 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/g0ck73kjuagx \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,542 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/nqgqryebwlcr \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,543 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/en9ic7loesdj \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,544 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/36lw8vcjptn4 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,545 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/9yor6cwv4ezw \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,546 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/4h3uut56oxrn \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,547 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/386r61cvh8ny \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,548 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/w3ip4tz6eciq \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,549 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/6ysxdkagfnfk \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:30:56,550 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/2tbw5rqmlsjo \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,551 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/fkb2eui0kilk \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,552 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/b9dkoyljk04n \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,553 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/1ms27o0mgnd8 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,554 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/gn8t0qz2u8n7 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,555 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/qjyl5vv8hq7r \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,556 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/pv2tcwf0urv7 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,557 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/6z3528vj1395 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,557 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/kynzzmyjlb7y \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,558 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/nd3yeg7cq8f4 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,559 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/1wb6dcakr1xz \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,560 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ibpib2zm6b3e \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,561 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/427h676cwvcd \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,562 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/yqveoimn6onv \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,563 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/399a5m74lnoy \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,564 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/z1zgf2hh9q2l \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,565 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/kylpi0w0eohc \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,566 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/flh0sis7ucr1 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,567 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/erjhamo0bdq2 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,568 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/9q75b11nkumv \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,568 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/4r9pg0rvrmuu \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,569 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/e5tz6c1ittpq \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,570 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/2dqryfjvcdcn \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,571 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/g610d1rerqd3 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,572 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/8wmdemzu3l26 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,573 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ducboy43ruad \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,574 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/fhp8zxr6pf4y \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,574 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ejqgj7fxhm8u \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,575 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/w39suyzut56n \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,576 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/fqchbh2e671j \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,577 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/4ausvv5th7pz \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,578 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/u2s4crill25j \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,579 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/ezep5lpjlhhr \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,580 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/zctd8wq4eju6 \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,581 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/vf516z61o03r \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,582 - INFO - HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/files/zq6kt1mn5wth \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:30:56,583 - INFO - Writing batch of 2 results to generated_questions/gemini-2.0-flash/answers.csv...\n",
      "2025-05-09 21:30:56,583 - INFO - Batch written successfully.\n",
      "2025-05-09 21:30:56,916 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:31:13,768 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:31:13,771 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:31:13,772 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:31:34,245 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:31:34,248 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:31:34,249 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:31:34,535 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:31:34,540 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:32:00,381 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 502 Bad Gateway\"\n",
      "2025-05-09 21:32:00,383 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:32:00,658 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:32:00,660 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:32:27,827 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 502 Bad Gateway\"\n",
      "2025-05-09 21:32:27,831 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:32:52,290 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:32:52,293 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:32:52,294 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:33:16,654 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:33:16,657 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:33:16,658 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:33:16,891 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:33:16,894 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:33:18,120 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:33:18,122 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:33:19,302 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:33:19,304 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:34:11,364 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:34:11,367 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:34:11,370 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:35:14,373 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:35:14,376 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:35:14,376 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:35:15,701 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:35:15,704 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:35:16,444 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:35:16,447 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:35:18,050 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:35:18,052 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:35:18,283 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:35:18,285 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:36:24,362 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 502 Bad Gateway\"\n",
      "2025-05-09 21:36:24,366 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:38:15,459 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:38:15,462 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:38:15,463 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:38:34,613 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:38:34,616 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:38:34,617 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:39:11,925 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:39:11,928 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:39:11,931 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:40:02,054 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:40:02,057 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:40:02,058 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:40:38,920 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:40:38,923 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:40:38,924 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:40:39,802 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:40:40,029 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:41:40,131 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:41:40,134 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:41:40,134 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:41:59,025 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:41:59,028 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:41:59,029 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:42:25,482 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:42:25,485 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:42:25,487 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:42:27,414 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:42:27,416 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:42:28,753 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:42:28,756 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:44:18,266 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:44:18,268 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:44:18,271 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:44:57,413 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:44:57,415 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:44:57,415 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:45:16,996 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:45:16,999 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:45:17,000 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:45:26,869 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:45:26,872 - ERROR - Video_ID 9SzIZz1CDcc (Async): Unexpected Input Prep Error: module 'google.genai.errors' has no attribute 'NotFoundError'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1593784/3084300841.py\", line 39, in perform_inference_single_async\n",
      "    file_object = await client.aio.files.get(name=file_api_name)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/files.py\", line 985, in get\n",
      "    response_dict = await self._api_client.async_request(\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/_api_client.py\", line 776, in async_request\n",
      "    result = await self._async_request(http_request=http_request, stream=False)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/_api_client.py\", line 720, in _async_request\n",
      "    await errors.APIError.raise_for_async_response(response)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/errors.py\", line 131, in raise_for_async_response\n",
      "    raise ServerError(status_code, response_json, response)\n",
      "google.genai.errors.ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The service is currently unavailable.', 'status': 'UNAVAILABLE'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1593784/3084300841.py\", line 41, in perform_inference_single_async\n",
      "    except genai_errors.NotFoundError: raise FileNotFoundError(f\"File API '{file_api_name}' not found.\")\n",
      "AttributeError: module 'google.genai.errors' has no attribute 'NotFoundError'\n",
      "2025-05-09 21:45:26,875 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:45:27,151 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:45:27,153 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:45:28,288 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:45:28,312 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:45:52,110 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:45:52,113 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:45:52,115 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:47:32,419 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:47:32,422 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:47:32,423 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:47:32,915 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:47:32,917 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:48:49,708 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:48:49,711 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:48:49,713 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:48:50,166 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:48:50,168 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:49:11,437 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:49:11,443 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:49:11,444 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:49:11,911 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:49:11,914 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:49:26,499 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:49:26,502 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:49:26,503 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:49:45,427 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:49:45,430 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:49:45,431 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:49:45,710 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:49:45,712 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:49:46,947 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:49:46,949 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:50:23,039 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 500 Internal Server Error\"\n",
      "2025-05-09 21:50:23,042 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:50:37,918 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:50:37,921 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:50:37,924 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:50:56,652 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:50:57,425 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:50:57,427 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:51:56,344 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 502 Bad Gateway\"\n",
      "2025-05-09 21:51:56,346 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:07,567 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:52:07,570 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:52:07,573 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:07,939 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:52:07,941 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:08,525 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:52:08,527 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:09,016 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:52:09,019 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:11,355 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:52:11,357 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:13,333 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:52:13,335 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:13,960 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:52:13,963 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:14,397 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:52:14,399 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:14,982 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:52:14,984 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:16,892 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:52:16,896 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:19,408 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:52:19,411 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:40,567 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:52:40,570 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:52:40,571 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:41,017 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:52:41,019 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:41,827 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:52:41,828 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:52:42,397 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:52:42,400 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:53:54,795 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:53:54,798 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:53:54,800 - ERROR - Video_ID fBExhFE2-ew (Async): Unexpected Input Prep Error: module 'google.genai.errors' has no attribute 'NotFoundError'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1593784/3084300841.py\", line 39, in perform_inference_single_async\n",
      "    file_object = await client.aio.files.get(name=file_api_name)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/files.py\", line 985, in get\n",
      "    response_dict = await self._api_client.async_request(\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/_api_client.py\", line 776, in async_request\n",
      "    result = await self._async_request(http_request=http_request, stream=False)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/_api_client.py\", line 720, in _async_request\n",
      "    await errors.APIError.raise_for_async_response(response)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/errors.py\", line 131, in raise_for_async_response\n",
      "    raise ServerError(status_code, response_json, response)\n",
      "google.genai.errors.ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The service is currently unavailable.', 'status': 'UNAVAILABLE'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1593784/3084300841.py\", line 41, in perform_inference_single_async\n",
      "    except genai_errors.NotFoundError: raise FileNotFoundError(f\"File API '{file_api_name}' not found.\")\n",
      "AttributeError: module 'google.genai.errors' has no attribute 'NotFoundError'\n",
      "2025-05-09 21:53:54,802 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:54:04,355 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:54:05,270 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:54:15,352 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:54:15,355 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:54:15,356 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:54:27,844 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:54:27,847 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:54:27,848 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:54:46,967 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:54:46,969 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:54:46,970 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:55:22,584 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:55:22,586 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:55:22,587 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:55:41,645 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:55:41,655 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:55:41,656 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:55:58,574 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:55:58,576 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:55:58,577 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:56:53,888 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:56:53,892 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:56:53,892 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:56:55,214 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:56:55,217 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:56:55,671 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:56:55,673 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:57:03,445 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:57:03,448 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:57:03,449 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:57:13,691 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:57:13,694 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:57:13,697 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:58:24,055 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 500 Internal Server Error\"\n",
      "2025-05-09 21:58:24,058 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:58:24,637 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:58:24,642 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:58:26,820 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:58:26,826 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:58:27,128 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 503 Service Unavailable\"\n",
      "2025-05-09 21:58:27,130 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:59:04,888 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:59:04,891 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:59:04,892 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 21:59:30,807 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 21:59:30,810 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 21:59:30,812 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:00:10,432 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:00:10,436 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:00:10,437 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:00:36,580 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:00:36,608 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:00:36,608 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:00:59,979 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:00:59,982 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:00:59,985 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:01:31,195 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:01:31,198 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:01:31,199 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:02:18,778 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:02:18,781 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:02:18,782 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:02:48,903 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:02:48,906 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:02:48,908 - ERROR - Video_ID 5NgU4w_qPBg (Async): Unexpected Input Prep Error: module 'google.genai.errors' has no attribute 'NotFoundError'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1593784/3084300841.py\", line 39, in perform_inference_single_async\n",
      "    file_object = await client.aio.files.get(name=file_api_name)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/files.py\", line 985, in get\n",
      "    response_dict = await self._api_client.async_request(\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/_api_client.py\", line 776, in async_request\n",
      "    result = await self._async_request(http_request=http_request, stream=False)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/_api_client.py\", line 720, in _async_request\n",
      "    await errors.APIError.raise_for_async_response(response)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/errors.py\", line 131, in raise_for_async_response\n",
      "    raise ServerError(status_code, response_json, response)\n",
      "google.genai.errors.ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The service is currently unavailable.', 'status': 'UNAVAILABLE'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1593784/3084300841.py\", line 41, in perform_inference_single_async\n",
      "    except genai_errors.NotFoundError: raise FileNotFoundError(f\"File API '{file_api_name}' not found.\")\n",
      "AttributeError: module 'google.genai.errors' has no attribute 'NotFoundError'\n",
      "2025-05-09 22:02:48,911 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:03:14,940 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:03:14,943 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:03:14,944 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:03:39,597 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:03:39,600 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:03:39,601 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:04:03,049 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:04:03,052 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:04:03,054 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:04:27,472 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:04:27,475 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:04:27,476 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:04:52,300 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:04:52,303 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:04:52,304 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:05:37,247 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:05:37,251 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:05:37,254 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:06:10,481 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:06:10,484 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:06:10,485 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:06:38,778 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:06:38,780 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:06:38,781 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:07:10,042 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:07:10,045 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:07:10,046 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:07:38,669 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:07:38,671 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:07:38,672 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:08:14,210 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:08:14,213 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:08:14,214 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:08:59,419 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:08:59,422 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:08:59,424 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:09:31,027 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:09:31,030 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:09:31,031 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:10:17,362 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:10:17,365 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:10:17,366 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:10:44,413 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:10:44,416 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:10:44,419 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:11:15,373 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:11:15,376 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:11:15,377 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:12:09,995 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:12:09,997 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:12:09,998 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:12:33,886 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:12:33,888 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:12:33,890 - ERROR - Video_ID vwajGCpsoJ0 (Async): Unexpected Input Prep Error: module 'google.genai.errors' has no attribute 'NotFoundError'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1593784/3084300841.py\", line 39, in perform_inference_single_async\n",
      "    file_object = await client.aio.files.get(name=file_api_name)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/files.py\", line 985, in get\n",
      "    response_dict = await self._api_client.async_request(\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/_api_client.py\", line 776, in async_request\n",
      "    result = await self._async_request(http_request=http_request, stream=False)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/_api_client.py\", line 720, in _async_request\n",
      "    await errors.APIError.raise_for_async_response(response)\n",
      "  File \"/data/long/miniconda3/envs/ccot/lib/python3.10/site-packages/google/genai/errors.py\", line 131, in raise_for_async_response\n",
      "    raise ServerError(status_code, response_json, response)\n",
      "google.genai.errors.ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The service is currently unavailable.', 'status': 'UNAVAILABLE'}}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1593784/3084300841.py\", line 41, in perform_inference_single_async\n",
      "    except genai_errors.NotFoundError: raise FileNotFoundError(f\"File API '{file_api_name}' not found.\")\n",
      "AttributeError: module 'google.genai.errors' has no attribute 'NotFoundError'\n",
      "2025-05-09 22:12:33,891 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-09 22:13:00,955 - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 22:13:00,958 - INFO - AFC remote call 1 is done.\n",
      "2025-05-09 22:13:00,958 - INFO - AFC is enabled with max remote calls: 10.\n"
     ]
    }
   ],
   "source": [
    "# --- Inference Functions ---\n",
    "\n",
    "async def perform_inference_single_async(\n",
    "    video_info: Dict,\n",
    "    client: Any,\n",
    "    semaphore: asyncio.Semaphore,\n",
    "    rate_limiter: Optional[AsyncRateLimiter],\n",
    "    results_queue: asyncio.Queue\n",
    ") -> None: # Return None as result is put in queue\n",
    "    \"\"\"\n",
    "    Async inference for one question, putting the result into a queue.\n",
    "    \"\"\"\n",
    "    video_id = video_info.get(\"video_id\", \"?\")\n",
    "    # prompt_text = build_prompt(question_info)\n",
    "    gcs_uri = video_info.get(\"gcs_uri\")\n",
    "    file_api_name = video_info.get(\"file_api_name\")\n",
    "    start_time = time.time()\n",
    "    result = None # Default result in case of early exit\n",
    "\n",
    "    \n",
    "    questions_list = generated_questions_dict.get(video_id, [])[:3]\n",
    "    \n",
    "    if f\"{video_id}.json\" in os.listdir(ANSWERS_DIR):\n",
    "        logger.info(f\"QID {video_id} (Async): Already processed. Skipping.\")\n",
    "        result = {\"video_id\": video_id, \"questions\": questions_list, \"duration\": 0, \"finish_reason\": \"N/A\", \"status\": \"Already Processed\"}\n",
    "        await results_queue.put(result)\n",
    "        return\n",
    "    # --- Prepare Inputs (Same as before) ---\n",
    "    try:\n",
    "        video_part = None\n",
    "        if USE_VERTEX:\n",
    "            # ... (GCS URI logic) ...\n",
    "            if not gcs_uri: raise ValueError(\"Missing GCS URI.\")\n",
    "            video_part = types.Part.from_uri(mime_type='video/mp4', file_uri=gcs_uri)\n",
    "        else: # Gemini API Mode\n",
    "            # ... (File API get logic) ...\n",
    "             if not file_api_name: raise ValueError(\"Missing File API name.\")\n",
    "             try:\n",
    "                 file_object = await client.aio.files.get(name=file_api_name)\n",
    "                 video_part = file_object\n",
    "             except genai_errors.NotFoundError: raise FileNotFoundError(f\"File API '{file_api_name}' not found.\")\n",
    "             except Exception as e: raise RuntimeError(f\"Failed get File API obj: {e}\")\n",
    "        \n",
    "        if video_part is None: raise RuntimeError(\"Video part preparation failed.\")\n",
    "        \n",
    "    except (ValueError, FileNotFoundError, RuntimeError) as e:\n",
    "        logger.error(f\"QID {video_id} (Async): Input Error - {e}\")\n",
    "        # Put error result in queue\n",
    "        result = {\"video_id\": video_id, \"questions\": questions_list, \"duration\": 0, \"finish_reason\": \"N/A\", \"status\": \"Failed (Input)\"}\n",
    "        await results_queue.put(result)\n",
    "        return # Exit the function\n",
    "    except Exception as e:\n",
    "         logger.error(f\"Video_ID {video_id} (Async): Unexpected Input Prep Error: {e}\", exc_info=True)\n",
    "         result = {\"video_id\": video_id, \"questions\": questions_list, \"duration\": 0, \"finish_reason\": \"N/A\", \"status\": \"Failed (Input Prep)\"}\n",
    "         await results_queue.put(result)\n",
    "         return # Exit the function\n",
    "\n",
    "    # --- Perform Inference with Retries, Semaphore, and Rate Limiting ---\n",
    "    async with semaphore:\n",
    "        for attempt in range(QUESTIONS_MAX_RETRIES + 1):\n",
    "            try:\n",
    "                if rate_limiter: await rate_limiter.acquire()\n",
    "                api_start = time.time()\n",
    "                logger.debug(f\"QID {video_id} (Async): Attempt {attempt + 1} sending request...\")\n",
    "            \n",
    "                try:\n",
    "                    chat: list = []\n",
    "                    \n",
    "                    for idx, q in enumerate(questions_list, 1):\n",
    "                        user_msg = types.Content(role=\"user\",\n",
    "                                                parts=[types.Part.from_text(text=q)])\n",
    "                        # always include video_part\n",
    "                        contents = [video_part] + chat + [user_msg]\n",
    "\n",
    "                        try:\n",
    "                            rsp = client.models.generate_content(model=MODEL_NAME,\n",
    "                                                                contents=contents,\n",
    "                                                                config=CONFIG)\n",
    "                            answer = rsp.text.strip()\n",
    "                            finish_reason = (rsp.candidates[0].finish_reason.name\n",
    "                                if rsp.candidates and rsp.candidates[0].finish_reason else \"UNKNOWN\")\n",
    "                        except Exception as e:\n",
    "                            answer, finish_reason = f\"ERROR: {e}\", \"Failed (API)\"\n",
    "                            \n",
    "                        chat.extend([\n",
    "                            user_msg,\n",
    "                            types.Content(role=\"model\",\n",
    "                                        parts=[types.Part.from_text(text=answer or \"…\")])\n",
    "                        ])\n",
    "                    \n",
    "                    # Save chat history\n",
    "                    \n",
    "                    saved_path = os.path.join(ANSWERS_DIR, f\"{video_id}.json\")\n",
    "                    os.makedirs(os.path.dirname(saved_path), exist_ok=True)\n",
    "                    with open(saved_path, \"w\") as f:\n",
    "                        json.dump(serialize_chat(chat), f, indent=2)\n",
    "                    status = \"Success\"\n",
    "                except Exception as e:\n",
    "                    finish_reason = \"Failed (API)\"\n",
    "                    print(f\"Error in response: {e}\")\n",
    "                    status = \"Blocked/Empty\"\n",
    "                    \n",
    "                result = {\"video_id\": video_id, \"questions\": questions_list, \"duration\": time.time()-start_time, \"finish_reason\": finish_reason, \"status\": status}\n",
    "                await results_queue.put(result)\n",
    "                logger.debug(f\"QID {video_id} (Async): Attempt {attempt + 1} {status} ({time.time()-api_start:.2f}s API / {time.time()-start_time:.2f}s Total). Result queued.\")\n",
    "                return # Exit function after success\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                 result = {\"video_id\": video_id, \"questions\": questions_list, \"duration\": time.time()-start_time, \"finish_reason\": finish_reason, \"status\": \"Failed (Unexpected Error)\"}\n",
    "                 await results_queue.put(result)\n",
    "                 return\n",
    "\n",
    "        # Fallback (Should not be reached if logic above is correct)\n",
    "        logger.error(f\"VideoID {video_id} (Async): Exited retry loop unexpectedly.\")\n",
    "        result = {\"video_id\": video_id, \"questions\": questions_list, \"duration\": time.time()-start_time, \"finish_reason\": \"UNKNOWN\", \"status\": \"Failed (Unknown)\"}\n",
    "        await results_queue.put(result)\n",
    "\n",
    "async def results_writer_task(\n",
    "    queue: asyncio.Queue,\n",
    "    filename: str,\n",
    "    write_batch_size: int = 20, # How many results to buffer before writing\n",
    "    write_interval_sec: float = 10.0 # Max time between writes\n",
    "):\n",
    "    \"\"\"Gets results from queue and writes them to CSV in batches.\"\"\"\n",
    "    results_buffer = []\n",
    "    last_write_time = time.monotonic()\n",
    "    # Define expected header based on the dict keys put in the queue\n",
    "    fieldnames = [\"video_id\", \"questions\", \"status\", \"duration_sec\", \"finish_reason\"]\n",
    "    file_exists = Path(filename).is_file()\n",
    "\n",
    "    logger.info(f\"Writer task started. Writing results to {filename}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Wait for an item with a timeout\n",
    "            result = await asyncio.wait_for(queue.get(), timeout=write_interval_sec)\n",
    "\n",
    "            if result is None: # Signal to terminate\n",
    "                logger.info(\"Writer task received termination signal.\")\n",
    "                break\n",
    "\n",
    "            if isinstance(result, dict):\n",
    "                 # Ensure duration is rounded here before adding to buffer\n",
    "                 result[\"duration_sec\"] = round(result.get(\"duration\", -1), 2)\n",
    "                 # Remove the original 'duration' key if desired\n",
    "                 result.pop('duration', None)\n",
    "                 results_buffer.append(result)\n",
    "            else:\n",
    "                logger.warning(f\"Writer task received non-dict item: {result}\")\n",
    "\n",
    "            queue.task_done() # Signal that the item was processed\n",
    "\n",
    "        except asyncio.TimeoutError:\n",
    "            # Timeout occurred, write buffer if not empty, even if batch size not reached\n",
    "            logger.debug(\"Writer task timeout reached.\")\n",
    "            pass # Continue to buffer check below\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Writer task encountered error getting from queue: {e}\", exc_info=True)\n",
    "            # Decide if this is fatal or if we should try to continue\n",
    "            await asyncio.sleep(1) # Prevent fast spinning on error\n",
    "            continue # Try to continue processing\n",
    "\n",
    "        # Check if we should write the buffer\n",
    "        buffer_size = len(results_buffer)\n",
    "        time_since_last_write = time.monotonic() - last_write_time\n",
    "        should_write = (\n",
    "             buffer_size > 0 and\n",
    "             (buffer_size >= write_batch_size or time_since_last_write >= write_interval_sec)\n",
    "        )\n",
    "\n",
    "        if should_write:\n",
    "            logger.info(f\"Writing batch of {buffer_size} results to {filename}...\")\n",
    "            try:\n",
    "                # Use 'with open' for proper handling\n",
    "                with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                    # Write header only if file didn't exist at the start\n",
    "                    if not file_exists:\n",
    "                        writer.writeheader()\n",
    "                        file_exists = True # Prevent writing header again\n",
    "                    writer.writerows(results_buffer)\n",
    "\n",
    "                results_buffer = [] # Clear buffer after successful write\n",
    "                last_write_time = time.monotonic()\n",
    "                logger.info(f\"Batch written successfully.\")\n",
    "            except IOError as e:\n",
    "                logger.error(f\"IOError writing results batch to {filename}: {e}. Results may be lost.\")\n",
    "                # Optional: Add retry logic here, or store failed batch elsewhere\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error writing results batch: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "    # --- Cleanup: Write any remaining items after receiving None signal ---\n",
    "    if results_buffer:\n",
    "        logger.info(f\"Writing final remaining {len(results_buffer)} results...\")\n",
    "        try:\n",
    "            with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                if not file_exists: writer.writeheader() # Check again in case file was deleted mid-run\n",
    "                writer.writerows(results_buffer)\n",
    "            logger.info(\"Final results written.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error writing final results batch: {e}\")\n",
    "\n",
    "    logger.info(\"Writer task finished.\")\n",
    "\n",
    "# --- Modified Main Execution Function with UI ---\n",
    "async def run_bulk_inference_async():\n",
    "    \"\"\"Runs the bulk inference process asynchronously with queue writer, rate limiting, and UI.\"\"\"\n",
    "\n",
    "    # --- Standard Setup ---\n",
    "    if ai_client is None: logger.error(\"AI Client not initialized.\"); display(Markdown(\"❌ AI Client not initialized.\")); return\n",
    "\n",
    "    # --- Load Data and Prepare Tasks ---\n",
    "    logger.info(\"Loading metadata for inference...\")\n",
    "    \n",
    "    # Load video questions for inference as a dictionaies of dicts\n",
    "    # Each dict contains video_id, gcs_uri, file_api_name, and other metadata\n",
    "    video_questions_for_inference = load_metadata_questions_generation(METADATA_FILE)\n",
    "    print(f\"Loaded {len(video_questions_for_inference)} video questions for inference.\")\n",
    "    \n",
    "    if not video_questions_for_inference: logger.warning(\"No video data ready.\"); return\n",
    "    processed_qids = load_processed_video_ids(ANSWERS_CHECK_DIR)\n",
    "\n",
    "    inference_tasks_input = []\n",
    "    tasks_skipped = 0\n",
    "    required_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "    for video_id, video_info in video_questions_for_inference.items():\n",
    "        # Check if video_id is correct:\n",
    "        if video_id == video_info.get('video_id'):\n",
    "            logger.debug(f\"Video ID {video_id} matches.\")\n",
    "        else:\n",
    "            logger.warning(f\"Video ID mismatch: {video_id} != {video_info.get('video_id')}.\")\n",
    "            continue\n",
    "        \n",
    "        if not video_id or video_id in processed_qids:\n",
    "            tasks_skipped += 1; continue\n",
    "        inference_tasks_input.append(video_info)\n",
    "\n",
    "    total_tasks = len(inference_tasks_input)\n",
    "    logger.info(f\"Prepared {total_tasks} new inference tasks. Skipped {tasks_skipped}.\" )\n",
    "    if total_tasks == 0: logger.info(\"No new questions to process.\"); return\n",
    "\n",
    "    # --- Setup Rate Limiter ---\n",
    "    rate_limiter = None\n",
    "    if REQUESTS_PER_MINUTE is not None and REQUESTS_PER_MINUTE > 0:\n",
    "        capacity = 10 # Example: Keep burst capacity moderate\n",
    "        rate_limiter = AsyncRateLimiter(rate=REQUESTS_PER_MINUTE, period=60.0, capacity=capacity)\n",
    "        logger.info(f\"Rate limiting enabled: {REQUESTS_PER_MINUTE} RPM, Capacity: {capacity}\")\n",
    "    else:\n",
    "        logger.info(\"Rate limiting disabled.\")\n",
    "\n",
    "    # --- Create Queue and Start Writer Task ---\n",
    "    results_queue = asyncio.Queue()\n",
    "    writer_handle = asyncio.create_task(\n",
    "        results_writer_task(results_queue, ANSWERS_CHECK_DIR)\n",
    "    )\n",
    "\n",
    "    # --- Schedule and Run Inference Tasks with Progress Bar ---\n",
    "    logger.info(f\"Starting async inference for {total_tasks} questions (Concurrency: {MAX_ASYNC_WORKERS})...\")\n",
    "    semaphore = asyncio.Semaphore(MAX_ASYNC_WORKERS)\n",
    "    start_bulk_time = time.time()\n",
    "\n",
    "    # Create coroutines\n",
    "    inference_coroutines = [\n",
    "        perform_inference_single_async(q_info, ai_client, semaphore, rate_limiter, results_queue)\n",
    "        for q_info in inference_tasks_input\n",
    "    ]\n",
    "\n",
    "    # --- Use asyncio.as_completed with tqdm_notebook for live progress ---\n",
    "    completed_count = 0\n",
    "    gather_exception = None\n",
    "    try:\n",
    "        # Create a future for each coroutine to track completion\n",
    "        tasks = [asyncio.ensure_future(coro) for coro in inference_coroutines]\n",
    "        # Use tqdm with as_completed\n",
    "        for future in tqdm(asyncio.as_completed(tasks), total=total_tasks, desc=\"Async Inference\"):\n",
    "            try:\n",
    "                await future # Wait for the next task to complete, raise exception if task failed\n",
    "                completed_count += 1\n",
    "            except Exception as task_exc:\n",
    "                 # Log error from individual task if it wasn't caught inside\n",
    "                 logger.error(f\"Error surfaced from an inference task: {task_exc}\", exc_info=False)\n",
    "                 # Optionally decide if you want to stop processing other tasks\n",
    "                 # gather_exception = task_exc # Store first exception\n",
    "                 # break # Or continue processing others\n",
    "    except Exception as outer_exc:\n",
    "        # Catch errors during the setup/iteration of as_completed itself\n",
    "        logger.error(f\"Error during as_completed processing: {outer_exc}\", exc_info=True)\n",
    "        gather_exception = outer_exc\n",
    "\n",
    "    # Ensure all tasks are awaited even if we broke early due to an error in one task\n",
    "    # (This might be redundant if as_completed handles cancellation correctly, but safer)\n",
    "    await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    bulk_duration = time.time() - start_bulk_time\n",
    "    logger.info(f\"Async inference task processing finished in {bulk_duration:.2f} seconds. Completed: {completed_count}/{total_tasks}.\")\n",
    "    if gather_exception:\n",
    "         logger.error(f\"Bulk inference encountered errors: {gather_exception}\")\n",
    "\n",
    "    # --- Signal Writer Task to Finish (ALWAYS DO THIS) ---\n",
    "    logger.info(\"Signaling writer task to complete...\")\n",
    "    await results_queue.put(None)\n",
    "\n",
    "    # --- Wait for Writer Task to Finish (ALWAYS DO THIS) ---\n",
    "    logger.info(\"Waiting for writer task to finish writing remaining results...\")\n",
    "    try:\n",
    "        await writer_handle # Wait until the writer processes the None signal and exits\n",
    "        logger.info(\"Writer task has finished.\")\n",
    "    except Exception as writer_exc:\n",
    "        logger.error(f\"Error waiting for writer task: {writer_exc}\", exc_info=True)\n",
    "\n",
    "    # --- Final Summary ---\n",
    "    logger.info(f\"Bulk inference process complete. See logs and {ANSWERS_CHECK_DIR}.\")\n",
    "    # Remove handler to prevent duplicate logs on re-run\n",
    "\n",
    "# --- Run the async function ---\n",
    "asyncio.run(run_bulk_inference_async())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfe17f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in generated_questions/gemini-2.0-flash/chat_history: 79\n"
     ]
    }
   ],
   "source": [
    "# Calucalte the numbers of file inside ANSWERS_DIR\n",
    "import os\n",
    "num_files = len([f for f in os.listdir(ANSWERS_DIR) if os.path.isfile(os.path.join(ANSWERS_DIR, f))])\n",
    "\n",
    "print(f\"Number of files in {ANSWERS_DIR}: {num_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38c4f7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(ANSWERS_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55000b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if f\"{video_id}.json\" in :\n",
    "    logger.info(f\"QID {video_id} (Async): Already processed. Skipping.\")\n",
    "    result = {\"video_id\": video_id, \"questions\": questions_list, \"duration\": 0, \"finish_reason\": \"N/A\", \"status\": \"Already Processed\"}\n",
    "    await results_queue.put(result)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e0943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:06:41,182 - INFO - Loaded results from generated_questions/gemini-2.0-flash/answers.csv (1 rows).\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Results Summary: 1 Success, 0 Failed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:06:41,187 - INFO - Removed duplicates, remaining 1 unique QIDs.\n"
     ]
    }
   ],
   "source": [
    "interim_cleanup_and_report(ANSWERS_CHECK_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
