{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9874197",
   "metadata": {},
   "source": [
    "#  Vertex Inference Unified\n",
    "\n",
    "This notebook uses the unified `google-genai` library (imported as `from google import genai`). It supports:\n",
    "- **Vertex AI Backend:** Uploads videos to GCS during the 'Prepare' step.\n",
    "- **Gemini API Backend:** Uploads videos using the **File API** during the 'Prepare' step.\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1.  **Import Libraries & Configure.**\n",
    "2.  **Config** - Set up the configuration for the pipeline, including the model, model config, prompts, and prompt config.\n",
    "2.  **Initialize Clients:** Set up AI client and Storage client.\n",
    "3.  **(Only the first time) Fetch Dataset:** Downloads metadata from HuggingFace.\n",
    "4.  **(Only the first time on each API type) Download, Extract & Prepare Videos:** Downloads, extracts, uploads (GCS/File API). Updates metadata.\n",
    "5.  **Bulk Inference (Async):** Performs inference using pre-uploaded video resources.\n",
    "6.  **Single Prompt Testing (UI):** Allows interactive testing of the video with prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19484ef",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "1. After switching from Vertex to Gemini and vice versa, be sure to follow the steps:\n",
    "    - Run all cells in order to re-upload the videos to the correct storage client, you can enable the SKIP_DOWNLOAD and SKIP_EXTRACT flags to skip the download and extraction steps. Only the upload step is needed\n",
    "\n",
    "2. Gemini API's file client has a expiry time of 1 day or so for the uploaded files. You may need to follow the steps above to re-upload the files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c07e40",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d7dd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`google.genai` SDK and helpers imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports (Corrected for `google.genai`)\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import datetime\n",
    "import zipfile\n",
    "import math\n",
    "import sys\n",
    "import asyncio\n",
    "from typing import Dict, List, Optional, Set, Tuple, Any\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import fractions\n",
    "\n",
    "# Google Cloud & AI Libraries (Unified SDK)\n",
    "try:\n",
    "    import google.genai as genai\n",
    "    from google.genai import types\n",
    "    from google.genai import errors as genai_errors\n",
    "    from google.api_core import exceptions as api_core_exceptions\n",
    "    # GCS Client (Optional, for Vertex Mode)\n",
    "    try:\n",
    "        from google.cloud import storage\n",
    "        GCS_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"INFO: google-cloud-storage not found. Vertex AI GCS operations unavailable.\")\n",
    "        storage = None\n",
    "        GCS_AVAILABLE = False\n",
    "    print(\"`google.genai` SDK and helpers imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR: Failed to import Google libraries: {e}. Install: pip install google-genai google-api-core google-cloud-storage\")\n",
    "    genai = None; types = None; genai_errors = None; api_core_exceptions = None\n",
    "    storage = None; GCS_AVAILABLE = False\n",
    "    raise ImportError(\"FATAL: `google.genai` or `google-api-core` SDK not found.\")\n",
    "\n",
    "# Data Handling & Progress\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# UI Elements\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "\n",
    "# Async in Notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbce404",
   "metadata": {},
   "source": [
    "## Config Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5e7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GCP Configuration ---\n",
    "\n",
    "PROJECT_ID = \"tiktokllm\" # Your Google Cloud Project ID (Needed for GCS and Vertex AI mode)\n",
    "LOCATION = \"us-central1\"      # Your Google Cloud Region (Needed for Vertex AI mode)\n",
    "GCS_BUCKET = \"seekdeepr4-ml-storage\" # Your GCS bucket name (Needed for video storage)\n",
    "\n",
    "# --- Choose Backend Mode ---\n",
    "# Set USE_VERTEX to True to use the Vertex AI backend (requires ADC or service account auth).\n",
    "# Set USE_VERTEX to False to use the Gemini API backend (requires GEMINI_API_KEY).\n",
    "USE_VERTEX = True  # <-- CHANGE THIS TO True TO USE VERTEX AI\n",
    "\n",
    "# --- Gemini API Key (Only required if USE_VERTEX is False) ---\n",
    "# IMPORTANT: Replace with your actual Gemini API Key if USE_VERTEX is False.\n",
    "# Consider loading from environment variables (GOOGLE_API_KEY) or a secure secrets manager.\n",
    "GEMINI_API_KEY = \"\"  # Replace with your actual Gemini API Key\n",
    "\n",
    "\n",
    "\n",
    "# --- File Paths ---\n",
    "DATASET_CSV = \"dataset.csv\"               # Input dataset metadata from HuggingFace\n",
    "METADATA_FILE = \"video_metadata_vertex_inj.csv\" if USE_VERTEX else \"video_metadata_non_vertex.csv\"      # Stores video info: video_id, local_path, gcs_uri (if Vertex), question data\n",
    "RESULTS_FILE = \"results_noncot_full_inference.csv\"              # Output file for inference predictions\n",
    "DOWNLOADS_DIR = \"downloads\"               # Directory for downloaded zip file\n",
    "EXTRACTED_VIDEOS_DIR = \"extracted_videos\" # Directory storing extracted .mp4 files locally\n",
    "SPEED_VIDEOS_DIR = \"speed_videos\"         # Stores sped up/slowed down videos\n",
    "HF_CACHE_DIR = \"./hf_cache\"               # Cache directory for HuggingFace datasets\n",
    "\n",
    "# --- Step 1: Fetch Dataset Configuration ---\n",
    "HF_DATASET_NAME = \"lmms-lab/AISG_Challenge\" # HuggingFace dataset identifier\n",
    "HF_DATASET_SPLIT = \"test\"                 # Dataset split to use\n",
    "SKIP_FETCH = False                        # Set True to skip fetching if DATASET_CSV exists\n",
    "\n",
    "# --- Step 2: Download & Prepare Videos Configuration ---\n",
    "VIDEO_ZIP_URL = \"https://huggingface.co/datasets/lmms-lab/AISG_Challenge/resolve/main/Benchmark-AllVideos-HQ-Encoded-challenge.zip?download=true\"\n",
    "ZIP_FILE_NAME = \"all_videos.zip\"\n",
    "SKIP_DOWNLOAD_ZIP = True                 # Set True to skip downloading if zip exists\n",
    "SKIP_EXTRACT = True                      # Set True to skip extraction if videos exist locally\n",
    "SKIP_PREPARE = False                      # Set True to skip video preparation (GCS upload for Vertex, metadata update)\n",
    "MAX_VIDEOS_TO_PROCESS = None              # Limit videos for testing (e.g., 5), None for all\n",
    "UPLOAD_BATCH_SIZE_GCS = 10                # Batch size for GCS uploads (Vertex mode only)\n",
    "\n",
    "# --- Inference Configuration ---\n",
    "# Choose a model name compatible with your selected method (Vertex AI or Gemini API)\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# Vertex AI: gemini-2.0-flash, gemini-2.0-flash-lite, gemini-2.0-pro-exp-02-05, gemini-2.0-flash-thinking-exp-01-21\n",
    "# Rate limits: https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#gemini-2.0-flash\n",
    "# Basically 500 requests per minute for 2.0-flash and 2.0-flash-lite (unlimited), 10 requests per minute for 2.0-pro-exp-02-05, gemini-2.5-flash-preview-04-17\n",
    "\n",
    "# Gemini API: gemini-2.0-flash, gemini-2.0-flash-lite, gemini-2.0-flash-thinking-exp-01-21, gemini-2.5-pro-exp-03-25\n",
    "# Rate limits: https://ai.google.dev/gemini-api/docs/rate-limits#tier-1\n",
    "# For free tier: 30 requests per minute for 2.0-flash and 2.0-flash-lite, 10 requests per minute for 2.0-pro-exp-02-05\n",
    "# For tier-1: 2000 requests per minute for 2.0-flash and 2.0-flash-lite (have to pay), 10 requests per minute for 2.0-pro-exp-02-05 and gemini-2.0-flash-thinking-exp-01-21, gemini-2.5-flash-preview-04-17\n",
    "\n",
    "# 1.0=normal speed, 0.5=half speed, etc.\n",
    "VIDEO_SPEED_FACTOR = 0.5\n",
    "\n",
    "# --- Setup Derived Paths & Directories ---\n",
    "zip_file_path = Path(DOWNLOADS_DIR) / ZIP_FILE_NAME\n",
    "extracted_videos_path = Path(EXTRACTED_VIDEOS_DIR)\n",
    "speed_videos_path = Path(SPEED_VIDEOS_DIR) / str(VIDEO_SPEED_FACTOR)\n",
    "Path(DOWNLOADS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "extracted_videos_path.mkdir(parents=True, exist_ok=True)\n",
    "speed_videos_path.mkdir(parents=True, exist_ok=True)\n",
    "Path(HF_CACHE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler(sys.stdout)])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Configuration Validation & Display --- #\n",
    "warnings_found = False\n",
    "if USE_VERTEX:\n",
    "    if not PROJECT_ID or PROJECT_ID == \"your-gcp-project-id\":\n",
    "        logger.error(\"Vertex AI mode requires PROJECT_ID to be set.\")\n",
    "        warnings_found = True\n",
    "    if not LOCATION:\n",
    "        logger.error(\"Vertex AI mode requires LOCATION to be set.\")\n",
    "        warnings_found = True\n",
    "    if not GCS_BUCKET or GCS_BUCKET == \"your-gcs-bucket-name\":\n",
    "        logger.error(\"Vertex AI mode requires GCS_BUCKET for video uploads.\")\n",
    "        warnings_found = True\n",
    "    if not GCS_AVAILABLE:\n",
    "        logger.error(\"Vertex AI mode requires 'google-cloud-storage', but it's not installed.\")\n",
    "        warnings_found = True\n",
    "else: # Gemini API Mode\n",
    "    # Check API Key (explicit or env var)\n",
    "    effective_api_key = GEMINI_API_KEY if GEMINI_API_KEY != \"YOUR_API_KEY_HERE\" else os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not effective_api_key:\n",
    "        logger.error(\"Gemini API mode requires GEMINI_API_KEY or GOOGLE_API_KEY environment variable.\")\n",
    "        warnings_found = True\n",
    "    else:\n",
    "        # Don't store the key in the config display if loaded from env\n",
    "        if GEMINI_API_KEY == \"YOUR_API_KEY_HERE\" and os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "            GEMINI_API_KEY = \"(Loaded from GOOGLE_API_KEY env var)\"\n",
    "        logger.info(\"Gemini API mode configured. Videos will be uploaded via File API.\")\n",
    "\n",
    "if warnings_found:\n",
    "    print(\"\\n\\n************************* WARNING *************************\")\n",
    "    print(\"Configuration errors detected above. Execution might fail.\")\n",
    "    print(\"***********************************************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d8e3e7",
   "metadata": {},
   "source": [
    "# Main Model Selection (Innovation 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd933ec",
   "metadata": {},
   "source": [
    "### NonCoT output models for Bulk Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d36ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see all available models, go into the NonCoT_output_models.py file\n",
    "# NonCoT models are used for Bulk Inference\n",
    "from models.NonCoT_output_models import get_non_cot_model\n",
    "non_cot_model_list = [\"gemini-2.5-flash-preview-04-17\", \"gemini-2.5-pro-exp-03-25\"]\n",
    "\n",
    "MODEL_NAME, SYSTEM_PROMPT, PROMPT_TEMPLATES, CONFIG, REQUESTS_PER_MINUTE, MAX_RETRIES, MAX_ASYNC_WORKERS  = get_non_cot_model(non_cot_model_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3fa7c",
   "metadata": {},
   "source": [
    "### COT output models for Bulk Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10dd6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To See All Available Models, Go into the CoT_ouput_models.py file\n",
    "# CoT models are used for Bulk Inference\n",
    "from models.CoT_ouput_models import get_cot_model\n",
    "CoT_model_list = [\"gemini-2.0-flash\",\"gemini-2.5-pro-preview-05-06\", \"gemini-2.5-pro-preview-03-25\"]\n",
    "MODEL_NAME, SYSTEM_PROMPT, PROMPT_TEMPLATES, CONFIG, REQUESTS_PER_MINUTE, MAX_RETRIES, MAX_ASYNC_WORKERS = get_cot_model(CoT_model_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f03ecf",
   "metadata": {},
   "source": [
    "### Summary Output Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f92ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see all available models, go into the Summary_models.py file\n",
    "from models.Summary_models import get_summary_model\n",
    "summary_model_list = [\"gemini-2.0-flash-ver1\", \"gemini-2.0-flash-ver2\", \"gemini-2.0-flash-ver3\"]\n",
    "\n",
    "QUESTION_MODEL_NAME, QUESTION_SYSTEM_PROMPT, QUESTION_CONFIG = get_summary_model(summary_model_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c003849",
   "metadata": {},
   "source": [
    "### Generated Questions Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eaea218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To See All Available Models, Go into the Generating_Questions_models.py file\n",
    "# Models are used to generate the questions\n",
    "from models.Generating_Questions_models import get_brainstorm_prompt\n",
    "CoT_model_list = [\"gemini-2.0-flash\",\"gemini-2.5-pro-preview-05-06\"]\n",
    "QUESTIONS_MODEL_NAME, QUESTIONS_PROMPT, QUESTIONS_SCHEMA, QUESTIONS_CONFIG, QUESTIONS_REQUESTS_PER_MINUTE, QUESTIONS_MAX_RETRIES, QUESTIONS_MAX_ASYNC_WORKERS = get_brainstorm_prompt(CoT_model_list[1], number_of_questions= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc5d20",
   "metadata": {},
   "source": [
    "# Basic Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563fba21",
   "metadata": {},
   "source": [
    "## Initialize Google Cloud Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a54202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Initializing Generative AI Client (`google.genai`)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Vertex AI backend (Project: tiktokllm, Loc: us-central1)..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✅ Vertex AI Client Initialized."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Initializing GCS Client (Vertex Mode Only)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✅ GCS Client Initialized (Bucket: 'seekdeepr4-ml-storage')."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✅ Client initialization complete."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "storage_client = None\n",
    "ai_client = None\n",
    "\n",
    "# --- Initialize Generative AI Client (`google.genai`) --- #\n",
    "display(Markdown(\"### Initializing Generative AI Client (`google.genai`)\"))\n",
    "try:\n",
    "    if USE_VERTEX:\n",
    "        display(Markdown(f\"Vertex AI backend (Project: {PROJECT_ID}, Loc: {LOCATION})...\"))\n",
    "        if not PROJECT_ID or not LOCATION or PROJECT_ID == \"your-gcp-project-id\":\n",
    "             raise ValueError(\"PROJECT_ID/LOCATION invalid for Vertex AI.\")\n",
    "        # Initialize Client for Vertex\n",
    "        ai_client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
    "        display(Markdown(f\"✅ Vertex AI Client Initialized.\"))\n",
    "    else: # Gemini API Mode\n",
    "        display(Markdown(\"Gemini API backend (using API Key)...\"))\n",
    "        effective_api_key = GEMINI_API_KEY if GEMINI_API_KEY != \"YOUR_API_KEY_HERE\" else os.environ.get(\"GOOGLE_API_KEY\")\n",
    "        if not effective_api_key:\n",
    "             if os.environ.get(\"GOOGLE_API_KEY\"): effective_api_key = None # Client uses env var\n",
    "             else: raise ValueError(\"Gemini API Key required but not found.\")\n",
    "        # Initialize Client for Gemini API\n",
    "        ai_client = genai.Client(api_key=effective_api_key, vertexai=False)\n",
    "        display(Markdown(f\"✅ Gemini API Client Initialized.\"))\n",
    "\n",
    "except ValueError as ve: display(Markdown(f\"❌ **Config Error:** {ve}\")); ai_client = None\n",
    "except Exception as e: display(Markdown(f\"❌ **AI Client Error:** {e}.\")); logger.error(\"AI Client Init Failed\", exc_info=True); ai_client = None\n",
    "\n",
    "# --- Initialize Storage Client (ONLY for Vertex AI mode) --- #\n",
    "if USE_VERTEX:\n",
    "    display(Markdown(\"### Initializing GCS Client (Vertex Mode Only)\"))\n",
    "    if not GCS_AVAILABLE: display(Markdown(\"❌ GCS lib missing.\")); raise RuntimeError(\"Missing GCS lib.\")\n",
    "    if not GCS_BUCKET or GCS_BUCKET == \"your-gcs-bucket-name\": display(Markdown(\"❌ GCS_BUCKET needed.\")); raise ValueError(\"GCS_BUCKET required.\")\n",
    "    try:\n",
    "        storage_client = storage.Client(project=PROJECT_ID)\n",
    "        if not storage_client.bucket(GCS_BUCKET).exists(): display(Markdown(f\"⚠️ GCS Bucket `{GCS_BUCKET}` inaccessible.\"))\n",
    "        else: display(Markdown(f\"✅ GCS Client Initialized (Bucket: '{GCS_BUCKET}').\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **GCS Client Error:** {e}.\")); logger.error(\"GCS Client Init Failed\", exc_info=True)\n",
    "        if not SKIP_PREPARE: raise RuntimeError(\"GCS client failed.\")\n",
    "        else: display(Markdown(\"⚠️ GCS client failed, but skipping prep.\"))\n",
    "else:\n",
    "    display(Markdown(\"### Initializing Gemini API Client (File API)\"))\n",
    "    try:\n",
    "        storage_client = ai_client.files\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **Gemini File API Client Error:** {e}.\")); logger.error(\"Gemini API Client Init Failed\", exc_info=True)\n",
    "    display(Markdown(f\"✅ Gemini File API Client Initialized.\"))\n",
    "\n",
    "# --- Final Checks --- #\n",
    "if ai_client is None: raise RuntimeError(\"AI client failed.\")\n",
    "if USE_VERTEX and storage_client is None and not SKIP_PREPARE: raise RuntimeError(\"GCS client failed for Vertex prep.\")\n",
    "display(Markdown(\"✅ Client initialization complete.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6c267",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4fc9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File/Data Handling ---\n",
    "def load_processed_qids(filename: str) -> Set[str]:\n",
    "    processed_qids = set()\n",
    "    if Path(filename).is_file():\n",
    "        try:\n",
    "            df = pd.read_csv(filename, usecols=['qid'], dtype={'qid': str}, on_bad_lines='warn')\n",
    "            processed_qids = set(df['qid'].dropna().unique())\n",
    "            logger.info(f\"Loaded {len(processed_qids)} processed QIDs from {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not read QIDs from {filename}: {e}. Assuming zero processed.\")\n",
    "    return processed_qids\n",
    "\n",
    "def download_file_with_progress(url: str, destination: Path):\n",
    "    logger.info(f\"Downloading {url} to {destination}...\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=600)\n",
    "        response.raise_for_status()\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024 * 1024\n",
    "        with open(destination, 'wb') as f, tqdm(\n",
    "            desc=f\"Downloading {destination.name}\", total=total_size, unit='iB', unit_scale=True, unit_divisor=1024\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                size = f.write(data)\n",
    "                bar.update(size)\n",
    "        if total_size != 0 and bar.n != total_size:\n",
    "            destination.unlink(missing_ok=True)\n",
    "            raise RuntimeError(f\"Download size mismatch for {destination.name}.\")\n",
    "        logger.info(f\"Successfully downloaded {destination}\")\n",
    "    except Exception as e:\n",
    "        destination.unlink(missing_ok=True)\n",
    "        logger.error(f\"Download failed for {url}: {e}\")\n",
    "        raise\n",
    "\n",
    "def extract_zip(zip_path: Path, extract_to: Path):\n",
    "    logger.info(f\"Extracting {zip_path.name} to {extract_to}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            members = [m for m in zip_ref.namelist() if not m.startswith('__MACOSX/') and not m.endswith('.DS_Store')]\n",
    "            with tqdm(total=len(members), desc=f\"Extracting {zip_path.name}\") as pbar:\n",
    "                for member in members:\n",
    "                    zip_ref.extract(member=member, path=extract_to)\n",
    "                    pbar.update(1)\n",
    "        logger.info(f\"Successfully extracted {zip_path} to {extract_to}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Extraction error: {e}\")\n",
    "        raise\n",
    "    \n",
    "def move_videos_to_main_directory(base_path):\n",
    "    \"\"\"Find all MP4 files in subdirectories and move them to the main directory.\"\"\"\n",
    "    logger.info(f\"Moving all videos to main directory: {base_path}\")\n",
    "    moved_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    # Find all MP4 files in subdirectories (but not in the main directory)\n",
    "    for file_path in list(base_path.glob('**/*.mp4')):\n",
    "        # Skip files already in the main directory or hidden Mac files\n",
    "        if file_path.parent == base_path or file_path.name.startswith('._'):\n",
    "            continue\n",
    "            \n",
    "        # Destination in the main directory\n",
    "        dest_path = base_path / file_path.name\n",
    "        \n",
    "        try:\n",
    "            # Move the file\n",
    "            shutil.move(str(file_path), str(dest_path))\n",
    "            moved_count += 1\n",
    "            if moved_count % 50 == 0:\n",
    "                logger.info(f\"Moved {moved_count} videos so far...\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error moving {file_path}: {e}\")\n",
    "            failed_count += 1\n",
    "    \n",
    "    logger.info(f\"Moved {moved_count} videos to main directory. Failed: {failed_count}\")\n",
    "    \n",
    "\n",
    "def create_or_update_metadata(metadata_path: str, dataset_df: pd.DataFrame, video_updates: Dict[str, Dict]):\n",
    "    try:\n",
    "        required_cols = ['video_id', 'qid']\n",
    "        update_cols = ['local_path', 'gcs_uri', 'file_api_name', 'status']\n",
    "        dtype_map = {'video_id': str, 'qid': str} # Ensure IDs are strings\n",
    "\n",
    "        if not Path(metadata_path).is_file():\n",
    "            logger.info(f\"Creating metadata file: {metadata_path}\")\n",
    "            meta_df = dataset_df.copy()\n",
    "            for col in update_cols: meta_df[col] = pd.NA\n",
    "            meta_df['status'] = 'pending'\n",
    "        else:\n",
    "            logger.debug(f\"Loading existing metadata: {metadata_path}\")\n",
    "            meta_df = pd.read_csv(metadata_path, dtype=dtype_map)\n",
    "            for col in update_cols: # Add missing update columns if needed\n",
    "                 if col not in meta_df.columns: meta_df[col] = pd.NA\n",
    "\n",
    "        if not all(col in meta_df.columns for col in required_cols):\n",
    "            raise ValueError(f\"Metadata missing required columns ({required_cols}).\")\n",
    "\n",
    "        updates_df = pd.DataFrame.from_dict(video_updates, orient='index')\n",
    "        updates_df.index.name = 'video_id'\n",
    "        updates_df.reset_index(inplace=True)\n",
    "        updates_df['video_id'] = updates_df['video_id'].astype(str)\n",
    "\n",
    "        # Use merge for robust updating across potentially multiple rows per video_id\n",
    "        # First, prepare updates DF with only the necessary columns (video_id + update_cols)\n",
    "        merge_cols = ['video_id'] + [col for col in update_cols if col in updates_df.columns]\n",
    "        updates_to_merge = updates_df[merge_cols].drop_duplicates(subset=['video_id'], keep='last')\n",
    "\n",
    "        # Merge, prioritizing updates\n",
    "        # Suffixes help identify original vs update cols if needed, but update will overwrite\n",
    "        merged_df = pd.merge(meta_df, updates_to_merge, on='video_id', how='left', suffixes=('', '_update'))\n",
    "\n",
    "        # Apply the updates\n",
    "        for col in update_cols:\n",
    "            update_col_name = col + '_update'\n",
    "            if update_col_name in merged_df.columns:\n",
    "                # Fill NAs in original col with update col, then drop update col\n",
    "                meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n",
    "                # Alternative: Directly update where update is not NA\n",
    "                # meta_df[col] = np.where(merged_df[update_col_name].notna(), merged_df[update_col_name], merged_df[col])\n",
    "\n",
    "        meta_df.to_csv(metadata_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Metadata file '{metadata_path}' updated with {len(video_updates)} video records.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating metadata {metadata_path}: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def load_metadata_for_inference(metadata_file: str = METADATA_FILE) -> Dict[str, List[Dict]]:\n",
    "    if not Path(metadata_file).is_file(): return {}\n",
    "    video_questions = defaultdict(list)\n",
    "    required_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "    try:\n",
    "        df = pd.read_csv(metadata_file, dtype=str).fillna('')\n",
    "        if 'video_id' not in df.columns or required_col not in df.columns:\n",
    "            logger.error(f\"Metadata missing 'video_id' or '{required_col}'.\")\n",
    "            return {}\n",
    "        valid_df = df[df['video_id'].astype(bool) & df[required_col].astype(bool)]\n",
    "        if len(valid_df) == 0:\n",
    "             logger.warning(f\"No videos found with '{required_col}' in {metadata_file}. Check Step 4.\")\n",
    "             return {}\n",
    "        for video_id, group in valid_df.groupby('video_id'):\n",
    "             video_questions[video_id] = group.to_dict('records')\n",
    "        logger.info(f\"Loaded {len(video_questions)} videos ({len(valid_df)} questions) with valid IDs for inference.\")\n",
    "        return dict(video_questions)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading metadata for inference: {e}\", exc_info=True)\n",
    "        return {}\n",
    "\n",
    "# --- Upload/Verification Helpers ---\n",
    "def upload_to_gcs(storage_client, bucket_name: str, source_file_path: Path, destination_blob_name: str) -> Optional[str]:\n",
    "    if not GCS_AVAILABLE or storage_client is None or not source_file_path.is_file(): return None\n",
    "    try:\n",
    "        blob = storage_client.bucket(bucket_name).blob(destination_blob_name)\n",
    "        blob.upload_from_filename(str(source_file_path))\n",
    "        gcs_uri = f\"gs://{bucket_name}/{destination_blob_name}\"\n",
    "        logger.debug(f\"GCS OK: {source_file_path} -> {gcs_uri}\")\n",
    "        return gcs_uri\n",
    "    except Exception as e:\n",
    "        logger.error(f\"GCS Fail: {source_file_path}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def upload_via_file_api(storage_client, local_path: Path, display_name: str) -> Optional[str]:\n",
    "    if storage_client is None or not local_path.is_file(): return None\n",
    "    try:\n",
    "        logger.debug(f\"Uploading {local_path} via File API...\")\n",
    "        uploaded_file = storage_client.upload(file=local_path)\n",
    "        logger.info(f\"File API OK: {local_path} -> {uploaded_file.name}\")\n",
    "        return uploaded_file.name\n",
    "    except Exception as e:\n",
    "        logger.error(f\"File API Fail: {local_path}. Error: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def verify_gcs_file_exists(storage_client, gcs_uri: str) -> bool:\n",
    "    if not GCS_AVAILABLE or storage_client is None or not gcs_uri: return False\n",
    "    try:\n",
    "        exists = storage.Blob.from_string(gcs_uri, client=storage_client).exists()\n",
    "        if not exists: logger.warning(f\"GCS verify failed: {gcs_uri}\")\n",
    "        return exists\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error verifying GCS {gcs_uri}: {e}\")\n",
    "        return False\n",
    "\n",
    "def verify_file_api_resource_exists(storage_client, file_api_name: str) -> bool:\n",
    "    if not storage_client or not file_api_name: return False\n",
    "    try:\n",
    "        _ = storage_client.get(name=file_api_name) # Sync get for verification\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error verifying File API {file_api_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def verify_local_file_exists(local_path: str) -> bool:\n",
    "    exists = Path(local_path).is_file() if local_path else False\n",
    "    if not exists: logger.warning(f\"Local verify failed: {local_path}\")\n",
    "    return exists\n",
    "\n",
    "# --- Prompt Building ---\n",
    "def build_prompt(question_info: dict, tag = 'Correctly-Led Questions') -> str:\n",
    "    question = question_info.get(\"question\", \"\")\n",
    "    q_type = question_info.get(\"question_type\", \"default\")\n",
    "    template = PROMPT_TEMPLATES.get(q_type, PROMPT_TEMPLATES[\"default\"])\n",
    "    # if q_type is MCQ\n",
    "    if q_type == \"Multiple-choice Question with a Single Correct Answer\":\n",
    "        return template.format(question=question).strip() + \"\\n\" + \"E. None of the above\"\n",
    "    return template.format(question=question).strip() + \"\\n\" + question_info.get(\"question_prompt\").strip()\n",
    "\n",
    "# --- Rate Limiter ---\n",
    "class AsyncRateLimiter:\n",
    "    \"\"\"\n",
    "    An asyncio-compatible token bucket rate limiter.\n",
    "\n",
    "    Args:\n",
    "        rate (int): The maximum number of requests allowed per period.\n",
    "        period (float): The time period in seconds (default: 60 for RPM).\n",
    "        capacity (int, optional): The maximum burst capacity. Defaults to `rate`.\n",
    "    \"\"\"\n",
    "    def __init__(self, rate: int, period: float = 60.0, capacity: Optional[int] = None):\n",
    "        if rate <= 0:\n",
    "            raise ValueError(\"Rate must be positive\")\n",
    "        if period <= 0:\n",
    "            raise ValueError(\"Period must be positive\")\n",
    "\n",
    "        self.rate = rate\n",
    "        self.period = float(period)\n",
    "        self.capacity = float(capacity if capacity is not None else rate)\n",
    "        self._tokens = self.capacity # Start full\n",
    "        self._last_refill_time = time.monotonic()\n",
    "        self._lock = asyncio.Lock()\n",
    "\n",
    "    def _get_tokens_per_second(self) -> float:\n",
    "        return self.rate / self.period\n",
    "\n",
    "    async def _refill(self):\n",
    "        \"\"\"Replenishes tokens based on elapsed time. Must be called under lock.\"\"\"\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self._last_refill_time\n",
    "        if elapsed > 0:\n",
    "            tokens_to_add = elapsed * self._get_tokens_per_second()\n",
    "            self._tokens = min(self.capacity, self._tokens + tokens_to_add)\n",
    "            self._last_refill_time = now\n",
    "\n",
    "    async def acquire(self):\n",
    "        \"\"\"\n",
    "        Acquires a token, waiting if necessary.\n",
    "        \"\"\"\n",
    "        async with self._lock:\n",
    "            await self._refill() # Refill based on time since last acquire/refill\n",
    "\n",
    "            while self._tokens < 1:\n",
    "                # Calculate how long to wait for 1 token\n",
    "                tokens_needed = 1.0 - self._tokens\n",
    "                wait_time = tokens_needed / self._get_tokens_per_second()\n",
    "\n",
    "                # Release the lock before sleeping\n",
    "                lock_released = True\n",
    "                try:\n",
    "                    self._lock.release()\n",
    "                    logger.debug(f\"Rate limit hit. Waiting for {wait_time:.3f}s for next token.\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                finally:\n",
    "                    # Re-acquire the lock if it was released\n",
    "                    if lock_released:\n",
    "                        await self._lock.acquire()\n",
    "\n",
    "                # Refill again after waiting, as more time has passed\n",
    "                await self._refill()\n",
    "\n",
    "            # Consume a token\n",
    "            self._tokens -= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363fbd1",
   "metadata": {},
   "source": [
    "# Download, Extract & Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d30a83",
   "metadata": {},
   "source": [
    "## Fetch Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(DATASET_CSV)\n",
    "\n",
    "if dataset_path.is_file() and SKIP_FETCH:\n",
    "    logger.info(f\"Dataset file '{DATASET_CSV}' exists and SKIP_FETCH is True. Skipping.\")\n",
    "    display(Markdown(f\"✅ Skipping fetch: Found existing `{DATASET_CSV}`.\"))\n",
    "    # Load the existing dataframe for use in Step 2\n",
    "    try:\n",
    "        dataset_df = pd.read_csv(dataset_path, dtype=str) # Load all as string initially\n",
    "        logger.info(f\"Loaded existing dataset from {DATASET_CSV} ({len(dataset_df)} rows).\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load existing dataset file {DATASET_CSV}: {e}\")\n",
    "        display(Markdown(f\"❌ Error loading existing `{DATASET_CSV}`: {e}. Please delete the file or set SKIP_FETCH=False.\"))\n",
    "        raise\n",
    "else:\n",
    "    logger.info(f\"Fetching dataset '{HF_DATASET_NAME}' (split: '{HF_DATASET_SPLIT}') from HuggingFace...\")\n",
    "    try:\n",
    "        dataset = load_dataset(HF_DATASET_NAME, split=HF_DATASET_SPLIT, cache_dir=HF_CACHE_DIR)\n",
    "        dataset_df = dataset.to_pandas()\n",
    "        # Ensure key columns are strings\n",
    "        for col in ['qid', 'video_id', 'question', 'question_type']:\n",
    "             if col in dataset_df.columns:\n",
    "                 dataset_df[col] = dataset_df[col].astype(str)\n",
    "        dataset_df.to_csv(dataset_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Successfully fetched dataset and saved to {DATASET_CSV} ({len(dataset_df)} rows).\")\n",
    "        display(Markdown(f\"✅ Dataset fetched and saved to `{DATASET_CSV}` ({len(dataset_df)} rows).\"))\n",
    "        display(dataset_df.head())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch or save dataset: {e}\", exc_info=True)\n",
    "        display(Markdown(f\"❌ **Error fetching dataset:** {e}. Check connection, dataset name/split, cache dir permissions.\"))\n",
    "        raise RuntimeError(\"Dataset fetching failed. Cannot continue.\")\n",
    "\n",
    "# Ensure dataset_df is loaded if skipping fetch didn't load it (e.g., first run with skip=True and no file)\n",
    "if 'dataset_df' not in locals():\n",
    "    if dataset_path.is_file():\n",
    "        try:\n",
    "            dataset_df = pd.read_csv(dataset_path, dtype=str)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Critical error: Could not load dataset from {DATASET_CSV} after attempting fetch/skip: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise RuntimeError(f\"Critical error: Dataset DataFrame not loaded and file {DATASET_CSV} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3452ecb",
   "metadata": {},
   "source": [
    "## Download, Extract, and Prepare Videos\n",
    "\n",
    "Downloads, extracts, and uploads videos (to GCS or File API). Updates `video_metadata.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9141c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(DATASET_CSV)\n",
    "dataset_df = pd.read_csv(dataset_path, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6d2dd",
   "metadata": {},
   "source": [
    "### Download Video Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1211df",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Downloading Archive\"))\n",
    "# Remember to set SKIP_DOWNLOAD_ZIP to True if you want to skip the download and you already have the zip file\n",
    "if zip_file_path.is_file() and SKIP_DOWNLOAD_ZIP:\n",
    "    display(Markdown(f\"✅ Skipping download: Found `{zip_file_path}`.\"))\n",
    "else:\n",
    "    try: download_file_with_progress(VIDEO_ZIP_URL, zip_file_path); display(Markdown(f\"✅ Downloaded: `{zip_file_path}`.\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **Download Error:** {e}.\"))\n",
    "        if not SKIP_EXTRACT or not SKIP_PREPARE: raise RuntimeError(f\"Download failed.\")\n",
    "        else: display(Markdown(\"⚠️ Download failed, skipping steps.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aca42e",
   "metadata": {},
   "source": [
    "### Extract Video Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Extracting Archive\"))\n",
    "# Check if the zip file exists and if we should skip extraction\n",
    "if any(extracted_videos_path.glob('*.mp4')) and SKIP_EXTRACT:\n",
    "    display(Markdown(f\"✅ Skipping extraction: Files in `{extracted_videos_path}`.\"))\n",
    "elif not zip_file_path.is_file():\n",
    "    display(Markdown(f\"❌ Cannot extract: `{zip_file_path}` missing.\"))\n",
    "    if not SKIP_PREPARE: raise RuntimeError(f\"Zip missing.\")\n",
    "    else: display(Markdown(\"⚠️ Extraction skipped (no zip).\"))\n",
    "else:\n",
    "    try: \n",
    "        extract_zip(zip_file_path, extracted_videos_path)\n",
    "        # Move all videos to main directory\n",
    "        move_videos_to_main_directory(extracted_videos_path)\n",
    "        display(Markdown(f\"✅ Extracted to `{extracted_videos_path}` and moved all videos to main directory.\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **Extraction Error:** {e}.\"))\n",
    "        if not SKIP_PREPARE: raise RuntimeError(\"Extraction failed.\")\n",
    "        else: display(Markdown(\"⚠️ Extraction failed, skipping prep.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff29045",
   "metadata": {},
   "source": [
    "### Slow/Speed Up Videos\n",
    "Losslessly change video speed while also re-encoding audio to maintain pitch. As\n",
    "a result, is super fast. Could be made faster if using asyncio to concurrently run\n",
    "ffmpeg. The video results are saved into **speed_videos/0.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_subprocess(cmd, check=True, capture_output=False):\n",
    "    \"\"\"Helper function to run subprocess asynchronously.\"\"\"\n",
    "    stdout_pipe = asyncio.subprocess.PIPE if capture_output else asyncio.subprocess.DEVNULL\n",
    "    # Capture stderr only if check is True or capture_output is True, otherwise DEVNULL\n",
    "    stderr_pipe = asyncio.subprocess.PIPE if check or capture_output else asyncio.subprocess.DEVNULL\n",
    "\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        *cmd,\n",
    "        stdout=stdout_pipe,\n",
    "        stderr=stderr_pipe\n",
    "    )\n",
    "    stdout, stderr = await process.communicate()\n",
    "\n",
    "    if check and process.returncode != 0:\n",
    "        error_msg = f\"Command '{' '.join(cmd)}' failed with return code {process.returncode}\"\n",
    "        stderr_decoded = stderr.decode(errors='ignore') if stderr else \"\"\n",
    "        if stderr_decoded:\n",
    "            error_msg += f\"\\nStderr: {stderr_decoded}\"\n",
    "        # Raise specific exception to potentially capture stderr later\n",
    "        raise subprocess.CalledProcessError(process.returncode, cmd, output=stdout, stderr=stderr)\n",
    "\n",
    "    return stdout, stderr, process.returncode\n",
    "\n",
    "async def process_single_video(vid_path, speed_videos_path, VIDEO_SPEED_FACTOR, semaphore):\n",
    "    \"\"\"Asynchronously processes a single video. Returns status string.\"\"\"\n",
    "    vid_path_str = str(vid_path.resolve())\n",
    "    out_path = speed_videos_path / vid_path.name\n",
    "    out_path_str = str(out_path.resolve())\n",
    "\n",
    "    async with semaphore: # Limit concurrency\n",
    "        if out_path.is_file():\n",
    "            return 'skipped'\n",
    "\n",
    "        if VIDEO_SPEED_FACTOR == 1.0:\n",
    "            try:\n",
    "                # Use asyncio.to_thread for potentially blocking I/O\n",
    "                await asyncio.to_thread(shutil.copy, vid_path_str, out_path_str)\n",
    "                return 'processed'\n",
    "            except Exception as e:\n",
    "                try:\n",
    "                    logger.error(f\"Error copying {vid_path.name}: {e}\")\n",
    "                except NameError:\n",
    "                    print(f\"Error copying {vid_path.name}: {e}\")\n",
    "                return 'error'\n",
    "\n",
    "        # --- Process video with speed change ---\n",
    "        tf_bitstream_path = None\n",
    "        tf_audio_path = None\n",
    "        tf_final_path = None\n",
    "        try:\n",
    "            # Create temporary files (synchronous part is okay here)\n",
    "            # Context manager ensures files are closed before ffmpeg uses them\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".h264\") as tf_b, \\\n",
    "                 tempfile.NamedTemporaryFile(delete=False, suffix=\".aac\") as tf_a, \\\n",
    "                 tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as tf_f:\n",
    "                tf_bitstream_name = tf_b.name\n",
    "                tf_audio_name = tf_a.name\n",
    "                tf_final_name = tf_f.name\n",
    "            # Store paths for cleanup\n",
    "            tf_bitstream_path = Path(tf_bitstream_name)\n",
    "            tf_audio_path = Path(tf_audio_name)\n",
    "            tf_final_path = Path(tf_final_name)\n",
    "\n",
    "\n",
    "            # Get original FPS\n",
    "            ffprobe_cmd = [\n",
    "                \"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"v\", \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n",
    "                \"-show_entries\", \"stream=r_frame_rate\", vid_path_str\n",
    "            ]\n",
    "            stdout, _, _ = await run_subprocess(ffprobe_cmd, check=True, capture_output=True)\n",
    "            fps = float(fractions.Fraction(stdout.decode().strip()))\n",
    "            new_fps = fps * VIDEO_SPEED_FACTOR\n",
    "\n",
    "            # Extract and speed up audio\n",
    "            factor = VIDEO_SPEED_FACTOR\n",
    "            filter_parts = []\n",
    "            while factor > 2.0:\n",
    "                filter_parts.append(\"atempo=2.0\")\n",
    "                factor /= 2.0\n",
    "            while factor < 0.5:\n",
    "                filter_parts.append(\"atempo=0.5\")\n",
    "                factor /= 0.5\n",
    "            if abs(factor - 1.0) > 1e-6:\n",
    "                 filter_parts.append(f\"atempo={factor:.6f}\")\n",
    "\n",
    "            if not filter_parts:\n",
    "                 audio_cmd = [\"ffmpeg\", \"-y\", \"-i\", vid_path_str, \"-vn\", \"-c:a\", \"copy\", tf_audio_name]\n",
    "            else:\n",
    "                audio_filter = \",\".join(filter_parts)\n",
    "                audio_cmd = [\"ffmpeg\", \"-y\", \"-i\", vid_path_str, \"-vn\", \"-filter:a\", audio_filter, \"-c:a\", \"aac\", \"-b:a\", \"128k\", tf_audio_name]\n",
    "            await run_subprocess(audio_cmd, check=True)\n",
    "\n",
    "\n",
    "            # Extract h264 bitstream\n",
    "            extract_cmd = [\"ffmpeg\", \"-y\", \"-i\", vid_path_str, \"-map\", \"0:v\", \"-c:v\", \"copy\", \"-bsf:v\", \"h264_mp4toannexb\", tf_bitstream_name]\n",
    "            await run_subprocess(extract_cmd, check=True)\n",
    "\n",
    "            # Remux bitstream with new audio and FPS\n",
    "            remux_cmd = [\"ffmpeg\", \"-y\", \"-fflags\", \"+genpts\", \"-r\", f\"{new_fps:.6f}\", \"-i\", tf_bitstream_name, \"-i\", tf_audio_name, \"-map\", \"0:v\", \"-map\", \"1:a\", \"-c:v\", \"copy\", \"-c:a\", \"copy\", tf_final_name]\n",
    "            await run_subprocess(remux_cmd, check=True)\n",
    "\n",
    "            # Move final file (use asyncio.to_thread)\n",
    "            await asyncio.to_thread(shutil.move, tf_final_name, out_path_str)\n",
    "            return 'processed'\n",
    "\n",
    "        except Exception as e:\n",
    "            err_msg = f\"Error processing {vid_path.name}: {e}\"\n",
    "            # Include ffmpeg stderr if available\n",
    "            if isinstance(e, subprocess.CalledProcessError) and e.stderr:\n",
    "                 err_msg += f\"\\nFFmpeg/FFprobe Stderr:\\n{e.stderr.decode(errors='ignore')}\"\n",
    "            try:\n",
    "                logger.error(err_msg)\n",
    "            except NameError:\n",
    "                print(err_msg)\n",
    "            return 'error'\n",
    "        finally:\n",
    "            # Clean up temporary files asynchronously using to_thread\n",
    "            async def _cleanup():\n",
    "                if tf_bitstream_path and tf_bitstream_path.exists():\n",
    "                    tf_bitstream_path.unlink(missing_ok=True)\n",
    "                if tf_audio_path and tf_audio_path.exists():\n",
    "                    tf_audio_path.unlink(missing_ok=True)\n",
    "                # tf_final is moved, only delete if error occurred before move\n",
    "                if tf_final_path and tf_final_path.exists():\n",
    "                    tf_final_path.unlink(missing_ok=True)\n",
    "            # Run sync cleanup in thread only if paths were assigned\n",
    "            if tf_bitstream_path or tf_audio_path or tf_final_path:\n",
    "                 await asyncio.to_thread(_cleanup)\n",
    "\n",
    "\n",
    "# --- Main Cell Logic ---\n",
    "\n",
    "async def run_processing(): # Wrap in an async function to use await\n",
    "    display(Markdown(\"### Preparing Videos\"))\n",
    "    if dataset_df is None:\n",
    "        raise RuntimeError(\"Dataset DF unavailable.\")\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    speed_videos_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    all_video_ids = sorted(list(dataset_df['video_id'].dropna().unique()))\n",
    "    # Use logging if available, otherwise print\n",
    "    try:\n",
    "        logger.info(f\"Processing {len(all_video_ids)} unique video IDs.\")\n",
    "    except NameError:\n",
    "        print(f\"Processing {len(all_video_ids)} unique video IDs.\")\n",
    "\n",
    "    vid_paths = list(extracted_videos_path.glob(\"*.mp4\"))\n",
    "\n",
    "    # Limit concurrency\n",
    "    concurrency_limit = MAX_ASYNC_WORKERS\n",
    "    try:\n",
    "        logger.info(f\"Using concurrency limit: {concurrency_limit}\")\n",
    "    except NameError:\n",
    "        print(f\"Using concurrency limit: {concurrency_limit}\")\n",
    "    semaphore = asyncio.Semaphore(concurrency_limit)\n",
    "\n",
    "    tasks = []\n",
    "    # Keep the familiar loop structure for creating tasks\n",
    "    print(f\"Preparing tasks for {len(vid_paths)} videos...\")\n",
    "    for vid_path in vid_paths:\n",
    "         # Create a task for each video processing job\n",
    "         # Pass necessary arguments to the task creator\n",
    "         task = asyncio.create_task(process_single_video(vid_path, speed_videos_path, VIDEO_SPEED_FACTOR, semaphore))\n",
    "         tasks.append(task)\n",
    "\n",
    "    # Now, run all the created tasks concurrently and display progress\n",
    "    # Use asyncio.as_completed with a standard tqdm progress bar\n",
    "    print(f\"Transforming {len(tasks)} Videos...\")\n",
    "    results = []\n",
    "    # Use the imported tqdm (now tqdm.auto) to create a standard progress bar instance\n",
    "    with tqdm(total=len(tasks), desc=\"Transforming Videos\", unit=\"video\") as pbar:\n",
    "        for future in asyncio.as_completed(tasks):\n",
    "            try:\n",
    "                result = await future # Get result from completed task\n",
    "                results.append(result)\n",
    "            except Exception as exc:\n",
    "                # Log errors from tasks that failed internally if not caught by process_single_video\n",
    "                # (process_single_video should ideally return 'error' status instead of raising)\n",
    "                try:\n",
    "                    logger.error(f\"Task for a video failed: {exc}\")\n",
    "                except NameError:\n",
    "                    print(f\"Task for a video failed: {exc}\")\n",
    "                results.append('error') # Count as error if task itself fails unexpectedly\n",
    "            finally:\n",
    "                 pbar.update(1) # Increment progress bar regardless of outcome\n",
    "\n",
    "\n",
    "    # Count results\n",
    "    processed = results.count('processed')\n",
    "    skipped = results.count('skipped')\n",
    "    errors = results.count('error')\n",
    "\n",
    "    print(f\"\\n\\n{skipped} videos skipped, {processed} videos processed, {errors} errors, {len(vid_paths)} total.\")\n",
    "\n",
    "# --- Execute the async processing ---\n",
    "# In a Jupyter Notebook, you usually need to await the top-level async function.\n",
    "# If top-level await isn't enabled, you might need nest_asyncio or run manually.\n",
    "# Using await directly is the most common way in modern notebooks.\n",
    "\n",
    "await run_processing()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a606806",
   "metadata": {},
   "source": [
    "### Preparing and Upload Videos to GCS or File API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc5cb1",
   "metadata": {},
   "source": [
    "Upload all videos into File API, and record the link to that video inside video_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare Videos (Upload GCS/File API) & Update Metadata --- #\n",
    "display(Markdown(\"### Preparing Videos & Updating Metadata\"))\n",
    "if SKIP_PREPARE:\n",
    "    display(Markdown(\"✅ Skipping video preparation.\"))\n",
    "elif storage_client is None:\n",
    "     display(Markdown(\"❌ Cannot prepare: Client not ready.\")); raise RuntimeError(\"Client missing.\")\n",
    "else:\n",
    "    if dataset_df is None: raise RuntimeError(\"Dataset DF unavailable.\")\n",
    "    all_video_ids = sorted(list(dataset_df['video_id'].dropna().unique()))\n",
    "    logger.info(f\"Processing {len(all_video_ids)} unique video IDs.\")\n",
    "\n",
    "    videos_to_process_ids = all_video_ids\n",
    "    if MAX_VIDEOS_TO_PROCESS is not None:\n",
    "        videos_to_process_ids = all_video_ids[:MAX_VIDEOS_TO_PROCESS]\n",
    "        logger.info(f\"Limiting to {len(videos_to_process_ids)} videos.\")\n",
    "\n",
    "    # Load existing metadata to check status\n",
    "    existing_statuses = {}\n",
    "    resource_ids = {}\n",
    "    required_id_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "    if Path(METADATA_FILE).is_file():\n",
    "        try:\n",
    "            existing_df = pd.read_csv(METADATA_FILE, dtype=str)\n",
    "            if 'video_id' in existing_df.columns and 'status' in existing_df.columns:\n",
    "                existing_statuses = pd.Series(existing_df.status.values, index=existing_df.video_id).to_dict()\n",
    "            if 'video_id' in existing_df.columns and required_id_col in existing_df.columns:\n",
    "                resource_ids = pd.Series(existing_df[required_id_col].values, index=existing_df.video_id).dropna().to_dict()\n",
    "            logger.info(\"Checked existing metadata statuses/IDs.\")\n",
    "        except Exception as e: logger.warning(f\"Could not load existing metadata: {e}\")\n",
    "\n",
    "    video_metadata_updates = {}\n",
    "    processed_count, upload_failures, missing_local, skipped_count = 0, 0, 0, 0\n",
    "    num_batches = math.ceil(len(videos_to_process_ids) / UPLOAD_BATCH_SIZE_GCS)\n",
    "    prep_mode = \"GCS Upload\" if USE_VERTEX else \"File API Upload\"\n",
    "\n",
    "    with tqdm(total=len(videos_to_process_ids), desc=f\"Preparing ({prep_mode})\") as pbar:\n",
    "        for i in range(0, len(videos_to_process_ids), UPLOAD_BATCH_SIZE_GCS):\n",
    "            batch_ids = videos_to_process_ids[i : i + UPLOAD_BATCH_SIZE_GCS]\n",
    "            batch_num = (i // UPLOAD_BATCH_SIZE_GCS) + 1\n",
    "            logger.info(f\"Prep Batch {batch_num}/{num_batches}...\")\n",
    "            current_batch_updates = {}\n",
    "\n",
    "            for video_id in batch_ids:\n",
    "                pbar.set_postfix_str(f\"ID: {video_id}\")\n",
    "                update_data = {\"local_path\": None, \"gcs_uri\": None, \"file_api_name\": None, \"status\": \"error_unknown\"}\n",
    "                local_video_path = speed_videos_path / f\"{video_id}.mp4\"\n",
    "                current_status = existing_statuses.get(video_id, 'pending')\n",
    "                existing_resource_id = resource_ids.get(video_id)\n",
    "                is_already_processed = False\n",
    "\n",
    "                # Check if already uploaded and verified\n",
    "                if current_status in ['uploaded_gcs', 'uploaded_file_api'] and existing_resource_id:\n",
    "                    verified = False\n",
    "                    if USE_VERTEX: verified = verify_gcs_file_exists(storage_client, existing_resource_id)\n",
    "                    else: verified = verify_file_api_resource_exists(storage_client, existing_resource_id)\n",
    "                    if verified:\n",
    "                        logger.debug(f\"Skipping verified video {video_id} ('{current_status}').\")\n",
    "                        is_already_processed = True\n",
    "                        skipped_count += 1\n",
    "                        update_data.update({ # Ensure metadata is consistent\n",
    "                            'local_path': str(local_video_path) if local_video_path.is_file() else None,\n",
    "                            'status': current_status,\n",
    "                            required_id_col: existing_resource_id\n",
    "                        })\n",
    "                    else:\n",
    "                        logger.warning(f\"Video {video_id} ({current_status}) needs re-processing (verification failed).\")\n",
    "                elif current_status != 'pending':\n",
    "                    logger.debug(f\"Video {video_id} has non-pending status '{current_status}' but no verified resource ID. Re-processing.\")\n",
    "\n",
    "                if is_already_processed:\n",
    "                    processed_count += 1\n",
    "                    current_batch_updates[video_id] = update_data\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Process if needed\n",
    "                if local_video_path.is_file():\n",
    "                    update_data[\"local_path\"] = str(local_video_path)\n",
    "                    resource_id_result = None\n",
    "                    if USE_VERTEX:\n",
    "                        blob_name = f\"videos/{video_id}.mp4\"\n",
    "                        resource_id_result = upload_to_gcs(storage_client, GCS_BUCKET, local_video_path, blob_name)\n",
    "                        if resource_id_result: update_data.update({\"gcs_uri\": resource_id_result, \"status\": \"uploaded_gcs\"})\n",
    "                        else: update_data[\"status\"] = \"gcs_upload_failed\"; upload_failures += 1\n",
    "                    else: # Gemini API\n",
    "                        resource_id_result = upload_via_file_api(storage_client, local_video_path, f\"vid_{video_id}\")\n",
    "                        if resource_id_result: update_data.update({\"file_api_name\": resource_id_result, \"status\": \"uploaded_file_api\"})\n",
    "                        else: update_data[\"status\"] = \"file_api_upload_failed\"; upload_failures += 1\n",
    "                else:\n",
    "                    logger.warning(f\"Local file missing: {local_video_path}\")\n",
    "                    missing_local += 1\n",
    "                    update_data[\"status\"] = \"local_missing\"\n",
    "\n",
    "                current_batch_updates[video_id] = update_data\n",
    "                processed_count += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Update metadata after batch\n",
    "            if current_batch_updates:\n",
    "                try: create_or_update_metadata(METADATA_FILE, dataset_df, current_batch_updates)\n",
    "                except Exception as e: logger.error(f\"Metadata update failed batch {batch_num}: {e}\")\n",
    "                video_metadata_updates.update(current_batch_updates)\n",
    "\n",
    "    logger.info(f\"Prep finished. Checked: {processed_count}, Skipped(verified): {skipped_count}, Missing Local: {missing_local}, Upload Failures: {upload_failures}\")\n",
    "    display(Markdown(f\"✅ Video preparation complete. See logs. Metadata: `{METADATA_FILE}`.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a920ea1",
   "metadata": {},
   "source": [
    "# Testing UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c768a3",
   "metadata": {},
   "source": [
    "## Single Prompt Single Question Testing UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee7061f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-28 03:27:41,255 - INFO - Loaded 289 videos (1500 questions) with valid IDs for inference.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Select Video & Question"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c33a028b3cc4f9eaeac86546506f413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Video ID:', options=(('Select video...', None), ('-HAFFvsDCr4 (5q)', '-HAFFvsDCr4'), ('-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536ec41cc31744108d17c8b18f457cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Question (QID):', disabled=True, layout=Layout(width='95%'), options=(('Select question.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac03538dcc64732a1ae7723eec55700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Run Inference', disabled=True, icon='play', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58da653742ed4eaeb91a96244d592f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def perform_inference_single_sync(question_info: Dict, client: Any) -> Dict[str, Any]:\n",
    "    qid = question_info.get(\"qid\", \"?\")\n",
    "    prompt_text = build_prompt(question_info)\n",
    "    gcs_uri = question_info.get(\"gcs_uri\")\n",
    "    file_api_name = question_info.get(\"file_api_name\")\n",
    "    start_time = time.time()\n",
    "    result = None\n",
    "\n",
    "    try:\n",
    "        video_part = None\n",
    "        if USE_VERTEX:\n",
    "            if not gcs_uri: raise ValueError(\"Missing GCS URI.\")\n",
    "            video_part = types.Part.from_uri(mime_type=\"video/mp4\", file_uri=gcs_uri)\n",
    "        else:\n",
    "            if not file_api_name: raise ValueError(\"Missing File API name.\")\n",
    "            try:\n",
    "                # Fetch File object sync\n",
    "                file_object = client.files.get(name=file_api_name)\n",
    "                video_part = file_object\n",
    "            except genai_errors.NotFoundError: raise FileNotFoundError(f\"File API '{file_api_name}' not found.\")\n",
    "            except Exception as e: raise RuntimeError(f\"Failed get File API obj: {e}\")\n",
    "\n",
    "        question_content = types.Content(\n",
    "            role=\"user\", \n",
    "            parts=[types.Part.from_text(text=prompt_text)]\n",
    "        )\n",
    "        contents = [\n",
    "            question_content,\n",
    "            video_part\n",
    "        ]\n",
    "            \n",
    "\n",
    "    except (ValueError, FileNotFoundError, RuntimeError) as e:\n",
    "        logger.error(f\"QID {qid} (Sync): Input Error - {e}\")\n",
    "        return {\"qid\": qid, \"pred\": f\"ERROR: Input Fail - {e}\", \"duration\": 0, \"status\": \"Failed (Input)\"}\n",
    "\n",
    "    # Inference with Retries (Sync)\n",
    "    for attempt in range(MAX_RETRIES + 1):\n",
    "        try:\n",
    "            api_start = time.time()\n",
    "            # Use sync client.models\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL_NAME,\n",
    "                contents=contents,\n",
    "                config=CONFIG,\n",
    "            )\n",
    "            answer, reason, status, err_detail = \"ERROR\", \"UNKNOWN\", \"Success\", \"\"\n",
    "            try: # Process Response\n",
    "                answer = response\n",
    "                if response.candidates: reason = response.candidates[0].finish_reason.name\n",
    "            except ValueError as ve:\n",
    "                status, err_detail = \"Blocked/Empty\", f\"ValueError: {ve}. \"\n",
    "                answer = f\"ERROR: {status}. {err_detail}\"\n",
    "            result = {\"qid\": qid, \"pred\": answer, \"duration\": time.time()-start_time, \"finish_reason\": reason, \"status\": status}\n",
    "            return result\n",
    "        except (api_core_exceptions.ResourceExhausted) as e:\n",
    "             if attempt < MAX_RETRIES: time.sleep(INITIAL_BACKOFF_SECONDS * (2**attempt))\n",
    "             else: result = {\"qid\": qid, \"pred\": f\"ERROR: Max Retries ({type(e).__name__}) - {e}\", \"duration\": time.time()-start_time, \"status\": \"Failed (Retries)\"}; return result\n",
    "        except genai_errors.APIError as e:\n",
    "             result = {\"qid\": qid, \"pred\": f\"ERROR: GenAI APIError - {e}\", \"duration\": time.time()-start_time, \"status\": \"Failed (API Error)\"}; return result\n",
    "        except Exception as e:\n",
    "             result = {\"qid\": qid, \"pred\": f\"ERROR: Unexpected - {e}\", \"duration\": time.time()-start_time, \"status\": \"Failed (Unexpected)\"}; return result\n",
    "    # Fallback\n",
    "    result = {\"qid\": qid, \"pred\": \"ERROR: Unknown after retries\", \"duration\": time.time()-start_time, \"status\": \"Failed (Unknown)\"}\n",
    "    return result\n",
    "\n",
    "# --- UI Setup (using `google.genai` types where needed) --- #\n",
    "ui_video_questions = {}\n",
    "try: ui_video_questions = load_metadata_for_inference(METADATA_FILE)\n",
    "except Exception as e: display(Markdown(f\"❌ UI Load Error: {e}\"))\n",
    "\n",
    "# (Widgets setup remains the same)\n",
    "video_options = [(\"Select video...\", None)]\n",
    "if ui_video_questions: video_options.extend(sorted([(f\"{vid} ({len(qs)}q)\", vid) for vid, qs in ui_video_questions.items()]))\n",
    "video_selector = widgets.Dropdown(options=video_options, description='Video ID:', disabled=not ui_video_questions, style={'description_width': 'initial'})\n",
    "question_selector = widgets.Dropdown(options=[(\"Select question...\", None)], description='Question (QID):', disabled=True, layout=widgets.Layout(width='95%'), style={'description_width': 'initial'})\n",
    "run_button = widgets.Button(description='Run Inference', disabled=True, button_style='primary', icon='play')\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# --- Widget Interaction Logic (Remains mostly the same, calls updated sync function) --- #\n",
    "def on_video_selected(change):\n",
    "    # (Same logic)\n",
    "    selected_video_id = change['new']\n",
    "    question_selector.options = [(\"Select question...\", None)]\n",
    "    question_selector.value = None\n",
    "    question_selector.disabled = True\n",
    "    run_button.disabled = True\n",
    "    output_area.clear_output()\n",
    "    if selected_video_id and selected_video_id in ui_video_questions:\n",
    "        questions = ui_video_questions[selected_video_id]\n",
    "        question_options = sorted([(f\"{q.get('qid', 'N/A')}: {q.get('question', '')[:80]}...\", q) for q in questions if q.get('qid')])\n",
    "        if question_options:\n",
    "            question_selector.options = [(\"Select question...\", None)] + question_options\n",
    "            question_selector.disabled = False\n",
    "\n",
    "def on_question_selected(change):\n",
    "    # (Same logic)\n",
    "    selected_question_info = change['new']\n",
    "    run_button.disabled = selected_question_info is None\n",
    "    output_area.clear_output()\n",
    "    if selected_question_info:\n",
    "        with output_area:\n",
    "            display(Markdown(\"### Selected Info\"))\n",
    "            id_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "            display(pd.Series({\n",
    "                 'qid': selected_question_info.get('qid'),\n",
    "                 'question': selected_question_info.get('question'),\n",
    "                 f'Resource ({id_col})': selected_question_info.get(id_col)\n",
    "            }).to_frame('Value'))\n",
    "\n",
    "def on_run_button_clicked(b):\n",
    "    run_button.disabled = True\n",
    "    output_area.clear_output()\n",
    "    with output_area:\n",
    "        if not video_selector.value or not question_selector.value: display(Markdown(\"❌ Select video & question.\")); run_button.disabled = False; return\n",
    "        if ai_client is None: display(Markdown(\"❌ AI Client not ready.\")); run_button.disabled = False; return\n",
    "\n",
    "        q_info = question_selector.value\n",
    "        qid = q_info.get('qid')\n",
    "        resource_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "        resource_id = q_info.get(resource_col)\n",
    "\n",
    "        display(Markdown(f\"### Running QID: {qid}\"))\n",
    "        display(Markdown(f\"--- Verifying Resource --- ({'GCS' if USE_VERTEX else 'File API'}) ---\"))\n",
    "        verified = False\n",
    "        if not resource_id: display(Markdown(f\"❌ Error: Missing '{resource_col}' ID.\"))\n",
    "        elif USE_VERTEX: verified = verify_gcs_file_exists(storage_client, resource_id)\n",
    "        else: verified = verify_file_api_resource_exists(storage_client, resource_id)\n",
    "\n",
    "        if not verified: display(Markdown(\"❌ Verification failed.\")); run_button.disabled = False; return\n",
    "        display(Markdown(f\"✅ Resource Verified: {resource_id}\"))\n",
    "\n",
    "        display(Markdown(\"Video Preview:\"))\n",
    "        video_path = Path(speed_videos_path) / f\"{video_selector.value}.mp4\"\n",
    "        if video_path.is_file():\n",
    "            video_widget = widgets.Video.from_file(video_path, width=400, height=300)\n",
    "            display(video_widget)\n",
    "\n",
    "        display(Markdown(\"### Inference Details\"))\n",
    "        display(Markdown(f\"**Video ID:** {video_selector.value} | **QID:** {qid}\"))\n",
    "        display(Markdown(f\"**Resource ID ({resource_col}):** {resource_id}\"))\n",
    "\n",
    "        prompt = build_prompt(q_info)\n",
    "        display(Markdown(\"### Prompt\"))\n",
    "        display(Markdown(f\"**Question:** {q_info.get('question', '')}\"))\n",
    "        display(Markdown(f\"**Question Type:** {q_info.get('question_type', 'default')}\"))\n",
    "        display(Markdown(f\"**Prompt Template:** {PROMPT_TEMPLATES.get(q_info.get('question_type', 'default'), PROMPT_TEMPLATES['default'])}\"))\n",
    "        display(Markdown(f\"**System Prompt:** {SYSTEM_PROMPT}\"))\n",
    "        display(HTML(f\"<pre style='white-space: pre-wrap; border: 1px solid #000; padding: 10px;'>{prompt}</pre>\"))\n",
    "        display(Markdown(\"--- Performing Inference (Sync) ---\"))\n",
    "\n",
    "        # CALL THE CORRECTED SYNC FUNCTION\n",
    "        inference_result = perform_inference_single_sync(q_info, ai_client)\n",
    "\n",
    "        display(Markdown(\"--- Result ---\"))\n",
    "        if inference_result and isinstance(inference_result, dict):\n",
    "             # (Result display + Save button logic remains the same)\n",
    "            status, duration, answer, reason = (\n",
    "                inference_result.get(\"status\", \"?\"), inference_result.get('duration', -1),\n",
    "                inference_result.get('pred', ''), inference_result.get('finish_reason', 'N/A')\n",
    "            )\n",
    "            display(Markdown(f\"**Status:** {status} | **Duration:** {duration:.2f}s | **Reason:** {reason}\"))\n",
    "            display(Markdown(\"**Response:**\"))\n",
    "            display(HTML(f\"<pre style='white-space: pre-wrap; border: 1px solid #000; padding: 10px;'>{answer}</pre>\"))\n",
    "        else:\n",
    "            display(Markdown(\"❌ Invalid result.\"))\n",
    "\n",
    "    run_button.disabled = False\n",
    "\n",
    "# Register & Display\n",
    "video_selector.observe(on_video_selected, names='value')\n",
    "question_selector.observe(on_question_selected, names='value')\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "display(Markdown(\"### Select Video & Question\")); \n",
    "display(video_selector)\n",
    "display(question_selector)\n",
    "display(run_button)\n",
    "display(output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237afdae",
   "metadata": {},
   "source": [
    "## Generated Questions Prompt chaining Testing UI - turn by turn Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a53c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Generate Questions for Video\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def generate_questions_for_video(question_info: Dict, client: Any) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates a list of questions for a given video ID.\n",
    "    Returns a list of questions for the video.\n",
    "    \"\"\"\n",
    "    qid = question_info.get(\"qid\", \"?\")\n",
    "    video_id = question_info.get(\"video_id\", \"?\")\n",
    "    prompt_text = QUESTIONS_PROMPT\n",
    "    gcs_uri = question_info.get(\"gcs_uri\")\n",
    "    file_api_name = question_info.get(\"file_api_name\")\n",
    "    gcs_uri = question_info.get(\"gcs_uri\")\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        if USE_VERTEX:\n",
    "            if not gcs_uri: raise ValueError(\"Missing GCS URI.\")\n",
    "            video_part = types.Part.from_uri(mime_type=\"video/mp4\", file_uri=gcs_uri)\n",
    "        else:\n",
    "            if not file_api_name: raise ValueError(\"Missing File API name.\")\n",
    "            try:\n",
    "                # Fetch File object sync\n",
    "                file_object = client.files.get(name=file_api_name)\n",
    "                video_part = file_object\n",
    "            except genai_errors.NotFoundError: raise FileNotFoundError(f\"File API '{file_api_name}' not found.\")\n",
    "            except Exception as e: raise RuntimeError(f\"Failed get File API obj: {e}\")\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ Video resource error: {e}\")); return\n",
    "    \n",
    "    video_content = types.Content(role=\"user\", parts=[types.Part.from_text(text=prompt_text)])\n",
    "    \n",
    "    if video_part is None: raise RuntimeError(\"Video part preparation failed.\")\n",
    "    contents = [video_content, video_part]\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "                    model=QUESTIONS_MODEL_NAME,\n",
    "                    contents=contents,\n",
    "                    config=QUESTIONS_CONFIG\n",
    "                )\n",
    "        try:\n",
    "            answer = response.parsed\n",
    "            # Create a dictionary to hold the questions with their types\n",
    "            answer_dict = {}\n",
    "            if isinstance(answer, list):\n",
    "                for item in answer:\n",
    "                    q_text = item.questions\n",
    "                    q_type = item.type.value\n",
    "                    if q_text:  # Only add non-empty questions\n",
    "                        answer_dict[q_type] = q_text\n",
    "            else:\n",
    "                raise ValueError(\"Response format is not a list of questions.\")\n",
    "                        \n",
    "            \n",
    "            if response.candidates and hasattr(response.candidates[0], 'finish_reason') and response.candidates[0].finish_reason is not None:\n",
    "                reason = response.candidates[0].finish_reason.name\n",
    "        except ValueError as ve:\n",
    "            status, err_detail = \"Blocked/Empty\", f\"ValueError: {ve}. \"\n",
    "            # ... (block/safety reason extraction) ...\n",
    "            answer = f\"ERROR: {status}. {err_detail}\"\n",
    "            reason = \"Blocked/Empty\"\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating questions for video {video_id} (QID: {qid}): {e}\")\n",
    "        answer = f\"ERROR: {e}\"\n",
    "        reason = \"Error\"\n",
    "    return answer_dict if isinstance(answer_dict, dict) else {\"default\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7259ed71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-28 03:31:56,574 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-05-28 03:32:04,647 - INFO - AFC remote call 1 is done.\n"
     ]
    }
   ],
   "source": [
    "# test = generate_questions_for_video(ui_video_questions['-HAFFvsDCr4'][0], ai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b338e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Request renderer – handles list + File objects safely\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def _render_request(contents: list, turn_idx: int):\n",
    "    \"\"\"\n",
    "    Pretty‑print the payload sent to the model.\n",
    "\n",
    "    Handles:\n",
    "    • `types.Part` (binary video or textual description)\n",
    "    • `types.Content` (normal chat messages)\n",
    "    • plain File objects (Vertex File API)\n",
    "    • nested lists of Parts (video_part list in Vertex branch)\n",
    "    \"\"\"\n",
    "    # flatten in case the first element itself is a list of Parts\n",
    "    flat: list[Any] = []\n",
    "    for item in contents:\n",
    "        if isinstance(item, list):\n",
    "            flat.extend(item)\n",
    "        else:\n",
    "            flat.append(item)\n",
    "\n",
    "    lines = []\n",
    "    for c in flat:\n",
    "        if isinstance(c, types.Part):\n",
    "            # text Part vs. binary video Part\n",
    "            if getattr(c, \"text\", None):\n",
    "                lines.append(f\"video_desc: {c.text}\")\n",
    "            else:\n",
    "                lines.append(\"[video/mp4]\")\n",
    "        elif hasattr(c, \"parts\"):                           # types.Content\n",
    "            role = getattr(c, \"role\", \"?\")\n",
    "            txt = getattr(c.parts[0], \"text\", \"\")\n",
    "            lines.append(f\"{role}: {txt}\")\n",
    "        else:                                               # plain File\n",
    "            lines.append(\"[file]\")\n",
    "\n",
    "    joined_lines = '\\n'.join(lines)\n",
    "    display(HTML(\n",
    "        f\"<div style='border:1px dashed #999;padding:8px;margin:8px 0;'>\"\n",
    "        f\"<strong>Turn {turn_idx} – Request sent to model:</strong>\"\n",
    "        f\"<pre style='white-space:pre-wrap;margin:4px 0 0;'>{joined_lines}</pre>\"\n",
    "        f\"</div>\"\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc70a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-27 20:37:44,496 - INFO - Loaded 289 videos (1500 questions) with valid IDs for inference.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Select Video & Question"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4cf385b12f4580bd35c5a0d996d5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Video ID:', options=(('Select video...', None), ('-HAFFvsDCr4 (5q)', '-HAFFvsDCr4'), ('-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f01c1753904a37a458e4843df056ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Question (QID):', disabled=True, layout=Layout(width='95%'), options=(('Select question.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfcbe37294b04a85b26eb7aed8c9dffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Run Inference', disabled=True, icon='play', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228f465346a94502b1924e5ff4856fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Perform Inference (Sync) – handles video questions\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def perform_inference_single_sync(question_info: Dict, client: Any) -> Dict[str, Any]:\n",
    "    qid = question_info.get(\"qid\", \"?\")\n",
    "    video_id = question_info.get(\"video_id\", \"?\")\n",
    "    prompt_text = build_prompt(question_info)\n",
    "    gcs_uri = question_info.get(\"gcs_uri\")\n",
    "    file_api_name = question_info.get(\"file_api_name\")\n",
    "    start_time = time.time()\n",
    "    result = None\n",
    "    \n",
    "\n",
    "    questions_dict = generate_questions_for_video(question_info, client)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Display all the questions will be asking to the model\n",
    "    if not questions_dict:\n",
    "        display(Markdown(f\"❌ No questions found for video ID: {video_id}.\")); return\n",
    "    display(Markdown(f\"### Processed questions for Video ID: {video_id}\"))\n",
    "    \n",
    "    questions_list = []\n",
    "    for q_type, q_text in questions_dict.items():\n",
    "        for q in q_text:\n",
    "            display(HTML(\n",
    "                f\"<div style='border:1px dashed #999;padding:8px;margin:8px 0;'>\"\n",
    "                f\"<strong>Question Type: {q_type}</strong>\"\n",
    "                f\"<pre style='white-space:pre-wrap;margin:4px 0 0;'>{q_text[0]}</pre>\"\n",
    "                f\"</div>\"\n",
    "            ))\n",
    "            prompt_q = PROMPT_TEMPLATES.get(q_type, PROMPT_TEMPLATES['default']).format(question = q_text[0])\n",
    "            questions_list.append(prompt_q)\n",
    "\n",
    "\n",
    "    questions_list.append(prompt_text)\n",
    "    display(HTML(\n",
    "        f\"<div style='border:1px dashed #999;padding:8px;margin:8px 0;'>\"\n",
    "        f\"<strong>Main Question:</strong>\"\n",
    "        f\"<pre style='white-space:pre-wrap;margin:4px 0 0;'>{prompt_text}</pre>\"\n",
    "        f\"</div>\"\n",
    "    ))\n",
    "            \n",
    "    try:\n",
    "        if USE_VERTEX:\n",
    "            if not gcs_uri: raise ValueError(\"Missing GCS URI.\")\n",
    "            video_part = types.Part.from_uri(mime_type=\"video/mp4\", file_uri=gcs_uri)\n",
    "        else:\n",
    "            if not file_api_name: raise ValueError(\"Missing File API name.\")\n",
    "            try:\n",
    "                # Fetch File object sync\n",
    "                file_object = client.files.get(name=file_api_name)\n",
    "                video_part = file_object\n",
    "            except genai_errors.NotFoundError: raise FileNotFoundError(f\"File API '{file_api_name}' not found.\")\n",
    "            except Exception as e: raise RuntimeError(f\"Failed get File API obj: {e}\")\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ Video resource error: {e}\")); return\n",
    "\n",
    "    chat: list = []\n",
    "\n",
    "    for idx, q in enumerate(questions_list, 1):\n",
    "        user_msg = types.Content(role=\"user\",\n",
    "                                parts=[types.Part.from_text(text=q)])\n",
    "        # always include video_part\n",
    "        contents = [video_part] + chat + [user_msg]\n",
    "\n",
    "        _render_request(contents, idx)\n",
    "\n",
    "        try:\n",
    "            rsp = client.models.generate_content(model=MODEL_NAME,\n",
    "                                                contents=contents,\n",
    "                                                config=CONFIG)\n",
    "            answer = rsp.text.strip()\n",
    "            if answer:\n",
    "                summary_content = types.Content(\n",
    "                role=\"user\",\n",
    "                parts=[\n",
    "                types.Part.from_text(text=q),\n",
    "                types.Part.from_text(text=answer)\n",
    "                ])\n",
    "                summary_rsp = client.models.generate_content(model=QUESTION_MODEL_NAME,\n",
    "                                    contents=summary_content,\n",
    "                                    config=QUESTION_CONFIG)\n",
    "                summary_answer = summary_rsp.text.strip()\n",
    "                finish_reason = (rsp.candidates[0].finish_reason.name if rsp.candidates and rsp.candidates[0].finish_reason else \"UNKNOWN\")\n",
    "        except Exception as e:\n",
    "            answer, finish_reason = f\"ERROR: {e}\", \"Failed (API)\"\n",
    "\n",
    "        display(HTML(\n",
    "            f\"<div style='border:1px solid #000;margin:8px 0;padding:10px;'>\"\n",
    "            f\"<b>Q{idx}:</b> {q}</div>\"\n",
    "        ))\n",
    "        \n",
    "        display(HTML(\n",
    "            f\"<div style='border:1px solid #000;margin:0 0 12px;\"\n",
    "            f\"padding:10px;background:#f9f9f9;'><b>CoT:</b> {answer}</div>\"\n",
    "        ))\n",
    "        display(HTML(\n",
    "            f\"<div style='border:1px solid #000;margin:0 0 12px;\"\n",
    "            f\"padding:10px;background:#f9f9f9;'><b>Summary Answer:</b> {summary_answer}</div>\"\n",
    "        ))\n",
    "\n",
    "        chat.extend([\n",
    "            user_msg,\n",
    "            types.Content(role=\"model\", parts=[types.Part.from_text(text=answer or \"…\")])\n",
    "        ])\n",
    "    \n",
    "    final_answer = chat[-1].parts[0].text if chat else \"No answer\"\n",
    "    \n",
    "    result = {\"qid\": qid, \"pred\": final_answer, \"duration\": time.time()-start_time, \"status\": \"Successful\"}\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- UI Setup (using `google.genai` types where needed) --- #\n",
    "ui_video_questions = {}\n",
    "try: ui_video_questions = load_metadata_for_inference(METADATA_FILE)\n",
    "except Exception as e: display(Markdown(f\"❌ UI Load Error: {e}\"))\n",
    "\n",
    "# (Widgets setup remains the same)\n",
    "video_options = [(\"Select video...\", None)]\n",
    "if ui_video_questions: video_options.extend(sorted([(f\"{vid} ({len(qs)}q)\", vid) for vid, qs in ui_video_questions.items()]))\n",
    "video_selector = widgets.Dropdown(options=video_options, description='Video ID:', disabled=not ui_video_questions, style={'description_width': 'initial'})\n",
    "question_selector = widgets.Dropdown(options=[(\"Select question...\", None)], description='Question (QID):', disabled=True, layout=widgets.Layout(width='95%'), style={'description_width': 'initial'})\n",
    "run_button = widgets.Button(description='Run Inference', disabled=True, button_style='primary', icon='play')\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# --- Widget Interaction Logic (Remains mostly the same, calls updated sync function) --- #\n",
    "def on_video_selected(change):\n",
    "    # (Same logic)\n",
    "    selected_video_id = change['new']\n",
    "    question_selector.options = [(\"Select question...\", None)]\n",
    "    question_selector.value = None\n",
    "    question_selector.disabled = True\n",
    "    run_button.disabled = True\n",
    "    output_area.clear_output()\n",
    "    if selected_video_id and selected_video_id in ui_video_questions:\n",
    "        questions = ui_video_questions[selected_video_id]\n",
    "        question_options = sorted([(f\"{q.get('qid', 'N/A')}: {q.get('question', '')[:80]}...\", q) for q in questions if q.get('qid')])\n",
    "        if question_options:\n",
    "            question_selector.options = [(\"Select question...\", None)] + question_options\n",
    "            question_selector.disabled = False\n",
    "\n",
    "def on_question_selected(change):\n",
    "    # (Same logic)\n",
    "    selected_question_info = change['new']\n",
    "    run_button.disabled = selected_question_info is None\n",
    "    output_area.clear_output()\n",
    "    if selected_question_info:\n",
    "        with output_area:\n",
    "            display(Markdown(\"### Selected Info\"))\n",
    "            id_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "            display(pd.Series({\n",
    "                'qid': selected_question_info.get('qid'),\n",
    "                'question': selected_question_info.get('question'),\n",
    "                f'Resource ({id_col})': selected_question_info.get(id_col)\n",
    "            }).to_frame('Value'))\n",
    "\n",
    "def on_run_button_clicked(b):\n",
    "    run_button.disabled = True\n",
    "    output_area.clear_output()\n",
    "    with output_area:\n",
    "        if not video_selector.value or not question_selector.value: display(Markdown(\"❌ Select video & question.\")); run_button.disabled = False; return\n",
    "        if ai_client is None: display(Markdown(\"❌ AI Client not ready.\")); run_button.disabled = False; return\n",
    "\n",
    "        q_info = question_selector.value\n",
    "        qid = q_info.get('qid')\n",
    "        resource_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "        resource_id = q_info.get(resource_col)\n",
    "\n",
    "        display(Markdown(f\"### Running QID: {qid}\"))\n",
    "        display(Markdown(f\"--- Verifying Resource --- ({'GCS' if USE_VERTEX else 'File API'}) ---\"))\n",
    "        verified = False\n",
    "        if not resource_id: display(Markdown(f\"❌ Error: Missing '{resource_col}' ID.\"))\n",
    "        elif USE_VERTEX: verified = verify_gcs_file_exists(storage_client, resource_id)\n",
    "        else: verified = verify_file_api_resource_exists(storage_client, resource_id)\n",
    "\n",
    "        if not verified: display(Markdown(\"❌ Verification failed.\")); run_button.disabled = False; return\n",
    "        display(Markdown(f\"✅ Resource Verified: {resource_id}\"))\n",
    "\n",
    "        display(Markdown(\"Video Preview:\"))\n",
    "        video_path = Path(speed_videos_path) / f\"{video_selector.value}.mp4\"\n",
    "        if video_path.is_file():\n",
    "            video_widget = widgets.Video.from_file(video_path, width=400, height=300)\n",
    "            display(video_widget)\n",
    "\n",
    "        display(Markdown(\"### Inference Details\"))\n",
    "        display(Markdown(f\"**Video ID:** {video_selector.value} | **QID:** {qid}\"))\n",
    "        display(Markdown(f\"**Resource ID ({resource_col}):** {resource_id}\"))\n",
    "\n",
    "        prompt = build_prompt(q_info)\n",
    "        display(Markdown(\"### Prompt\"))\n",
    "        display(Markdown(f\"**Question:** {q_info.get('question', '')}\"))\n",
    "        display(Markdown(f\"**Question Type:** {q_info.get('question_type', 'default')}\"))\n",
    "        display(Markdown(f\"**Prompt Template:** {PROMPT_TEMPLATES.get(q_info.get('question_type', 'default'), PROMPT_TEMPLATES['default'])}\"))\n",
    "        display(Markdown(f\"**System Prompt:** {SYSTEM_PROMPT}\"))\n",
    "        display(HTML(f\"<pre style='white-space: pre-wrap; border: 1px solid #000; padding: 10px;'>{prompt}</pre>\"))\n",
    "        display(Markdown(\"--- Performing Inference (Sync) ---\"))\n",
    "\n",
    "        # CALL THE CORRECTED SYNC FUNCTION\n",
    "        inference_result = perform_inference_single_sync(q_info, ai_client)\n",
    "\n",
    "        display(Markdown(\"###--- Result ---\"))\n",
    "        if inference_result and isinstance(inference_result, dict):\n",
    "            # (Result display + Save button logic remains the same)\n",
    "            status, duration, answer, reason = (\n",
    "                inference_result.get(\"status\", \"?\"), inference_result.get('duration', -1),\n",
    "                inference_result.get('pred', ''), inference_result.get('finish_reason', 'N/A')\n",
    "            )\n",
    "            display(Markdown(f\"**Status:** {status} | **Duration:** {duration:.2f}s | **Reason:** {reason}\"))\n",
    "            display(Markdown(\"**Response:**\"))\n",
    "            display(HTML(f\"<pre style='white-space: pre-wrap; border: 1px solid #000; padding: 10px;'>{answer}</pre>\"))\n",
    "        else:\n",
    "            display(Markdown(\"❌ Invalid result.\"))\n",
    "\n",
    "    run_button.disabled = False\n",
    "\n",
    "# Register & Display\n",
    "video_selector.observe(on_video_selected, names='value')\n",
    "question_selector.observe(on_question_selected, names='value')\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "display(Markdown(\"### Select Video & Question\")); \n",
    "display(video_selector)\n",
    "display(question_selector)\n",
    "display(run_button)\n",
    "display(output_area)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiktok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
