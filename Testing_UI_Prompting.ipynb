{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9874197",
   "metadata": {},
   "source": [
    "#  Vertex Inference Unified\n",
    "\n",
    "This notebook uses the unified `google-genai` library (imported as `from google import genai`). It supports:\n",
    "- **Vertex AI Backend:** Uploads videos to GCS during the 'Prepare' step.\n",
    "- **Gemini API Backend:** Uploads videos using the **File API** during the 'Prepare' step.\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1.  **Import Libraries & Configure.**\n",
    "2.  **Config** - Set up the configuration for the pipeline, including the model, model config, prompts, and prompt config.\n",
    "2.  **Initialize Clients:** Set up AI client and Storage client.\n",
    "3.  **(Only the first time) Fetch Dataset:** Downloads metadata from HuggingFace.\n",
    "4.  **(Only the first time on each API type) Download, Extract & Prepare Videos:** Downloads, extracts, uploads (GCS/File API). Updates metadata.\n",
    "5.  **Bulk Inference (Async):** Performs inference using pre-uploaded video resources.\n",
    "6.  **Single Prompt Testing (UI):** Allows interactive testing of the video with prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19484ef",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "1. After switching from Vertex to Gemini and vice versa, be sure to follow the steps:\n",
    "    - Run all cells in order to re-upload the videos to the correct storage client, you can enable the SKIP_DOWNLOAD and SKIP_EXTRACT flags to skip the download and extraction steps. Only the upload step is needed\n",
    "\n",
    "2. Gemini API's file client has a expiry time of 1 day or so for the uploaded files. You may need to follow the steps above to re-upload the files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c07e40",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports (Corrected for `google.genai`)\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import datetime\n",
    "import zipfile\n",
    "import math\n",
    "import sys\n",
    "import asyncio\n",
    "from typing import Dict, List, Optional, Set, Tuple, Any\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import fractions\n",
    "\n",
    "# Google Cloud & AI Libraries (Unified SDK)\n",
    "try:\n",
    "    import google.genai as genai\n",
    "    from google.genai import types\n",
    "    from google.genai import errors as genai_errors\n",
    "    from google.api_core import exceptions as api_core_exceptions\n",
    "    # GCS Client (Optional, for Vertex Mode)\n",
    "    try:\n",
    "        from google.cloud import storage\n",
    "        GCS_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"INFO: google-cloud-storage not found. Vertex AI GCS operations unavailable.\")\n",
    "        storage = None\n",
    "        GCS_AVAILABLE = False\n",
    "    print(\"`google.genai` SDK and helpers imported successfully.\")\n",
    "except ImportError as e:\n",
    "     print(f\"ERROR: Failed to import Google libraries: {e}. Install: pip install google-genai google-api-core google-cloud-storage\")\n",
    "     genai = None; types = None; genai_errors = None; api_core_exceptions = None\n",
    "     storage = None; GCS_AVAILABLE = False\n",
    "     raise ImportError(\"FATAL: `google.genai` or `google-api-core` SDK not found.\")\n",
    "\n",
    "# Data Handling & Progress\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# UI Elements\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "\n",
    "# Async in Notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbce404",
   "metadata": {},
   "source": [
    "## Config Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GCP Configuration ---\n",
    "\n",
    "PROJECT_ID = \"YOUR_PROJECT_ID_HERE\" # Your Google Cloud Project ID (Needed for GCS and Vertex AI mode)\n",
    "LOCATION = \"us-central1\"      # Your Google Cloud Region (Needed for Vertex AI mode)\n",
    "GCS_BUCKET = \"YOUR_GCS_BUCKET_HERE\" # Your GCS bucket name (Needed for video storage)\n",
    "\n",
    "# --- Choose Backend Mode ---\n",
    "# Set USE_VERTEX to True to use the Vertex AI backend (requires ADC or service account auth).\n",
    "# Set USE_VERTEX to False to use the Gemini API backend (requires GEMINI_API_KEY).\n",
    "USE_VERTEX = False  # <-- CHANGE THIS TO True TO USE VERTEX AI\n",
    "\n",
    "# --- Gemini API Key (Only required if USE_VERTEX is False) ---\n",
    "# IMPORTANT: Replace with your actual Gemini API Key if USE_VERTEX is False.\n",
    "# Consider loading from environment variables (GOOGLE_API_KEY) or a secure secrets manager.\n",
    "GEMINI_API_KEY = \"\"  # Replace with your actual Gemini API Key\n",
    "\n",
    "\n",
    "\n",
    "# --- File Paths ---\n",
    "DATASET_CSV = \"dataset.csv\"               # Input dataset metadata from HuggingFace\n",
    "METADATA_FILE = \"video_metadata_vertex.csv\" if USE_VERTEX else \"video_metadata_non_vertex.csv\"      # Stores video info: video_id, local_path, gcs_uri (if Vertex), question data\n",
    "RESULTS_FILE = \"results_noncot_full_inference.csv\"              # Output file for inference predictions\n",
    "DOWNLOADS_DIR = \"downloads\"               # Directory for downloaded zip file\n",
    "EXTRACTED_VIDEOS_DIR = \"extracted_videos\" # Directory storing extracted .mp4 files locally\n",
    "SPEED_VIDEOS_DIR = \"speed_videos\"         # Stores sped up/slowed down videos\n",
    "HF_CACHE_DIR = \"./hf_cache\"               # Cache directory for HuggingFace datasets\n",
    "\n",
    "# --- Step 1: Fetch Dataset Configuration ---\n",
    "HF_DATASET_NAME = \"lmms-lab/AISG_Challenge\" # HuggingFace dataset identifier\n",
    "HF_DATASET_SPLIT = \"test\"                 # Dataset split to use\n",
    "SKIP_FETCH = False                        # Set True to skip fetching if DATASET_CSV exists\n",
    "\n",
    "# --- Step 2: Download & Prepare Videos Configuration ---\n",
    "VIDEO_ZIP_URL = \"https://huggingface.co/datasets/lmms-lab/AISG_Challenge/resolve/main/Benchmark-AllVideos-HQ-Encoded-challenge.zip?download=true\"\n",
    "ZIP_FILE_NAME = \"all_videos.zip\"\n",
    "SKIP_DOWNLOAD_ZIP = True                 # Set True to skip downloading if zip exists\n",
    "SKIP_EXTRACT = True                      # Set True to skip extraction if videos exist locally\n",
    "SKIP_PREPARE = False                      # Set True to skip video preparation (GCS upload for Vertex, metadata update)\n",
    "MAX_VIDEOS_TO_PROCESS = None              # Limit videos for testing (e.g., 5), None for all\n",
    "UPLOAD_BATCH_SIZE_GCS = 10                # Batch size for GCS uploads (Vertex mode only)\n",
    "\n",
    "# --- Inference Configuration ---\n",
    "# Choose a model name compatible with your selected method (Vertex AI or Gemini API)\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# Vertex AI: gemini-2.0-flash, gemini-2.0-flash-lite, gemini-2.0-pro-exp-02-05, gemini-2.0-flash-thinking-exp-01-21\n",
    "# Rate limits: https://cloud.google.com/vertex-ai/generative-ai/docs/quotas#gemini-2.0-flash\n",
    "# Basically 500 requests per minute for 2.0-flash and 2.0-flash-lite (unlimited), 10 requests per minute for 2.0-pro-exp-02-05, gemini-2.5-flash-preview-04-17\n",
    "\n",
    "# Gemini API: gemini-2.0-flash, gemini-2.0-flash-lite, gemini-2.0-flash-thinking-exp-01-21, gemini-2.5-pro-exp-03-25\n",
    "# Rate limits: https://ai.google.dev/gemini-api/docs/rate-limits#tier-1\n",
    "# For free tier: 30 requests per minute for 2.0-flash and 2.0-flash-lite, 10 requests per minute for 2.0-pro-exp-02-05\n",
    "# For tier-1: 2000 requests per minute for 2.0-flash and 2.0-flash-lite (have to pay), 10 requests per minute for 2.0-pro-exp-02-05 and gemini-2.0-flash-thinking-exp-01-21, gemini-2.5-flash-preview-04-17\n",
    "\n",
    "# 1.0=normal speed, 0.5=half speed, etc.\n",
    "VIDEO_SPEED_FACTOR = 0.5\n",
    "\n",
    "# --- Setup Derived Paths & Directories ---\n",
    "zip_file_path = Path(DOWNLOADS_DIR) / ZIP_FILE_NAME\n",
    "extracted_videos_path = Path(EXTRACTED_VIDEOS_DIR)\n",
    "speed_videos_path = Path(SPEED_VIDEOS_DIR) / str(VIDEO_SPEED_FACTOR)\n",
    "Path(DOWNLOADS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "extracted_videos_path.mkdir(parents=True, exist_ok=True)\n",
    "speed_videos_path.mkdir(parents=True, exist_ok=True)\n",
    "Path(HF_CACHE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler(sys.stdout)])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Configuration Validation & Display --- #\n",
    "warnings_found = False\n",
    "if USE_VERTEX:\n",
    "    if not PROJECT_ID or PROJECT_ID == \"your-gcp-project-id\":\n",
    "        logger.error(\"Vertex AI mode requires PROJECT_ID to be set.\")\n",
    "        warnings_found = True\n",
    "    if not LOCATION:\n",
    "        logger.error(\"Vertex AI mode requires LOCATION to be set.\")\n",
    "        warnings_found = True\n",
    "    if not GCS_BUCKET or GCS_BUCKET == \"your-gcs-bucket-name\":\n",
    "        logger.error(\"Vertex AI mode requires GCS_BUCKET for video uploads.\")\n",
    "        warnings_found = True\n",
    "    if not GCS_AVAILABLE:\n",
    "        logger.error(\"Vertex AI mode requires 'google-cloud-storage', but it's not installed.\")\n",
    "        warnings_found = True\n",
    "else: # Gemini API Mode\n",
    "    # Check API Key (explicit or env var)\n",
    "    effective_api_key = GEMINI_API_KEY if GEMINI_API_KEY != \"YOUR_API_KEY_HERE\" else os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not effective_api_key:\n",
    "        logger.error(\"Gemini API mode requires GEMINI_API_KEY or GOOGLE_API_KEY environment variable.\")\n",
    "        warnings_found = True\n",
    "    else:\n",
    "        # Don't store the key in the config display if loaded from env\n",
    "        if GEMINI_API_KEY == \"YOUR_API_KEY_HERE\" and os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "            GEMINI_API_KEY = \"(Loaded from GOOGLE_API_KEY env var)\"\n",
    "        logger.info(\"Gemini API mode configured. Videos will be uploaded via File API.\")\n",
    "\n",
    "if warnings_found:\n",
    "     print(\"\\n\\n************************* WARNING *************************\")\n",
    "     print(\"Configuration errors detected above. Execution might fail.\")\n",
    "     print(\"***********************************************************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d8e3e7",
   "metadata": {},
   "source": [
    "# Main Model Selection (Innovation 1)\n",
    "Agentic CoT - Chain of Thought + Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f679e23",
   "metadata": {},
   "source": [
    "## 1. COT and NonCOT Model for Bulk Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd933ec",
   "metadata": {},
   "source": [
    "### NonCoT output models for Bulk Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d36ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see all available models, go into the NonCoT_output_models.py file\n",
    "# NonCoT models are used for Bulk Inference\n",
    "from models.NonCoT_output_models import get_non_cot_model\n",
    "non_cot_model_list = [\"gemini-2.5-flash-preview-04-17\", \"gemini-2.5-pro-exp-03-25\"]\n",
    "\n",
    "MODEL_NAME, SYSTEM_PROMPT, PROMPT_TEMPLATES, CONFIG, REQUESTS_PER_MINUTE, MAX_RETRIES, MAX_ASYNC_WORKERS  = get_non_cot_model(non_cot_model_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3fa7c",
   "metadata": {},
   "source": [
    "### COT output models for Bulk Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To See All Available Models, Go into the CoT_ouput_models.py file\n",
    "# CoT models are used for Bulk Inference\n",
    "from models.CoT_ouput_models import get_cot_model\n",
    "CoT_model_list = [\"gemini-2.0-flash\",\"gemini-2.5-pro-preview-05-06\", \"gemini-2.5-pro-preview-03-25\"]\n",
    "MODEL_NAME, SYSTEM_PROMPT, PROMPT_TEMPLATES, CONFIG, REQUESTS_PER_MINUTE, MAX_RETRIES, MAX_ASYNC_WORKERS = get_cot_model(CoT_model_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f03ecf",
   "metadata": {},
   "source": [
    "### Summary Output Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f92ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see all available models, go into the Summary_models.py file\n",
    "from models.Summary_models import get_summary_model\n",
    "summary_model_list = [\"gemini-2.0-flash-ver1\", \"gemini-2.0-flash-ver2\", \"gemini-2.0-flash-ver3\"]\n",
    "\n",
    "QUESTION_MODEL_NAME, QUESTION_SYSTEM_PROMPT, QUESTION_CONFIG = get_summary_model(summary_model_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c003849",
   "metadata": {},
   "source": [
    "### Generated Questions Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82780583",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS_MODEL_NAME = \"gemini-2.0-flash\"\n",
    "QUESTIONS_DIR = os.path.join(f\"generated_questions/{QUESTIONS_MODEL_NAME}\", \"questions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc5d20",
   "metadata": {},
   "source": [
    "# Basic Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563fba21",
   "metadata": {},
   "source": [
    "## Initialize Google Cloud Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a54202",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = None\n",
    "ai_client = None\n",
    "\n",
    "# --- Initialize Generative AI Client (`google.genai`) --- #\n",
    "display(Markdown(\"### Initializing Generative AI Client (`google.genai`)\"))\n",
    "try:\n",
    "    if USE_VERTEX:\n",
    "        display(Markdown(f\"Vertex AI backend (Project: {PROJECT_ID}, Loc: {LOCATION})...\"))\n",
    "        if not PROJECT_ID or not LOCATION or PROJECT_ID == \"your-gcp-project-id\":\n",
    "             raise ValueError(\"PROJECT_ID/LOCATION invalid for Vertex AI.\")\n",
    "        # Initialize Client for Vertex\n",
    "        ai_client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
    "        display(Markdown(f\"✅ Vertex AI Client Initialized.\"))\n",
    "    else: # Gemini API Mode\n",
    "        display(Markdown(\"Gemini API backend (using API Key)...\"))\n",
    "        effective_api_key = GEMINI_API_KEY if GEMINI_API_KEY != \"YOUR_API_KEY_HERE\" else os.environ.get(\"GOOGLE_API_KEY\")\n",
    "        if not effective_api_key:\n",
    "             if os.environ.get(\"GOOGLE_API_KEY\"): effective_api_key = None # Client uses env var\n",
    "             else: raise ValueError(\"Gemini API Key required but not found.\")\n",
    "        # Initialize Client for Gemini API\n",
    "        ai_client = genai.Client(api_key=effective_api_key, vertexai=False)\n",
    "        display(Markdown(f\"✅ Gemini API Client Initialized.\"))\n",
    "\n",
    "except ValueError as ve: display(Markdown(f\"❌ **Config Error:** {ve}\")); ai_client = None\n",
    "except Exception as e: display(Markdown(f\"❌ **AI Client Error:** {e}.\")); logger.error(\"AI Client Init Failed\", exc_info=True); ai_client = None\n",
    "\n",
    "# --- Initialize Storage Client (ONLY for Vertex AI mode) --- #\n",
    "if USE_VERTEX:\n",
    "    display(Markdown(\"### Initializing GCS Client (Vertex Mode Only)\"))\n",
    "    if not GCS_AVAILABLE: display(Markdown(\"❌ GCS lib missing.\")); raise RuntimeError(\"Missing GCS lib.\")\n",
    "    if not GCS_BUCKET or GCS_BUCKET == \"your-gcs-bucket-name\": display(Markdown(\"❌ GCS_BUCKET needed.\")); raise ValueError(\"GCS_BUCKET required.\")\n",
    "    try:\n",
    "        storage_client = storage.Client(project=PROJECT_ID)\n",
    "        if not storage_client.bucket(GCS_BUCKET).exists(): display(Markdown(f\"⚠️ GCS Bucket `{GCS_BUCKET}` inaccessible.\"))\n",
    "        else: display(Markdown(f\"✅ GCS Client Initialized (Bucket: '{GCS_BUCKET}').\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **GCS Client Error:** {e}.\")); logger.error(\"GCS Client Init Failed\", exc_info=True)\n",
    "        if not SKIP_PREPARE: raise RuntimeError(\"GCS client failed.\")\n",
    "        else: display(Markdown(\"⚠️ GCS client failed, but skipping prep.\"))\n",
    "else:\n",
    "    display(Markdown(\"### Initializing Gemini API Client (File API)\"))\n",
    "    try:\n",
    "        storage_client = ai_client.files\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ **Gemini File API Client Error:** {e}.\")); logger.error(\"Gemini API Client Init Failed\", exc_info=True)\n",
    "    display(Markdown(f\"✅ Gemini File API Client Initialized.\"))\n",
    "\n",
    "# --- Final Checks --- #\n",
    "if ai_client is None: raise RuntimeError(\"AI client failed.\")\n",
    "if USE_VERTEX and storage_client is None and not SKIP_PREPARE: raise RuntimeError(\"GCS client failed for Vertex prep.\")\n",
    "display(Markdown(\"✅ Client initialization complete.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6c267",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc9694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File/Data Handling ---\n",
    "def load_processed_qids(filename: str) -> Set[str]:\n",
    "    processed_qids = set()\n",
    "    if Path(filename).is_file():\n",
    "        try:\n",
    "            df = pd.read_csv(filename, usecols=['qid'], dtype={'qid': str}, on_bad_lines='warn')\n",
    "            processed_qids = set(df['qid'].dropna().unique())\n",
    "            logger.info(f\"Loaded {len(processed_qids)} processed QIDs from {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not read QIDs from {filename}: {e}. Assuming zero processed.\")\n",
    "    return processed_qids\n",
    "\n",
    "def download_file_with_progress(url: str, destination: Path):\n",
    "    logger.info(f\"Downloading {url} to {destination}...\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=600)\n",
    "        response.raise_for_status()\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024 * 1024\n",
    "        with open(destination, 'wb') as f, tqdm(\n",
    "            desc=f\"Downloading {destination.name}\", total=total_size, unit='iB', unit_scale=True, unit_divisor=1024\n",
    "        ) as bar:\n",
    "            for data in response.iter_content(block_size):\n",
    "                size = f.write(data)\n",
    "                bar.update(size)\n",
    "        if total_size != 0 and bar.n != total_size:\n",
    "            destination.unlink(missing_ok=True)\n",
    "            raise RuntimeError(f\"Download size mismatch for {destination.name}.\")\n",
    "        logger.info(f\"Successfully downloaded {destination}\")\n",
    "    except Exception as e:\n",
    "        destination.unlink(missing_ok=True)\n",
    "        logger.error(f\"Download failed for {url}: {e}\")\n",
    "        raise\n",
    "\n",
    "def extract_zip(zip_path: Path, extract_to: Path):\n",
    "    logger.info(f\"Extracting {zip_path.name} to {extract_to}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            members = [m for m in zip_ref.namelist() if not m.startswith('__MACOSX/') and not m.endswith('.DS_Store')]\n",
    "            with tqdm(total=len(members), desc=f\"Extracting {zip_path.name}\") as pbar:\n",
    "                for member in members:\n",
    "                    zip_ref.extract(member=member, path=extract_to)\n",
    "                    pbar.update(1)\n",
    "        logger.info(f\"Successfully extracted {zip_path} to {extract_to}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Extraction error: {e}\")\n",
    "        raise\n",
    "    \n",
    "def move_videos_to_main_directory(base_path):\n",
    "    \"\"\"Find all MP4 files in subdirectories and move them to the main directory.\"\"\"\n",
    "    logger.info(f\"Moving all videos to main directory: {base_path}\")\n",
    "    moved_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    # Find all MP4 files in subdirectories (but not in the main directory)\n",
    "    for file_path in list(base_path.glob('**/*.mp4')):\n",
    "        # Skip files already in the main directory or hidden Mac files\n",
    "        if file_path.parent == base_path or file_path.name.startswith('._'):\n",
    "            continue\n",
    "            \n",
    "        # Destination in the main directory\n",
    "        dest_path = base_path / file_path.name\n",
    "        \n",
    "        try:\n",
    "            # Move the file\n",
    "            shutil.move(str(file_path), str(dest_path))\n",
    "            moved_count += 1\n",
    "            if moved_count % 50 == 0:\n",
    "                logger.info(f\"Moved {moved_count} videos so far...\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error moving {file_path}: {e}\")\n",
    "            failed_count += 1\n",
    "    \n",
    "    logger.info(f\"Moved {moved_count} videos to main directory. Failed: {failed_count}\")\n",
    "    \n",
    "\n",
    "def create_or_update_metadata(metadata_path: str, dataset_df: pd.DataFrame, video_updates: Dict[str, Dict]):\n",
    "    try:\n",
    "        required_cols = ['video_id', 'qid']\n",
    "        update_cols = ['local_path', 'gcs_uri', 'file_api_name', 'status']\n",
    "        dtype_map = {'video_id': str, 'qid': str} # Ensure IDs are strings\n",
    "\n",
    "        if not Path(metadata_path).is_file():\n",
    "            logger.info(f\"Creating metadata file: {metadata_path}\")\n",
    "            meta_df = dataset_df.copy()\n",
    "            for col in update_cols: meta_df[col] = pd.NA\n",
    "            meta_df['status'] = 'pending'\n",
    "        else:\n",
    "            logger.debug(f\"Loading existing metadata: {metadata_path}\")\n",
    "            meta_df = pd.read_csv(metadata_path, dtype=dtype_map)\n",
    "            for col in update_cols: # Add missing update columns if needed\n",
    "                 if col not in meta_df.columns: meta_df[col] = pd.NA\n",
    "\n",
    "        if not all(col in meta_df.columns for col in required_cols):\n",
    "            raise ValueError(f\"Metadata missing required columns ({required_cols}).\")\n",
    "\n",
    "        updates_df = pd.DataFrame.from_dict(video_updates, orient='index')\n",
    "        updates_df.index.name = 'video_id'\n",
    "        updates_df.reset_index(inplace=True)\n",
    "        updates_df['video_id'] = updates_df['video_id'].astype(str)\n",
    "\n",
    "        # Use merge for robust updating across potentially multiple rows per video_id\n",
    "        # First, prepare updates DF with only the necessary columns (video_id + update_cols)\n",
    "        merge_cols = ['video_id'] + [col for col in update_cols if col in updates_df.columns]\n",
    "        updates_to_merge = updates_df[merge_cols].drop_duplicates(subset=['video_id'], keep='last')\n",
    "\n",
    "        # Merge, prioritizing updates\n",
    "        # Suffixes help identify original vs update cols if needed, but update will overwrite\n",
    "        merged_df = pd.merge(meta_df, updates_to_merge, on='video_id', how='left', suffixes=('', '_update'))\n",
    "\n",
    "        # Apply the updates\n",
    "        for col in update_cols:\n",
    "            update_col_name = col + '_update'\n",
    "            if update_col_name in merged_df.columns:\n",
    "                # Fill NAs in original col with update col, then drop update col\n",
    "                meta_df[col] = merged_df[update_col_name].fillna(merged_df[col])\n",
    "                # Alternative: Directly update where update is not NA\n",
    "                # meta_df[col] = np.where(merged_df[update_col_name].notna(), merged_df[update_col_name], merged_df[col])\n",
    "\n",
    "        meta_df.to_csv(metadata_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Metadata file '{metadata_path}' updated with {len(video_updates)} video records.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating metadata {metadata_path}: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def load_metadata_for_inference(metadata_file: str = METADATA_FILE) -> Dict[str, List[Dict]]:\n",
    "    if not Path(metadata_file).is_file(): return {}\n",
    "    video_questions = defaultdict(list)\n",
    "    required_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "    try:\n",
    "        df = pd.read_csv(metadata_file, dtype=str).fillna('')\n",
    "        if 'video_id' not in df.columns or required_col not in df.columns:\n",
    "            logger.error(f\"Metadata missing 'video_id' or '{required_col}'.\")\n",
    "            return {}\n",
    "        valid_df = df[df['video_id'].astype(bool) & df[required_col].astype(bool)]\n",
    "        if len(valid_df) == 0:\n",
    "             logger.warning(f\"No videos found with '{required_col}' in {metadata_file}. Check Step 4.\")\n",
    "             return {}\n",
    "        for video_id, group in valid_df.groupby('video_id'):\n",
    "             video_questions[video_id] = group.to_dict('records')\n",
    "        logger.info(f\"Loaded {len(video_questions)} videos ({len(valid_df)} questions) with valid IDs for inference.\")\n",
    "        return dict(video_questions)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading metadata for inference: {e}\", exc_info=True)\n",
    "        return {}\n",
    "\n",
    "# --- Upload/Verification Helpers ---\n",
    "def upload_to_gcs(storage_client, bucket_name: str, source_file_path: Path, destination_blob_name: str) -> Optional[str]:\n",
    "    if not GCS_AVAILABLE or storage_client is None or not source_file_path.is_file(): return None\n",
    "    try:\n",
    "        blob = storage_client.bucket(bucket_name).blob(destination_blob_name)\n",
    "        blob.upload_from_filename(str(source_file_path))\n",
    "        gcs_uri = f\"gs://{bucket_name}/{destination_blob_name}\"\n",
    "        logger.debug(f\"GCS OK: {source_file_path} -> {gcs_uri}\")\n",
    "        return gcs_uri\n",
    "    except Exception as e:\n",
    "        logger.error(f\"GCS Fail: {source_file_path}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def upload_via_file_api(storage_client, local_path: Path, display_name: str) -> Optional[str]:\n",
    "    if storage_client is None or not local_path.is_file(): return None\n",
    "    try:\n",
    "        logger.debug(f\"Uploading {local_path} via File API...\")\n",
    "        uploaded_file = storage_client.upload(file=local_path)\n",
    "        logger.info(f\"File API OK: {local_path} -> {uploaded_file.name}\")\n",
    "        return uploaded_file.name\n",
    "    except Exception as e:\n",
    "        logger.error(f\"File API Fail: {local_path}. Error: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def verify_gcs_file_exists(storage_client, gcs_uri: str) -> bool:\n",
    "    if not GCS_AVAILABLE or storage_client is None or not gcs_uri: return False\n",
    "    try:\n",
    "        exists = storage.Blob.from_string(gcs_uri, client=storage_client).exists()\n",
    "        if not exists: logger.warning(f\"GCS verify failed: {gcs_uri}\")\n",
    "        return exists\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error verifying GCS {gcs_uri}: {e}\")\n",
    "        return False\n",
    "\n",
    "def verify_file_api_resource_exists(storage_client, file_api_name: str) -> bool:\n",
    "    if not storage_client or not file_api_name: return False\n",
    "    try:\n",
    "        _ = storage_client.get(name=file_api_name) # Sync get for verification\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error verifying File API {file_api_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def verify_local_file_exists(local_path: str) -> bool:\n",
    "    exists = Path(local_path).is_file() if local_path else False\n",
    "    if not exists: logger.warning(f\"Local verify failed: {local_path}\")\n",
    "    return exists\n",
    "\n",
    "# --- Prompt Building ---\n",
    "def build_prompt(question_info: dict) -> str:\n",
    "    question = question_info.get(\"question\", \"\")\n",
    "    q_type = question_info.get(\"question_type\", \"default\")\n",
    "    template = PROMPT_TEMPLATES.get(q_type, PROMPT_TEMPLATES[\"default\"])\n",
    "    # if q_type is MCQ\n",
    "    if q_type == \"Multiple-choice Question with a Single Correct Answer\":\n",
    "        return template.format(question=question).strip() + \"\\n\" + \"E. None of the above\"\n",
    "    return template.format(question=question).strip() + \"\\n\" + question_info.get(\"question_prompt\").strip()\n",
    "\n",
    "# --- Rate Limiter ---\n",
    "class AsyncRateLimiter:\n",
    "    \"\"\"\n",
    "    An asyncio-compatible token bucket rate limiter.\n",
    "\n",
    "    Args:\n",
    "        rate (int): The maximum number of requests allowed per period.\n",
    "        period (float): The time period in seconds (default: 60 for RPM).\n",
    "        capacity (int, optional): The maximum burst capacity. Defaults to `rate`.\n",
    "    \"\"\"\n",
    "    def __init__(self, rate: int, period: float = 60.0, capacity: Optional[int] = None):\n",
    "        if rate <= 0:\n",
    "            raise ValueError(\"Rate must be positive\")\n",
    "        if period <= 0:\n",
    "            raise ValueError(\"Period must be positive\")\n",
    "\n",
    "        self.rate = rate\n",
    "        self.period = float(period)\n",
    "        self.capacity = float(capacity if capacity is not None else rate)\n",
    "        self._tokens = self.capacity # Start full\n",
    "        self._last_refill_time = time.monotonic()\n",
    "        self._lock = asyncio.Lock()\n",
    "\n",
    "    def _get_tokens_per_second(self) -> float:\n",
    "        return self.rate / self.period\n",
    "\n",
    "    async def _refill(self):\n",
    "        \"\"\"Replenishes tokens based on elapsed time. Must be called under lock.\"\"\"\n",
    "        now = time.monotonic()\n",
    "        elapsed = now - self._last_refill_time\n",
    "        if elapsed > 0:\n",
    "            tokens_to_add = elapsed * self._get_tokens_per_second()\n",
    "            self._tokens = min(self.capacity, self._tokens + tokens_to_add)\n",
    "            self._last_refill_time = now\n",
    "\n",
    "    async def acquire(self):\n",
    "        \"\"\"\n",
    "        Acquires a token, waiting if necessary.\n",
    "        \"\"\"\n",
    "        async with self._lock:\n",
    "            await self._refill() # Refill based on time since last acquire/refill\n",
    "\n",
    "            while self._tokens < 1:\n",
    "                # Calculate how long to wait for 1 token\n",
    "                tokens_needed = 1.0 - self._tokens\n",
    "                wait_time = tokens_needed / self._get_tokens_per_second()\n",
    "\n",
    "                # Release the lock before sleeping\n",
    "                lock_released = True\n",
    "                try:\n",
    "                    self._lock.release()\n",
    "                    logger.debug(f\"Rate limit hit. Waiting for {wait_time:.3f}s for next token.\")\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                finally:\n",
    "                    # Re-acquire the lock if it was released\n",
    "                    if lock_released:\n",
    "                        await self._lock.acquire()\n",
    "\n",
    "                # Refill again after waiting, as more time has passed\n",
    "                await self._refill()\n",
    "\n",
    "            # Consume a token\n",
    "            self._tokens -= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a920ea1",
   "metadata": {},
   "source": [
    "# Testing UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c768a3",
   "metadata": {},
   "source": [
    "## Single Prompt Single Question Testing UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7061f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference_single_sync(question_info: Dict, client: Any) -> Dict[str, Any]:\n",
    "    qid = question_info.get(\"qid\", \"?\")\n",
    "    prompt_text = build_prompt(question_info)\n",
    "    gcs_uri = question_info.get(\"gcs_uri\")\n",
    "    file_api_name = question_info.get(\"file_api_name\")\n",
    "    start_time = time.time()\n",
    "    result = None\n",
    "\n",
    "    try:\n",
    "        video_part = None\n",
    "        if USE_VERTEX:\n",
    "            if not gcs_uri: raise ValueError(\"Missing GCS URI.\")\n",
    "            video_part = types.Part.from_uri(mime_type=\"video/mp4\", file_uri=gcs_uri)\n",
    "        else:\n",
    "            if not file_api_name: raise ValueError(\"Missing File API name.\")\n",
    "            try:\n",
    "                # Fetch File object sync\n",
    "                file_object = client.files.get(name=file_api_name)\n",
    "                video_part = file_object\n",
    "            except genai_errors.NotFoundError: raise FileNotFoundError(f\"File API '{file_api_name}' not found.\")\n",
    "            except Exception as e: raise RuntimeError(f\"Failed get File API obj: {e}\")\n",
    "\n",
    "        question_content = types.Content(\n",
    "            role=\"user\", \n",
    "            parts=[types.Part.from_text(text=prompt_text)]\n",
    "        )\n",
    "        contents = [\n",
    "            question_content,\n",
    "            video_part\n",
    "        ]\n",
    "            \n",
    "\n",
    "    except (ValueError, FileNotFoundError, RuntimeError) as e:\n",
    "        logger.error(f\"QID {qid} (Sync): Input Error - {e}\")\n",
    "        return {\"qid\": qid, \"pred\": f\"ERROR: Input Fail - {e}\", \"duration\": 0, \"status\": \"Failed (Input)\"}\n",
    "\n",
    "    # Inference with Retries (Sync)\n",
    "    for attempt in range(MAX_RETRIES + 1):\n",
    "        try:\n",
    "            api_start = time.time()\n",
    "            # Use sync client.models\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL_NAME,\n",
    "                contents=contents,\n",
    "                config=CONFIG,\n",
    "            )\n",
    "            answer, reason, status, err_detail = \"ERROR\", \"UNKNOWN\", \"Success\", \"\"\n",
    "            try: # Process Response\n",
    "                answer = response\n",
    "                if response.candidates: reason = response.candidates[0].finish_reason.name\n",
    "            except ValueError as ve:\n",
    "                status, err_detail = \"Blocked/Empty\", f\"ValueError: {ve}. \"\n",
    "                answer = f\"ERROR: {status}. {err_detail}\"\n",
    "            result = {\"qid\": qid, \"pred\": answer, \"duration\": time.time()-start_time, \"finish_reason\": reason, \"status\": status}\n",
    "            return result\n",
    "        except (api_core_exceptions.ResourceExhausted) as e:\n",
    "             if attempt < MAX_RETRIES: time.sleep(INITIAL_BACKOFF_SECONDS * (2**attempt))\n",
    "             else: result = {\"qid\": qid, \"pred\": f\"ERROR: Max Retries ({type(e).__name__}) - {e}\", \"duration\": time.time()-start_time, \"status\": \"Failed (Retries)\"}; return result\n",
    "        except genai_errors.APIError as e:\n",
    "             result = {\"qid\": qid, \"pred\": f\"ERROR: GenAI APIError - {e}\", \"duration\": time.time()-start_time, \"status\": \"Failed (API Error)\"}; return result\n",
    "        except Exception as e:\n",
    "             result = {\"qid\": qid, \"pred\": f\"ERROR: Unexpected - {e}\", \"duration\": time.time()-start_time, \"status\": \"Failed (Unexpected)\"}; return result\n",
    "    # Fallback\n",
    "    result = {\"qid\": qid, \"pred\": \"ERROR: Unknown after retries\", \"duration\": time.time()-start_time, \"status\": \"Failed (Unknown)\"}\n",
    "    return result\n",
    "\n",
    "# --- UI Setup (using `google.genai` types where needed) --- #\n",
    "ui_video_questions = {}\n",
    "try: ui_video_questions = load_metadata_for_inference(METADATA_FILE)\n",
    "except Exception as e: display(Markdown(f\"❌ UI Load Error: {e}\"))\n",
    "\n",
    "# (Widgets setup remains the same)\n",
    "video_options = [(\"Select video...\", None)]\n",
    "if ui_video_questions: video_options.extend(sorted([(f\"{vid} ({len(qs)}q)\", vid) for vid, qs in ui_video_questions.items()]))\n",
    "video_selector = widgets.Dropdown(options=video_options, description='Video ID:', disabled=not ui_video_questions, style={'description_width': 'initial'})\n",
    "question_selector = widgets.Dropdown(options=[(\"Select question...\", None)], description='Question (QID):', disabled=True, layout=widgets.Layout(width='95%'), style={'description_width': 'initial'})\n",
    "run_button = widgets.Button(description='Run Inference', disabled=True, button_style='primary', icon='play')\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# --- Widget Interaction Logic (Remains mostly the same, calls updated sync function) --- #\n",
    "def on_video_selected(change):\n",
    "    # (Same logic)\n",
    "    selected_video_id = change['new']\n",
    "    question_selector.options = [(\"Select question...\", None)]\n",
    "    question_selector.value = None\n",
    "    question_selector.disabled = True\n",
    "    run_button.disabled = True\n",
    "    output_area.clear_output()\n",
    "    if selected_video_id and selected_video_id in ui_video_questions:\n",
    "        questions = ui_video_questions[selected_video_id]\n",
    "        question_options = sorted([(f\"{q.get('qid', 'N/A')}: {q.get('question', '')[:80]}...\", q) for q in questions if q.get('qid')])\n",
    "        if question_options:\n",
    "            question_selector.options = [(\"Select question...\", None)] + question_options\n",
    "            question_selector.disabled = False\n",
    "\n",
    "def on_question_selected(change):\n",
    "    # (Same logic)\n",
    "    selected_question_info = change['new']\n",
    "    run_button.disabled = selected_question_info is None\n",
    "    output_area.clear_output()\n",
    "    if selected_question_info:\n",
    "        with output_area:\n",
    "            display(Markdown(\"### Selected Info\"))\n",
    "            id_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "            display(pd.Series({\n",
    "                 'qid': selected_question_info.get('qid'),\n",
    "                 'question': selected_question_info.get('question'),\n",
    "                 f'Resource ({id_col})': selected_question_info.get(id_col)\n",
    "            }).to_frame('Value'))\n",
    "\n",
    "def on_run_button_clicked(b):\n",
    "    run_button.disabled = True\n",
    "    output_area.clear_output()\n",
    "    with output_area:\n",
    "        if not video_selector.value or not question_selector.value: display(Markdown(\"❌ Select video & question.\")); run_button.disabled = False; return\n",
    "        if ai_client is None: display(Markdown(\"❌ AI Client not ready.\")); run_button.disabled = False; return\n",
    "\n",
    "        q_info = question_selector.value\n",
    "        qid = q_info.get('qid')\n",
    "        resource_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "        resource_id = q_info.get(resource_col)\n",
    "\n",
    "        display(Markdown(f\"### Running QID: {qid}\"))\n",
    "        display(Markdown(f\"--- Verifying Resource --- ({'GCS' if USE_VERTEX else 'File API'}) ---\"))\n",
    "        verified = False\n",
    "        if not resource_id: display(Markdown(f\"❌ Error: Missing '{resource_col}' ID.\"))\n",
    "        elif USE_VERTEX: verified = verify_gcs_file_exists(storage_client, resource_id)\n",
    "        else: verified = verify_file_api_resource_exists(storage_client, resource_id)\n",
    "\n",
    "        if not verified: display(Markdown(\"❌ Verification failed.\")); run_button.disabled = False; return\n",
    "        display(Markdown(f\"✅ Resource Verified: {resource_id}\"))\n",
    "\n",
    "        display(Markdown(\"Video Preview:\"))\n",
    "        video_path = Path(extracted_videos_path) / f\"{video_selector.value}.mp4\"\n",
    "        if video_path.is_file():\n",
    "            video_widget = widgets.Video.from_file(video_path, width=400, height=300)\n",
    "            display(video_widget)\n",
    "\n",
    "        display(Markdown(\"### Inference Details\"))\n",
    "        display(Markdown(f\"**Video ID:** {video_selector.value} | **QID:** {qid}\"))\n",
    "        display(Markdown(f\"**Resource ID ({resource_col}):** {resource_id}\"))\n",
    "\n",
    "        prompt = build_prompt(q_info)\n",
    "        display(Markdown(\"### Prompt\"))\n",
    "        display(Markdown(f\"**Question:** {q_info.get('question', '')}\"))\n",
    "        display(Markdown(f\"**Question Type:** {q_info.get('question_type', 'default')}\"))\n",
    "        display(Markdown(f\"**Prompt Template:** {PROMPT_TEMPLATES.get(q_info.get('question_type', 'default'), PROMPT_TEMPLATES['default'])}\"))\n",
    "        display(Markdown(f\"**System Prompt:** {SYSTEM_PROMPT}\"))\n",
    "        display(HTML(f\"<pre style='white-space: pre-wrap; border: 1px solid #000; padding: 10px;'>{prompt}</pre>\"))\n",
    "        display(Markdown(\"--- Performing Inference (Sync) ---\"))\n",
    "\n",
    "        # CALL THE CORRECTED SYNC FUNCTION\n",
    "        inference_result = perform_inference_single_sync(q_info, ai_client)\n",
    "\n",
    "        display(Markdown(\"--- Result ---\"))\n",
    "        if inference_result and isinstance(inference_result, dict):\n",
    "             # (Result display + Save button logic remains the same)\n",
    "            status, duration, answer, reason = (\n",
    "                inference_result.get(\"status\", \"?\"), inference_result.get('duration', -1),\n",
    "                inference_result.get('pred', ''), inference_result.get('finish_reason', 'N/A')\n",
    "            )\n",
    "            display(Markdown(f\"**Status:** {status} | **Duration:** {duration:.2f}s | **Reason:** {reason}\"))\n",
    "            display(Markdown(\"**Response:**\"))\n",
    "            display(HTML(f\"<pre style='white-space: pre-wrap; border: 1px solid #000; padding: 10px;'>{answer}</pre>\"))\n",
    "        else:\n",
    "            display(Markdown(\"❌ Invalid result.\"))\n",
    "\n",
    "    run_button.disabled = False\n",
    "\n",
    "# Register & Display\n",
    "video_selector.observe(on_video_selected, names='value')\n",
    "question_selector.observe(on_question_selected, names='value')\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "display(Markdown(\"### Select Video & Question\")); \n",
    "display(video_selector)\n",
    "display(question_selector)\n",
    "display(run_button)\n",
    "display(output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237afdae",
   "metadata": {},
   "source": [
    "## Generated Questions Prompt chaining Testing UI - turn by turn Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7531a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Fetch Generated Questions for each videos ---\n",
    "import ast\n",
    "\n",
    "def get_questions_for_video(questions_file: str = QUESTIONS_DIR) -> Dict[str, List[str]]:\n",
    "    if not Path(questions_file).is_file(): return {}\n",
    "    video_questions = defaultdict(list)\n",
    "    try:\n",
    "        df = pd.read_csv(questions_file, dtype=str).fillna('')\n",
    "        if 'video_id' not in df.columns or 'questions' not in df.columns:\n",
    "            logger.error(f\"Questions file missing 'video_id' or 'question'.\")\n",
    "            return {}\n",
    "        valid_df = df[df['video_id'].astype(bool) & df['questions'].astype(bool)]\n",
    "        if len(valid_df) == 0:\n",
    "             logger.warning(f\"No videos found with questions in {questions_file}. Check Step 4.\")\n",
    "             return {}\n",
    "        for video_id in valid_df['video_id'].unique():\n",
    "            question_str = valid_df[valid_df['video_id'] == video_id]['questions'].iloc[0]\n",
    "            questions_list = ast.literal_eval(question_str)\n",
    "            video_questions[video_id] = questions_list\n",
    "        logger.info(f\"Loaded {len(video_questions)} videos ({len(valid_df)} questions) with valid IDs for inference.\")\n",
    "        return dict(video_questions)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading questions: {e}\", exc_info=True)\n",
    "        return {}\n",
    "\n",
    "generated_questions_dict = get_questions_for_video(QUESTIONS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b338e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1⃣  Request renderer – handles list + File objects safely\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def _render_request(contents: list, turn_idx: int):\n",
    "    \"\"\"\n",
    "    Pretty‑print the payload sent to the model.\n",
    "\n",
    "    Handles:\n",
    "    • `types.Part` (binary video or textual description)\n",
    "    • `types.Content` (normal chat messages)\n",
    "    • plain File objects (Vertex File API)\n",
    "    • nested lists of Parts (video_part list in Vertex branch)\n",
    "    \"\"\"\n",
    "    # flatten in case the first element itself is a list of Parts\n",
    "    flat: list[Any] = []\n",
    "    for item in contents:\n",
    "        if isinstance(item, list):\n",
    "            flat.extend(item)\n",
    "        else:\n",
    "            flat.append(item)\n",
    "\n",
    "    lines = []\n",
    "    for c in flat:\n",
    "        if isinstance(c, types.Part):\n",
    "            # text Part vs. binary video Part\n",
    "            if getattr(c, \"text\", None):\n",
    "                lines.append(f\"video_desc: {c.text}\")\n",
    "            else:\n",
    "                lines.append(\"[video/mp4]\")\n",
    "        elif hasattr(c, \"parts\"):                           # types.Content\n",
    "            role = getattr(c, \"role\", \"?\")\n",
    "            txt = getattr(c.parts[0], \"text\", \"\")\n",
    "            lines.append(f\"{role}: {txt}\")\n",
    "        else:                                               # plain File\n",
    "            lines.append(\"[file]\")\n",
    "\n",
    "    joined_lines = '\\n'.join(lines)\n",
    "    display(HTML(\n",
    "        f\"<div style='border:1px dashed #999;padding:8px;margin:8px 0;'>\"\n",
    "        f\"<strong>Turn {turn_idx} – Request sent to model:</strong>\"\n",
    "        f\"<pre style='white-space:pre-wrap;margin:4px 0 0;'>{joined_lines}</pre>\"\n",
    "        f\"</div>\"\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc70a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference_single_sync(question_info: Dict, client: Any) -> Dict[str, Any]:\n",
    "    qid = question_info.get(\"qid\", \"?\")\n",
    "    video_id = question_info.get(\"video_id\", \"?\")\n",
    "    prompt_text = build_prompt(question_info)\n",
    "    gcs_uri = question_info.get(\"gcs_uri\")\n",
    "    file_api_name = question_info.get(\"file_api_name\")\n",
    "    start_time = time.time()\n",
    "    result = None\n",
    "    \n",
    "\n",
    "    questions_list = generated_questions_dict.get(video_id, [])[:3]        \n",
    "        \n",
    "    questions_list.append(prompt_text)\n",
    "            \n",
    "    try:\n",
    "        if USE_VERTEX:\n",
    "            if not gcs_uri: raise ValueError(\"Missing GCS URI.\")\n",
    "            video_part = types.Part.from_uri(mime_type=\"video/mp4\", file_uri=gcs_uri)\n",
    "        else:\n",
    "            if not file_api_name: raise ValueError(\"Missing File API name.\")\n",
    "            try:\n",
    "                # Fetch File object sync\n",
    "                file_object = client.files.get(name=file_api_name)\n",
    "                video_part = file_object\n",
    "            except genai_errors.NotFoundError: raise FileNotFoundError(f\"File API '{file_api_name}' not found.\")\n",
    "            except Exception as e: raise RuntimeError(f\"Failed get File API obj: {e}\")\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"❌ Video resource error: {e}\")); return\n",
    "\n",
    "    chat: list = []\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    for idx, q in enumerate(questions_list, 1):\n",
    "        user_msg = types.Content(role=\"user\",\n",
    "                                 parts=[types.Part.from_text(text=q)])\n",
    "        # always include video_part\n",
    "        contents = [video_part] + chat + [user_msg]\n",
    "\n",
    "        _render_request(contents, idx)\n",
    "\n",
    "        try:\n",
    "            rsp = client.models.generate_content(model=MODEL_NAME,\n",
    "                                                 contents=contents,\n",
    "                                                 config=CONFIG)\n",
    "            answer = rsp.text.strip()\n",
    "            if answer:\n",
    "              summary_content = types.Content(\n",
    "              role=\"user\",\n",
    "              parts=[\n",
    "                  types.Part.from_text(text=q),\n",
    "                  types.Part.from_text(text=answer)\n",
    "              ])\n",
    "              summary_rsp = client.models.generate_content(model=QUESTION_MODEL_NAME,\n",
    "                                    contents=summary_content,\n",
    "                                    config=QUESTION_CONFIG)\n",
    "              sumamry_answer = summary_rsp.text.strip()\n",
    "            finish_reason = (rsp.candidates[0].finish_reason.name\n",
    "                             if rsp.candidates and rsp.candidates[0].finish_reason else \"UNKNOWN\")\n",
    "        except Exception as e:\n",
    "            answer, finish_reason = f\"ERROR: {e}\", \"Failed (API)\"\n",
    "\n",
    "        display(HTML(\n",
    "            f\"<div style='border:1px solid #000;margin:8px 0;padding:10px;'>\"\n",
    "            f\"<b>Q{idx}:</b> {q}</div>\"\n",
    "        ))\n",
    "        display(HTML(\n",
    "            f\"<div style='border:1px solid #000;margin:0 0 12px;\"\n",
    "            f\"padding:10px;background:#f9f9f9;'><b>CoT:</b> {answer}</div>\"\n",
    "        ))\n",
    "        display(HTML(\n",
    "            f\"<div style='border:1px solid #000;margin:0 0 12px;\"\n",
    "            f\"padding:10px;background:#f9f9f9;'><b>Summary Answer:</b> {sumamry_answer}</div>\"\n",
    "        ))\n",
    "\n",
    "        chat.extend([\n",
    "            user_msg,\n",
    "            types.Content(role=\"model\",\n",
    "                          parts=[types.Part.from_text(text=answer or \"…\")])\n",
    "        ])\n",
    "    \n",
    "    final_answer = chat[-1].parts[0].text if chat else \"No answer\"\n",
    "    \n",
    "    # def serialize_chat(chat):\n",
    "    #     return [\n",
    "    #         {\n",
    "    #             \"role\": msg.role,\n",
    "    #             \"parts\": [part.text for part in msg.parts]\n",
    "    #         } for msg in chat\n",
    "    #     ]\n",
    "\n",
    "    # with open(\"chat_history.json\", \"w\") as f:\n",
    "    #     json.dump(serialize_chat(chat), f, indent=2)\n",
    "    \n",
    "    result = {\"qid\": qid, \"pred\": final_answer, \"duration\": time.time()-start_time, \"status\": \"Successful\"}\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- UI Setup (using `google.genai` types where needed) --- #\n",
    "ui_video_questions = {}\n",
    "try: ui_video_questions = load_metadata_for_inference(METADATA_FILE)\n",
    "except Exception as e: display(Markdown(f\"❌ UI Load Error: {e}\"))\n",
    "\n",
    "# (Widgets setup remains the same)\n",
    "video_options = [(\"Select video...\", None)]\n",
    "if ui_video_questions: video_options.extend(sorted([(f\"{vid} ({len(qs)}q)\", vid) for vid, qs in ui_video_questions.items()]))\n",
    "video_selector = widgets.Dropdown(options=video_options, description='Video ID:', disabled=not ui_video_questions, style={'description_width': 'initial'})\n",
    "question_selector = widgets.Dropdown(options=[(\"Select question...\", None)], description='Question (QID):', disabled=True, layout=widgets.Layout(width='95%'), style={'description_width': 'initial'})\n",
    "run_button = widgets.Button(description='Run Inference', disabled=True, button_style='primary', icon='play')\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# --- Widget Interaction Logic (Remains mostly the same, calls updated sync function) --- #\n",
    "def on_video_selected(change):\n",
    "    # (Same logic)\n",
    "    selected_video_id = change['new']\n",
    "    question_selector.options = [(\"Select question...\", None)]\n",
    "    question_selector.value = None\n",
    "    question_selector.disabled = True\n",
    "    run_button.disabled = True\n",
    "    output_area.clear_output()\n",
    "    if selected_video_id and selected_video_id in ui_video_questions:\n",
    "        questions = ui_video_questions[selected_video_id]\n",
    "        question_options = sorted([(f\"{q.get('qid', 'N/A')}: {q.get('question', '')[:80]}...\", q) for q in questions if q.get('qid')])\n",
    "        if question_options:\n",
    "            question_selector.options = [(\"Select question...\", None)] + question_options\n",
    "            question_selector.disabled = False\n",
    "\n",
    "def on_question_selected(change):\n",
    "    # (Same logic)\n",
    "    selected_question_info = change['new']\n",
    "    run_button.disabled = selected_question_info is None\n",
    "    output_area.clear_output()\n",
    "    if selected_question_info:\n",
    "        with output_area:\n",
    "            display(Markdown(\"### Selected Info\"))\n",
    "            id_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "            display(pd.Series({\n",
    "                 'qid': selected_question_info.get('qid'),\n",
    "                 'question': selected_question_info.get('question'),\n",
    "                 f'Resource ({id_col})': selected_question_info.get(id_col)\n",
    "            }).to_frame('Value'))\n",
    "\n",
    "def on_run_button_clicked(b):\n",
    "    run_button.disabled = True\n",
    "    output_area.clear_output()\n",
    "    with output_area:\n",
    "        if not video_selector.value or not question_selector.value: display(Markdown(\"❌ Select video & question.\")); run_button.disabled = False; return\n",
    "        if ai_client is None: display(Markdown(\"❌ AI Client not ready.\")); run_button.disabled = False; return\n",
    "\n",
    "        q_info = question_selector.value\n",
    "        qid = q_info.get('qid')\n",
    "        resource_col = 'gcs_uri' if USE_VERTEX else 'file_api_name'\n",
    "        resource_id = q_info.get(resource_col)\n",
    "\n",
    "        display(Markdown(f\"### Running QID: {qid}\"))\n",
    "        display(Markdown(f\"--- Verifying Resource --- ({'GCS' if USE_VERTEX else 'File API'}) ---\"))\n",
    "        verified = False\n",
    "        if not resource_id: display(Markdown(f\"❌ Error: Missing '{resource_col}' ID.\"))\n",
    "        elif USE_VERTEX: verified = verify_gcs_file_exists(storage_client, resource_id)\n",
    "        else: verified = verify_file_api_resource_exists(storage_client, resource_id)\n",
    "\n",
    "        if not verified: display(Markdown(\"❌ Verification failed.\")); run_button.disabled = False; return\n",
    "        display(Markdown(f\"✅ Resource Verified: {resource_id}\"))\n",
    "\n",
    "        display(Markdown(\"Video Preview:\"))\n",
    "        video_path = Path(extracted_videos_path) / f\"{video_selector.value}.mp4\"\n",
    "        if video_path.is_file():\n",
    "            video_widget = widgets.Video.from_file(video_path, width=400, height=300)\n",
    "            display(video_widget)\n",
    "\n",
    "        display(Markdown(\"### Inference Details\"))\n",
    "        display(Markdown(f\"**Video ID:** {video_selector.value} | **QID:** {qid}\"))\n",
    "        display(Markdown(f\"**Resource ID ({resource_col}):** {resource_id}\"))\n",
    "\n",
    "        prompt = build_prompt(q_info)\n",
    "        display(Markdown(\"### Prompt\"))\n",
    "        display(Markdown(f\"**Question:** {q_info.get('question', '')}\"))\n",
    "        display(Markdown(f\"**Question Type:** {q_info.get('question_type', 'default')}\"))\n",
    "        display(Markdown(f\"**Prompt Template:** {PROMPT_TEMPLATES.get(q_info.get('question_type', 'default'), PROMPT_TEMPLATES['default'])}\"))\n",
    "        display(Markdown(f\"**System Prompt:** {SYSTEM_PROMPT}\"))\n",
    "        display(HTML(f\"<pre style='white-space: pre-wrap; border: 1px solid #000; padding: 10px;'>{prompt}</pre>\"))\n",
    "        display(Markdown(\"--- Performing Inference (Sync) ---\"))\n",
    "\n",
    "        # CALL THE CORRECTED SYNC FUNCTION\n",
    "        inference_result = perform_inference_single_sync(q_info, ai_client)\n",
    "\n",
    "        display(Markdown(\"###--- Result ---\"))\n",
    "        if inference_result and isinstance(inference_result, dict):\n",
    "             # (Result display + Save button logic remains the same)\n",
    "            status, duration, answer, reason = (\n",
    "                inference_result.get(\"status\", \"?\"), inference_result.get('duration', -1),\n",
    "                inference_result.get('pred', ''), inference_result.get('finish_reason', 'N/A')\n",
    "            )\n",
    "            display(Markdown(f\"**Status:** {status} | **Duration:** {duration:.2f}s | **Reason:** {reason}\"))\n",
    "            display(Markdown(\"**Response:**\"))\n",
    "            display(HTML(f\"<pre style='white-space: pre-wrap; border: 1px solid #000; padding: 10px;'>{answer}</pre>\"))\n",
    "        else:\n",
    "            display(Markdown(\"❌ Invalid result.\"))\n",
    "\n",
    "    run_button.disabled = False\n",
    "\n",
    "# Register & Display\n",
    "video_selector.observe(on_video_selected, names='value')\n",
    "question_selector.observe(on_question_selected, names='value')\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "display(Markdown(\"### Select Video & Question\")); \n",
    "display(video_selector)\n",
    "display(question_selector)\n",
    "display(run_button)\n",
    "display(output_area)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
